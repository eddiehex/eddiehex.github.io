[{"title":"各类分布及其MLE推导","url":"/2025/04/20/distribution_and_mle/","content":"\n**核心概念：为什么需要概率分布？**\n\n想象一下你想描述一个随机现象的结果，比如：\n\n*   抛一次硬币的结果（正面还是反面？）\n*   明天会不会下雨（是还是否？）\n*   一个班级学生的身高（具体数值是多少？）\n*   一小时内某个路口通过的汽车数量（具体数量是多少？）\n\n概率分布就是用来**数学化地描述这些随机现象（随机变量）可能出现的结果以及每个结果出现的可能性（概率或概率密度）**的工具。它就像一个“蓝图”或“配方”，告诉我们数据大概长什么样。\n\n**关键组成部分：**\n\n*   **随机变量 (Random Variable):** 一个变量，它的值是某个随机过程的结果。通常用大写字母表示，如 X、Y。\n    *   **离散随机变量 (Discrete):** 只能取有限个或可数无限个特定值（如上面例子中的硬币结果、下雨与否、汽车数量）。\n    *   **连续随机变量 (Continuous):** 可以取某个区间内的任意值（如上面例子中的身高）。\n*   **参数 (Parameters):** 控制分布具体形状和位置的“旋钮”或“设定值”。它们通常是未知的，是我们需要通过数据（比如用 MLE）来估计的对象。用希腊字母表示很常见，如 p, λ, μ, σ。\n*   **概率质量函数 (PMF) 或 概率密度函数 (PDF):**\n    *   **PMF (Probability Mass Function):** 用于离散随机变量。它给出了随机变量取某个特定值的概率。通常写作 P(X=k)。所有可能值的概率加起来必须等于 1 (∑P(X=k)=1)。\n    *   **PDF (Probability Density Function):** 用于连续随机变量。它本身的值不是概率，但它描述了数据在某个点附近的相对可能性（密度）。某个区间 [a,b] 内的概率是通过对 PDF 在该区间上积分得到的 $P(a \\le X \\le b) = \\int_{a}^{b} f(x) dx$。整个定义域上的积分必须等于 1 ($\\int_{-\\infty}^{\\infty} f(x) dx = 1$)。通常写作 f(x)。\n\n这个 PMF 或 PDF 的数学表达式，就是我们用来计算“给定参数下，观测到某个数据的可能性”的基础，也是构建 MLE 中似然函数的关键！\n\n**由浅入深看几种常见分布：**\n\n**场景一：处理单个“是/否”或“成功/失败”事件 (离散)**\n\n*   **分布：** 伯努利分布 (Bernoulli Distribution)\n*   **描述：** 单次试验，只有两个可能结果（比如成功/失败，正面/反面，是/否），通常编码为 1 和 0。\n*   **随机变量 X：** X=1 代表“成功”，X=0 代表“失败”。\n*   **参数 θ=p：** p 是“成功” (X=1) 的概率 (0≤p≤1)。那么“失败” (X=0) 的概率就是 1−p。\n*   **PMF 表达式 (P(X=k∣p)):** 这是一个巧妙的写法，能同时表达 X=1 和 X=0 的情况：\n\n    $P(X=k | p) = p^k (1-p)^{1-k}$  其中 $k \\in \\{0, 1\\}$\n\n    验证一下：\n\n    *   当 k=1 (成功)时， $P(X=1 | p) = p^1 (1-p)^{1-1} = p^1 (1-p)^0 = p$。\n    *   当 k=0 (失败)时， $P(X=0 | p) = p^0 (1-p)^{1-0} = p^0 (1-p)^1 = 1-p$。\n*   **Demo:** 抛掷一枚可能不均匀的硬币一次。假设我们猜测 p=0.7（正面概率是 0.7）。\n\n    *   那么，观察到正面 (k=1) 的概率是 $P(X=1 | p=0.7) = 0.7$。\n    *   观察到反面 (k=0) 的概率是 $P(X=0 | p=0.7) = 1 - 0.7 = 0.3$。\n*   **MLE 应用：** 如果你观测到一次抛掷结果是 $x_1$ (比如 $x_1 = 1$)，那么这次观测的似然度 (Likelihood) 就是 $L(p | x_1) = P(X=x_1 | p) = p^{x_1} (1-p)^{1-x_1}$。如果有多次独立观测 $x_1, x_2, ..., x_n$，联合似然函数就是 $L(p) = \\prod_{i=1}^{n} p^{x_i} (1-p)^{1-x_i}$。\n\n**场景二：处理多次独立的“是/否”试验中，“成功”的总次数 (离散)**\n\n*   **分布：** 二项分布 (Binomial Distribution)\n*   **描述：** 进行 n 次独立的伯努利试验（每次试验成功的概率都是 p），计算总共“成功”了多少次。\n*   **随机变量 X：** X 代表 n 次试验中成功的总次数。X 可以取 0, 1, 2, ..., n 这些整数值。\n*   **参数 θ=(n,p)：** n 是总试验次数（通常是已知的），p 是单次试验成功的概率（通常是需要估计的）。\n*   **PMF 表达式 (P(X=k∣n,p)):**\n\n    $P(X=k | n, p) = \\binom{n}{k} p^k (1-p)^{n-k}$  其中 $k = 0, 1, ..., n$\n\n    $\\binom{n}{k} = \\frac{n!}{k!(n-k)!}$ 是组合数，表示从 n 次试验中选出 k 次成功有多少种组合方式。\n\n    *   $p^k$ 是 k 次成功发生的概率。\n    *   $(1-p)^{n-k}$ 是 n−k 次失败发生的概率。\n*   **Demo:** 抛掷一枚硬币 10 次 (n=10)，假设 p=0.7。想知道恰好观察到 8 次正面 (k=8) 的概率是多少？\n\n    $P(X=8 | n=10, p=0.7) = \\binom{10}{8} (0.7)^8 (1-0.7)^{10-8} = 45 \\times (0.7)^8 \\times (0.3)^2 \\approx 0.233$。\n*   **MLE 应用：**\n    *   情况 A (已知 n 次试验，观测到总成功数 k)： 如果你做了 n 次试验，观测到总共有 k 次成功，那么似然函数就是 $L(p | k, n) = \\binom{n}{k} p^k (1-p)^{n-k}$。然后你寻找使这个 L(p) 最大的 p。\n    *   情况 B (已知 n 次试验的每次结果)： 如果你知道每次试验的结果 $x_1, x_2, ..., x_n$（每个 $x_i$ 是 0 或 1），这其实回到了伯努利的情况。你使用 n 个伯努利观测的联合似然函数：$L(p) = \\prod_{i=1}^{n} p^{x_i} (1-p)^{1-x_i}$。最终通过 MLE 估计出的 p 在这两种情况下是相同的，等于样本中成功的比例 $\\frac{\\sum x_i}{n}$ 或 $\\frac{k}{n}$。\n\n**场景三：处理单位时间/空间内发生某事件的次数 (离散)**\n\n*   **分布：** 泊松分布 (Poisson Distribution)\n*   **描述：** 计算在固定的时间段、区域、体积等内，某个稀有事件发生的次数。假设事件独立发生，且平均发生率恒定。\n*   **随机变量 X：** X 代表在给定单位内事件发生的次数。X 可以取 0, 1, 2, ... 这些非负整数。\n*   **参数 θ=λ (lambda):** λ 是单位时间/空间内事件发生的平均次数 (λ>0)。\n*   **PMF 表达式 (P(X=k∣λ)):**\n\n    $P(X=k | \\lambda) = \\frac{e^{-\\lambda} \\lambda^k}{k!}$  其中 $k = 0, 1, 2, ...$\n\n    *   $e \\approx 2.71828$ 是自然常数。\n    *   $k!$ 是 k 的阶乘。\n*   **Demo:** 某个呼叫中心平均每小时接到 5 个电话 (λ=5)。想知道在接下来的一小时内，恰好接到 3 个电话 (k=3) 的概率是多少？\n\n    $P(X=3 | \\lambda=5) = \\frac{e^{-5} 5^3}{3!} = \\frac{e^{-5} \\times 125}{6} \\approx 0.14$。\n*   **MLE 应用：** 如果你观测了 n 个时间段，每个时间段内事件发生的次数分别是 $x_1, x_2, ..., x_n$。假设它们都服从参数为 λ 的泊松分布。\n\n    *   单个观测 $x_i$ 的概率是 $P(X=x_i | \\lambda) = \\frac{e^{-\\lambda} \\lambda^{x_i}}{x_i!}$。\n    *   联合似然函数是 $L(\\lambda) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{x_i}}{x_i!} = \\frac{e^{-n\\lambda} \\lambda^{\\sum x_i}}{\\prod x_i!}$。\n    *   通过最大化这个 L(λ)（或其对数），可以得到 λ 的 MLE 估计值，结果是样本均值 $\\hat{\\lambda}_{MLE} = \\frac{1}{n} \\sum_{i=1}^{n} x_i$。\n\n**场景四：处理某个区间内均匀随机取值 (连续)**\n\n*   **分布：** 均匀分布 (Uniform Distribution)\n*   **描述：** 在一个指定的区间 [a,b] 内，随机变量取任何值的可能性都是相等的。区间外取值的概率为 0。\n*   **随机变量 X：** 在 [a,b] 区间内取值。\n*   **参数 θ=(a,b)：** a 是区间的下界，b 是区间的上界 (a<b)。\n*   **PDF 表达式 (f(x∣a,b)):**\n\n    $ f(x | a, b) = \\begin{cases} \\frac{1}{b-a} & \\text{如果 } a \\le x \\le b \\\\ 0 & \\text{其他情况} \\end{cases} $\n\n    注意 PDF 的值是 $\\frac{1}{b-a}$ 这个常数，它不是概率，而是概率密度。整个区间的宽度是 b−a，高度是 $\\frac{1}{b-a}$，所以总面积（总概率）是 $(b-a) \\times \\frac{1}{b-a} = 1$。\n*   **Demo:** 计算机生成 [0,1] 之间的随机数 (a=0, b=1)。\n\n    *   PDF 是 $f(x | 0, 1) = \\frac{1}{1-0} = 1$，当 $0 \\le x \\le 1$ 时。\n    *   取值在 [0.2, 0.3] 之间的概率是 $\\int_{0.2}^{0.3} 1 dx = [x]_{0.2}^{0.3} = 0.3 - 0.2 = 0.1$。这正好是区间长度 (0.3−0.2) 乘以 PDF 的值 (1)。\n*   **MLE 应用：** 如果你观测到数据 $x_1, ..., x_n$，假设它们来自 Uniform(a, b) 分布。\n\n    *   似然函数 $L(a, b) = \\prod_{i=1}^{n} f(x_i | a, b)$。仅当所有的 $x_i$ 都在 [a, b] 区间内时，似然函数才不为零，此时 $L(a, b) = (\\frac{1}{b-a})^n$。\n    *   为了最大化 L(a, b)，我们需要让区间 [a, b] 尽可能小（这样 $\\frac{1}{b-a}$ 就尽可能大），但同时必须包含所有的观测数据 $x_i$。\n    *   所以 a 的 MLE 是样本最小值 $\\hat{a}_{MLE} = \\min(x_i)$，b 的 MLE 是样本最大值 $\\hat{b}_{MLE} = \\max(x_i)$。\n\n**场景五：处理事件发生前的等待时间 (连续)**\n\n*   **分布：** 指数分布 (Exponential Distribution)\n*   **描述：** 用于描述独立事件发生之间所需的时间，或者某个物品的使用寿命（如果损耗率恒定）。它是泊松过程的“姐妹”分布。\n*   **随机变量 X：** 等待时间或寿命 (X≥0)。\n*   **参数 θ=λ (lambda):** λ 是事件发生的速率 (rate parameter, λ>0)，即单位时间平均发生多少次事件。有时也用 β=1/λ (scale parameter) 作为参数。我们这里用速率参数 λ。\n*   **PDF 表达式 (f(x∣λ)):**\n\n    $ f(x | \\lambda) = \\begin{cases} \\lambda e^{-\\lambda x} & \\text{如果 } x \\ge 0 \\\\ 0 & \\text{如果 } x < 0 \\end{cases} $\n*   **Demo:** 假设某个零件的平均寿命是 100 小时。那么失效率 λ=1/100=0.01 次/小时。\n\n    *   该零件恰好在 50 小时 (x=50) 时刻的概率密度是 $f(50 | \\lambda=0.01) = 0.01 e^{-0.01 \\times 50} = 0.01 e^{-0.5} \\approx 0.00607$。\n    *   该零件寿命超过 50 小时的概率是 $P(X > 50) = \\int_{50}^{\\infty} 0.01 e^{-0.01x} dx = [-e^{-0.01x}]_{50}^{\\infty} = 0 - (-e^{-0.5}) = e^{-0.5} \\approx 0.607$。\n*   **MLE 应用：** 观测到 n 个同类零件的寿命 $x_1, ..., x_n$，假设服从参数为 λ 的指数分布。\n\n    *   似然函数 $L(\\lambda) = \\prod_{i=1}^{n} \\lambda e^{-\\lambda x_i} = \\lambda^n e^{-\\lambda \\sum x_i}$ (假设所有 $x_i \\ge 0$)。\n    *   最大化对数似然函数 $\\ell(\\lambda) = n \\ln \\lambda - \\lambda \\sum x_i$。\n    *   求导并令其为零 $\\frac{d\\ell}{d\\lambda} = \\frac{n}{\\lambda} - \\sum x_i = 0$。\n    *   解得 λ 的 MLE 是 $\\hat{\\lambda}_{MLE} = \\frac{n}{\\sum x_i} = \\frac{1}{\\bar{x}}$，即样本均值的倒数。这很直观：平均寿命越长（$\\bar{x}$ 大），失效率 λ 就越小。\n\n**场景六：处理自然界中常见的测量误差、人群身高体重等 (连续)**\n\n*   **分布：** 正态分布 (Normal Distribution) / 高斯分布 (Gaussian Distribution)\n*   **描述：** 自然界和工程中极其常见的分布，形状像一个对称的钟。由中心位置和分散程度决定。\n*   **随机变量 X：** 可以取任何实数值 (−∞, ∞)。\n*   **参数 θ=(μ, σ²)：** μ (mu) 是分布的均值（决定钟形曲线的中心位置），σ² (sigma squared) 是方差（决定钟形曲线的胖瘦，σ²>0）。σ 是标准差。\n*   **PDF 表达式 (f(x∣μ,σ²)):**\n\n    $f(x | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$\n\n    *   π ≈ 3.14159。\n    *   exp(y) 就是 $e^y$。\n\n    这个公式看起来复杂，但它精确地定义了那个钟形曲线。\n*   **Demo:** 假设某地成年男性的身高服从正态分布，均值为 175cm (μ=175)，方差为 25 (σ²=25，即标准差 $\\sigma=5$cm)。\n\n    *   身高恰好为 180cm (x=180) 的概率密度是 $f(180 | \\mu=175, \\sigma^2=25) = \\frac{1}{\\sqrt{2\\pi \\times 25}} \\exp\\left(-\\frac{(180-175)^2}{2 \\times 25}\\right) = \\frac{1}{\\sqrt{50\\pi}} \\exp\\left(-\\frac{25}{50}\\right) \\approx \\frac{1}{12.53} e^{-0.5} \\approx 0.0484$。\n*   **MLE 应用：** 观测到 n 个样本数据 $x_1, ..., x_n$，假设它们来自 $N(\\mu, \\sigma^2)$。\n\n    *   单个观测 $x_i$ 的概率密度是 $f(x_i | \\mu, \\sigma^2)$ (上面的公式)。\n    *   联合似然函数 $L(\\mu, \\sigma^2) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i-\\mu)^2}{2\\sigma^2}\\right)$。\n    *   对数似然函数 $\\ell(\\mu, \\sigma^2) = \\sum_{i=1}^{n} \\left[ \\ln\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right) - \\frac{(x_i-\\mu)^2}{2\\sigma^2} \\right] = \\sum_{i=1}^{n} \\left[ -\\frac{1}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln(\\sigma^2) - \\frac{(x_i-\\mu)^2}{2\\sigma^2} \\right] = -\\frac{n}{2}\\ln(2\\pi) - \\frac{n}{2}\\ln(\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2$\n    *   分别对 μ 和 σ² 求偏导数，并令它们等于零。\n    *   解得 MLE 估计值：\n\n        *   $\\hat{\\mu}_{MLE} = \\frac{1}{n} \\sum_{i=1}^{n} x_i = \\bar{x}$ （样本均值）\n        *   $\\hat{\\sigma}^2_{MLE} = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\bar{x})^2$ （样本方差，注意分母是 n 而不是 n-1）\n\n**总结与 MLE 连接**\n\n你看，每种分布都有其适用的场景、需要估计的参数 θ、以及一个核心的数学表达式 P(X=k∣θ) (PMF for discrete) 或 f(x∣θ) (PDF for continuous)。\n\n当你使用 MLE 时，构建联合似然函数的步骤是：\n\n1.  根据你的数据特点和业务场景，选择一个合适的概率分布模型。 这是最关键的一步，需要基于对数据生成过程的理解。\n2.  写出该分布的 PMF 或 PDF 表达式 P(xi∣θ) 或 f(xi∣θ)。 这是数学基础。\n3.  假设你的 n 个数据点 x1,...,xn 是独立同分布 (i.i.d.) 的。 这是标准假设。\n4.  将每个数据点的 PMF/PDF 值连乘起来，得到似然函数 L(θ):\n    *   离散: $L(\\theta) = \\prod_{i=1}^{n} P(x_i | \\theta)$\n    *   连续: $L(\\theta) = \\prod_{i=1}^{n} f(x_i | \\theta)$\n\n    这就是你问的“构建联合分布函数”在 MLE 语境下的具体操作——构建似然函数。\n5.  （通常）取对数得到对数似然函数 $ℓ(θ) = ∑i=1n lnP(xi∣θ)$ 或 $ℓ(θ) = ∑i=1n lnf(xi∣θ)$。\n6.  通过优化方法（求导置零或数值优化）找到使 ℓ(θ) 最大的参数 $θ^MLE$。\n\n希望这个由浅入深、结合例子的梳理能帮助你更好地理解概率分布及其在 MLE 中的应用。不用担心一次性全部记住，关键是理解核心思想和流程，需要时再查阅具体的公式。多练习几个例子会更有帮助！\n","tags":["Statistics","MLE"],"categories":["Statistics"]},{"title":"深度学习入门：从 MLP 到 RNN/LSTM，核心架构解析","url":"/2025/04/18/deeplearning/","content":"\n深度学习是机器学习领域一个非常强大且活跃的分支，它主要依赖于**人工神经网络**，特别是那些拥有**多层**结构的神经网络，来从数据中学习复杂的模式。理解不同的神经网络架构是入门深度学习的关键一步，因为不同的架构擅长处理不同类型的数据和任务。\n\n今天，我们将回顾以下几种核心架构：\n\n1. **MLP (多层感知机)：** 最基础的前馈网络。\n2. **CNN (卷积神经网络)：** 图像处理的利器。\n3. **RNN (循环神经网络) 及 LSTM：** 处理序列数据的能手。\n\n## 1. MLP (Multilayer Perceptron): 深度学习的基石\n\nMLP，即多层感知机，是最基础、最经典的一种**前馈神经网络**。你可以将它视为更复杂网络的构建块。\n\n**架构特点：**\n\n- 由**输入层**、一个或多个**隐藏层**和**输出层**组成。\n- 层与层之间是**全连接 (Fully Connected)** 的：上一层的每一个神经元节点都连接到下一层的每一个神经元节点。\n\n**处理过程（前向传播）：**\n\n1. **输入层：** 接收原始数据。**输入层的节点数量**等于你**单个样本的特征数量**。每个节点代表样本的一个特征值。\n2. **隐藏层/输出层：** 数据从上一层流向当前层。在当前层的每个节点，进行以下两个关键步骤：\n   - **线性转换 (Linear Transformation)：** 接收来自上一层所有节点的输出，将它们分别乘以对应的连接**权重 (Weights)**，然后求和，并加上该节点的**偏置 (Bias)**。这步计算是线性的 (Wx + b)。\n   - **非线性激活 (Non-linear Activation)：** 将线性转换的结果通过一个**非线性激活函数**（如 ReLU）。**这是非常关键的一步！**它引入了非线性，使得多层网络能够学习复杂的非线性关系，而不是仅仅叠加多个线性转换（那样最终仍是线性）。\n3. 这个“线性转换 + 非线性激活”的过程在每个隐藏层重复进行，直到数据到达输出层，产生最终预测结果。\n\n**总结：** MLP 通过多层**非线性**转换，将输入数据映射到输出空间。它是一种通用模型，但处理高维、有结构的数据（如图像）效率不高。\n\n## 2. CNN (Convolutional Neural Network): 图像世界的专家\n\nCNN，即卷积神经网络，是专门为处理具有**网格结构**的数据而设计的，最成功的应用领域是**图像处理**。它通过模仿人类视觉系统的一些特性来工作。\n\n**架构特点：**\n\nCNN 的核心在于其特殊的层：\n\n- **卷积层 (Convolutional Layer)：** CNN 的灵魂。\n- **池化层 (Pooling Layer)：** 常用于降维。\n- **全连接层 (Fully Connected Layer)：** 通常在网络的末端用于分类。\n\n**处理过程：**\n\n1. **输入层：** 接收图像数据。图像是 3D 的（高度、宽度、通道数）。\n2. **卷积层：**\n   - 使用小的**滤波器 (Filter) / 卷积核 (Kernel)** 在输入图片（或上一层的特征图）上**滑动**。\n   - 滤波器内包含学习到的权重，它与覆盖区域的像素值进行卷积运算（乘加）。\n   - 每个滤波器检测一种特定的**局部特征**（如边缘、纹理）。\n   - **参数共享：** 同一个滤波器在图片不同位置重复使用，大大减少参数。\n   - **局部连接：** 每个神经元只连接到输入的一个小区域。\n   - 输出是一系列**特征图 (Feature Map)**，每个特征图由一个滤波器生成，表示该特征在输入中的分布。\n3. **激活函数：** 对卷积层的输出（特征图）应用非线性激活。\n4. **池化层 (可选)：**\n   - 在特征图上滑动窗口，取窗口内的最大值（最大池化）或平均值（平均池化）。\n   - **作用：** 减小特征图的空间尺寸（降维），减少计算量，并增加对特征位置微小变化的鲁棒性。\n5. **重复：** 通常会堆叠多个卷积层和池化层。随着网络加深，学习到的特征越来越抽象和复杂。\n6. **展平层 (Flatten Layer)：** 在将 3D 的特征图输入到 MLP 式的全连接层之前，需要将其展平为一个 1D 向量。\n7. **全连接层 & 输出层：** 接收展平后的特征向量，进行 MLP 式的计算，最终输出分类或回归结果。\n\n**总结：** CNN 利用卷积和池化操作，有效地捕捉图像的**空间局部性**和**特征的层次结构**，并通过参数共享提高了效率，使其成为图像处理领域的首选架构。\n\n## 3. RNN (Recurrent Neural Network) & LSTM: 序列数据的记忆者\n\nRNN，即循环神经网络，是为处理**序列数据**而设计的，比如文本、时间序列、语音等，这些数据点之间存在**顺序**和**依赖关系**。\n\n**RNN 核心思想：记忆**\n\n与 MLP 和 CNN 不同，RNN 具有**“记忆”**能力。它在处理序列中的当前数据点时，会考虑之前处理过的数据点的信息。这种记忆体现在其内部的**隐藏状态 (Hidden State)** 中，隐藏状态会在处理序列的过程中不断更新，并将信息从上一个时间步传递到下一个时间步，形成一个**循环**。\n\n**处理过程：**\n\n1. **输入：** 序列数据，一个数据点（如一个词的向量）在**一个时间步 (Time Step)** 输入到 RNN 单元。\n2. **RNN 单元计算：** 在每个时间步 't'，RNN 单元接收：\n   - 当前输入 x_t。\n   - **上一个时间步的隐藏状态** h_{t-1}（这就是记忆的来源）。\n   - 它将 x_t 和 h_{t-1} 结合（通过加权求和和激活函数）进行计算。\n3. **输出：** 产生**当前时间步的隐藏状态** h_t（新的记忆）和可选的当前时间步的输出 y_t。\n4. **循环：** 计算出的 h_t 会被作为**下一个时间步**的 h_{t-1} 输入到同一个 RNN 单元中。\n\n**RNN 的挑战：长距离依赖**\n\n标准 RNN 在处理**长序列**时，由于梯度消失等问题，很难有效地捕捉和利用序列中**长距离的依赖关系**。早期的信息在多次传递和计算后容易丢失。\n\n**LSTM (Long Short-Term Memory): 增强的记忆**\n\nLSTM 是 RNN 的一个重要改进，专门为了解决标准 RNN 的长距离依赖问题而设计。\n\n**LSTM 的核心：细胞状态 (Cell State) 和 门 (Gates)**\n\nLSTM 引入了一个独立的**细胞状态 (Cell State)**，它像一条信息高速公路，可以相对完整地传递信息。同时，它有三个特殊的**“门”**来精确控制信息的流动：\n\n1. **遗忘门 (Forget Gate)：** 控制从**上一个细胞状态**中**丢弃**多少信息。\n2. **输入门 (Input Gate)：** 控制将当前输入和隐藏状态中的**哪些新信息**添加到**当前细胞状态**中。\n3. **输出门 (Output Gate)：** 控制从**当前细胞状态**中**输出**多少信息作为当前的**隐藏状态**。\n\n通过这些门的协同作用，LSTM 能够**选择性地记忆**重要的信息，**遗忘**不重要的信息，从而在长序列中有效地保持和利用长期记忆。\n\n**总结：** RNN（包括 LSTM）通过引入循环连接和隐藏状态，使得网络能够处理序列数据并具备记忆能力。LSTM 通过细胞状态和门控机制，克服了标准 RNN 在长序列上的局限性，是处理文本、语音等序列数据的强大工具。GRU 是 LSTM 的一个简化变体。\n\n**结语**\n\nMLP、CNN 和 RNN/LSTM 是深度学习中最基本也是最重要的几种神经网络架构。它们各自有不同的结构特点和适用场景：\n\n- **MLP：** 通用基础，理解线性+非线性转换。\n- **CNN：** 擅长网格数据（图像），理解卷积、池化和空间层次特征。\n- **RNN/LSTM：** 擅长序列数据，理解循环、隐藏状态和长期记忆。\n\n","tags":["deeplearning"],"categories":["deeplearning"]},{"title":"Stock Analysis three basic agents builds method","url":"/2025/02/17/stock_analysis_three_basic_agents_builds_method/","content":"\n## COT Prompt设计框架**\n\n#### **1. 情绪分析模块**\n**目标**：评估市场对个股的短期情绪波动与投资者行为倾向。\n\n**推理步骤与指标**：\n\n1. **技术指标信号**  \n   - 计算RSI（14日周期）：若>70则超买，<30则超卖  \n   - 监测MACD柱状图变化：DIF与DEA交叉信号（金叉/死叉）  \n   - 检查布林带：股价突破上轨（看涨）或下轨（看跌）  \n   ```公式  \n   RSI = 100 - (100 / (1 + 平均涨幅/平均跌幅))  \n   MACD = EMA(12) - EMA(26), Signal = EMA(MACD,9)  \n   ```\n\n\n2. **市场行为数据**  \n   - 换手率：近5日平均换手率>5%表明高关注度  \n   - 融资余额变化：连续3日融资买入额增长超过10%视为情绪升温  \n   - 北向资金流向：单日净流入占比流通市值>0.5%为积极信号  \n\n3. **舆情与新闻解析**  \n   - 使用NLP分析个股相关新闻标题情感倾向（正向/负向词频比）  \n   - 监测社交媒体讨论热度：东方财富论坛日发帖量突增2倍触发预警  \n\n4. **综合情绪指数**  \n   采用加权模型整合指标（参考）：  \n   ```  \n   情绪指数 = 0.3×RSI标准化值 + 0.2×换手率Z-score + 0.25×融资余额增长率 + 0.15×新闻情感得分 + 0.1×北向资金流量  \n   ```\n  \n   **阈值**：指数>0.8（过度乐观，建议减仓）；<-0.8（恐慌超卖，关注反弹）  \n\n---\n\n#### **2. 基本面分析模块**\n**目标**：评估个股的长期价值与经营质量。\n\n**推理步骤与指标**：\n1. **财务健康度诊断**  \n   - **盈利能力**：近3年ROE均值>15%，毛利率行业排名前30%  \n   - **偿债能力**：流动比率>2，资产负债率<行业均值  \n   - **现金流质量**：经营活动现金流净额/净利润>0.8（排除会计粉饰）  \n\n2. **成长性评估**  \n   - 复合增长率（CAGR）：过去5年营收CAGR>10%且净利润CAGR>15%  \n   - 研发投入强度：研发费用/营收占比高于行业75分位数  \n\n3. **估值合理性判断**  \n   - 动态PE分位数：当前PE位于近5年30%以下为低估  \n   - PEG比率：PEG<1（盈利增速匹配估值）  \n   ```公式  \n   PEG = (当前PE) / (预期净利润增长率×100)  \n   ```\n\n\n4. **行业与竞争分析**  \n   - 市场份额变化：近3年市占率提升幅度>2个百分点  \n   - 护城河评分：基于品牌溢价、专利数量、成本优势构建5级评分体系  \n\n**综合决策矩阵**：  \n| 维度                                                         | 权重 | 评分（1-5） | 加权得分 |\n| ------------------------------------------------------------ | ---- | ----------- | -------- |\n| 财务健康度                                                   | 30%  | -           | -        |\n| 成长性                                                       | 25%  | -           | -        |\n| 估值                                                         | 20%  | -           | -        |\n| 行业地位                                                     | 25%  | -           | -        |\n| **结论阈值**：总分≥4.0（强烈推荐）；2.5-4.0（中性）；<2.5（规避） |      |             |          |\n\n---\n\n#### **3. 技术分析模块**\n**目标**：识别价格趋势与关键买卖信号。\n\n**推理步骤与指标**：\n1. **趋势判定**  \n   - 均线系统：50日均线上穿200日均线（黄金交叉）确认长期上升趋势  \n   - ADX指标：ADX>25且+DI>-DI为强趋势信号  \n\n2. **动量与波动分析**  \n   - 布林带收缩突破：带宽（上轨-下轨）<10%历史分位后放量突破上轨  \n   - 成交量突变：单日成交量>5日均量2倍且价格上涨（确认资金介入）  \n\n3. **形态学信号**  \n   - 头肩底形态：右肩成交量>左肩，突破颈线幅度>3%  \n   - 杯柄形态：杯部回撤<30%，柄部缩量整理  \n\n4. **多因子择时模型**  \n   集成MACD、RSI、成交量变异系数构建信号强度：  \n   ```  \n   信号强度 = 0.4×MACD柱状图斜率 + 0.3×RSI偏离度 + 0.3×成交量Z-score  \n   ```\n  \n   **触发条件**：强度>1.5且K线收于5日最高点（买入）；<-1.5且跌破20日均线（卖出）  \n\n---\n\n### **COT Prompt输出规范**\n1. **结构化呈现**：分模块输出分析结论，使用表格对比关键指标与阈值。  \n2. **风险提示**：标注数据盲区（如财报审计风险、政策不确定性）。  \n3. **操作建议**：根据三模块综合评分生成概率化建议（例：情绪过热+基本面稳健+技术突破=70%概率短期看涨）。  \n\n**示例输出框架**：  \n```markdown\n### 情绪分析结论  \n- RSI(14)=68（接近超买）  \n- 融资余额周增幅+12%（情绪升温）  \n- 综合情绪指数=0.75（警戒区间） → 建议跟踪北向资金流向变化  \n\n### 基本面评估  \n- ROE三年均值18%（优于85%同行）  \n- PEG=0.89（估值合理）  \n- 行业地位评分4.2/5（龙头优势稳固）  \n\n### 技术信号  \n- MACD金叉+成交量突破（强买入信号）  \n- 短期目标价：前高+8%  \n\n**综合建议**：当前配置权重建议提升至15%，止损位设于50日均线下方3%  \n```\n\n","tags":["COT"],"categories":["Prompt"]},{"title":"深入浅出 Langchain 与 LangGraph：LLM 应用开发的双剑客","url":"/2025/02/05/langchain_langgraph_llm/","content":"\n## 一、核心概念对比：Chain vs. Graph\n\nLangchain 和 LangGraph 的核心区别在于它们构建应用的**基本模型**：\n\n*   **Langchain:** 专注于构建 **链式 (Chain)** 应用，将 LLM、Prompt、工具等组件 **线性串联**，形成一个顺序执行的工作流程。这种方式适合构建相对简单、流程固定的应用。\n*   **LangGraph:** 专注于构建 **图状 (Graph)** 应用，将 LLM、Prompt、工具等组件视为 **节点 (Node)**，节点之间通过 **边 (Edge)** 连接，形成一个更复杂、更灵活、可定制、支持状态管理的图状工作流程。这种方式适合构建需要复杂逻辑控制、状态管理和灵活性的应用，例如复杂的 Agent、多轮对话系统等。\n\n如果把 LLM 应用比作流程图，那么 Langchain 就像是 **流程图中的直线型流程**，而 LangGraph 则是 **带有分支、循环和复杂逻辑的流程图**。\n\n## 二、常见函数分类与对比\n\n本节将按照功能对 Langchain 和 LangGraph 的常见函数进行分类，并对比它们在类似功能上的实现方式。\n\n### 1. LLM (语言模型) 相关\n\n无论是 Langchain 还是 LangGraph，都需要与 LLM 进行交互。两者在这方面的处理方式类似，都是直接初始化 LLM 对象，例如 `ChatOpenAI`、`OpenAI` 等。\n\n**示例 (Langchain & LangGraph 类似):**\n\n```python\nfrom langchain_openai import ChatOpenAI\n\n# Langchain & LangGraph 中都一样\nllm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n```\n\n### 2. Prompt (提示词) 相关\n\nLangchain 和 LangGraph 都使用 `PromptTemplate` 来构建提示词模板，方便管理和复用 Prompt。\n\n**示例 (Langchain & LangGraph 类似):**\n\n```python\nfrom langchain.prompts import PromptTemplate\n\nprompt = PromptTemplate.from_template(\"请用 {language} 总结以下内容：{text}\")\n\n# Langchain 中可以直接格式化 prompt\nformatted_prompt_langchain = prompt.format(language=\"中文\", text=\"Langchain 是一个用于构建 LLM 应用的框架...\")\nprint(formatted_prompt_langchain)\n\n# LangGraph 中通常在 Node 中格式化 prompt\n# (后续 Node 示例会展示)\n```\n\n### 3. Chain (链) / Node (节点) 相关\n\n这是 Langchain 和 LangGraph 最核心的区别所在。\n\n#### 3.1 Langchain - Chain (链)\n\n*   **概念:** 将多个组件 (LLM, Prompt, 工具等) 线性串联起来，形成一个工作流程。\n*   **常见 Chain 类型:**\n    *   `LLMChain`: 最基础的 Chain，将 Prompt 和 LLM 结合。\n    *   `SequentialChain`: 将多个 Chain 顺序连接起来。\n    *   `SimpleSequentialChain`: 简化版的 `SequentialChain`。\n    *   `RetrievalQAChain`: 用于问答，结合检索器 (Retriever)。\n    *   `AgentExecutor`: 用于执行 Agent 的 Chain。\n*   **核心函数/类:**\n    *   `LLMChain(llm=..., prompt=...)`\n    *   `SequentialChain(chains=[...], input_variables=[...], output_variables=[...])`\n    *   `SimpleSequentialChain(chains=[...])`\n    *   `RetrievalQAChain.from_llm(llm=..., retriever=...)`\n    *   `AgentExecutor.from_agent_and_tools(agent=..., tools=...)`\n\n**示例 - Langchain Chain (`LLMChain`):**\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain\n\nllm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\nprompt = PromptTemplate.from_template(\"请用 {language} 总结以下内容：{text}\")\n\nchain = LLMChain(llm=llm, prompt=prompt)\n\noutput = chain.run(language=\"中文\", text=\"Langchain 是一个用于构建 LLM 应用的框架...\")\nprint(output)\n```\n\n#### 3.2 LangGraph - Node (节点)\n\n*   **概念:** 图中的一个计算单元，可以是 LLM 调用、函数调用、工具调用等。Node 是构建 Graph 的基本 building block。\n*   **Node 的形式:** 可以是函数、Langchain Runnable (例如 `LLMChain`, `RetrievalQAChain`) 等。\n*   **核心函数/类 (Graph 构建相关):**\n    *   `StateGraph(AgentState)`: 创建状态图，`AgentState` 是自定义的状态类。\n    *   `graph.add_node(\"node_name\", node_function)`: 向图中添加节点。\n    *   `graph.add_edge(\"node1_name\", \"node2_name\")`: 添加节点之间的有向边，表示流程顺序。\n    *   `graph.set_entry_point(\"node_name\")`: 设置图的入口节点。\n    *   `graph.set_conditional_edges(\"node_name\", conditional_mapping)`: 添加条件边，根据节点输出决定下一步走向。\n    *   `graph.set_finish_point(\"node_name\")`: 设置图的结束节点。\n\n**示例 - LangGraph Node (函数作为 Node):**\n\n```python\nfrom langgraph.graph import StateGraph\nfrom langchain_core.messages import AIMessage, HumanMessage\nfrom langgraph.pregel import PregelProcess\n\n# 定义状态类 (简单示例)\nclass AgentState:\n    messages: list[dict] = []\n\n# 定义一个 Node 函数\ndef summarize_node(state):\n    messages = state.get(\"messages\", [])\n    text_to_summarize = messages[-1][\"content\"] # 假设最后一条消息是用户输入\n    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n    prompt = PromptTemplate.from_template(\"请用中文总结以下内容：{text}\")\n    chain = LLMChain(llm=llm, prompt=prompt)\n    summary = chain.run(text=text_to_summarize)\n    return {\"messages\": messages + [{\"role\": \"assistant\", \"content\": summary}]} # 更新状态\n\n# 创建状态图\ngraph = StateGraph(AgentState)\n\n# 添加 Node\ngraph.add_node(\"summarize\", summarize_node)\n\n# 设置入口节点\ngraph.set_entry_point(\"summarize\")\n\n# 设置结束节点 (这里简化，直接结束)\ngraph.set_finish_point(\"summarize\")\n\n# 编译图\napp = graph.compile()\n\n# 运行图\ninputs = {\"messages\": [{\"role\": \"user\", \"content\": \"Langgraph 是一个用于构建复杂 LLM 应用的框架...\"}]}\nresult = app.invoke(inputs)\nprint(result)\n```\n\n**示例 - LangGraph Node (Langchain Runnable 作为 Node):**\n\n```python\nfrom langgraph.graph import StateGraph\nfrom langchain_openai import ChatOpenAI\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain\n\n# 定义状态类 (简单示例)\nclass AgentState:\n    messages: list[dict] = []\n\n# 创建 Langchain Runnable (LLMChain)\nllm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\nprompt = PromptTemplate.from_template(\"请用中文总结以下内容：{text}\")\nsummarize_chain = LLMChain(llm=llm, prompt=prompt)\n\n# 定义 Node 函数 (直接使用 Runnable)\ndef summarize_node(state):\n    messages = state.get(\"messages\", [])\n    text_to_summarize = messages[-1][\"content\"] # 假设最后一条消息是用户输入\n    summary = summarize_chain.run(text=text_to_summarize)\n    return {\"messages\": messages + [{\"role\": \"assistant\", \"content\": summary}]} # 更新状态\n\n# 创建状态图 (与上例相同)\ngraph = StateGraph(AgentState)\ngraph.add_node(\"summarize\", summarize_node)\ngraph.set_entry_point(\"summarize\")\ngraph.set_finish_point(\"summarize\")\napp = graph.compile()\n\n# 运行图 (与上例相同)\ninputs = {\"messages\": [{\"role\": \"user\", \"content\": \"Langgraph 是一个用于构建复杂 LLM 应用的框架...\"}]}\nresult = app.invoke(inputs)\nprint(result)\n```\n\n### 4. Agent (代理) 相关\n\n#### 4.1 Langchain - Agent\n\n*   **概念:** 能够根据用户输入和环境信息，自主决定下一步行动 (例如使用工具、调用 LLM)，并进行迭代，直到完成目标。\n*   **核心组件:**\n    *   `Agent`: 负责决策下一步行动。通常需要 `Prompt` 来指导 Agent 的行为。\n    *   `Tools`: Agent 可以使用的工具 (例如搜索、计算器、数据库查询等)。\n    *   `AgentExecutor`: 负责执行 Agent 的 Chain，管理 Agent 的状态和工具的使用。\n*   **核心函数/类:**\n    *   `create_react_agent(llm=..., tools=..., prompt=...)`: 创建 ReAct 类型的 Agent。\n    *   `AgentExecutor.from_agent_and_tools(agent=..., tools=...)`: 创建 AgentExecutor。\n\n**示例 - Langchain Agent (`ReAct Agent`):**\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import create_react_agent, AgentExecutor\nfrom langchain.tools import DuckDuckGoSearchRun\n\nllm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\ntools = [DuckDuckGoSearchRun()] # 使用 DuckDuckGo 搜索工具\n\nprompt = PromptTemplate.from_template(\"\"\"\n你是一个有用的助手，可以使用工具回答问题。\n\n{agent_scratchpad}\n\"\"\") # 简化 Prompt，实际 ReAct Prompt 会更复杂\n\nagent = create_react_agent(llm=llm, tools=tools, prompt=prompt)\nagent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)\n\noutput = agent_executor.run(\"今天北京天气怎么样？\")\nprint(output)\n```\n\n#### 4.2 LangGraph - Agent\n\n*   **概念:** Agent 被视为 Graph 中的一个或多个 Node。LangGraph 提供了更灵活的方式来构建 Agent 的工作流程，可以实现更复杂的状态管理、条件判断和循环逻辑。\n*   **核心概念:**\n    *   `AgentState`: 用于管理 Agent 的状态，例如对话历史、工具使用记录、中间结果等。\n    *   Agent 的决策和行动逻辑被分解到不同的 Node 中，并通过 Edge 连接。\n*   **示例 (LangGraph Agent 框架):** LangGraph 提供了一个 `pregel_process` 模块，用于构建更复杂的 Agent 流程，例如：\n    *   `pregel_process.from_functions(...)`: 从函数定义 Agent 的 Node 和 Edge。\n    *   `pregel_process.add_node(...)`, `pregel_process.add_edge(...)`, `pregel_process.set_conditional_edges(...)` 等用于构建更细粒度的 Agent 图结构。\n\n**示例 - LangGraph Agent (简化的概念示例，非完整可运行代码):**\n\n```python\nfrom langgraph.graph import StateGraph\nfrom langchain_openai import ChatOpenAI\nfrom langchain.tools import DuckDuckGoSearchRun\nfrom langchain.prompts import PromptTemplate\n\n# 定义 AgentState (包含对话历史和工具输出)\nclass AgentState:\n    messages: list[dict] = []\n    tool_output: str = None\n    next_action: str = None # 例如 \"search\", \"answer\", \"finalize\"\n\n# 定义 Node 函数 - 决策下一步行动\ndef agent_decision_node(state):\n    messages = state.get(\"messages\", [])\n    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n    prompt = PromptTemplate.from_template(\"\"\"\n    你是一个 Agent，根据对话历史决定下一步行动。\n    可选行动： search, answer, finalize\n    对话历史：{messages}\n    请输出下一步行动 (search, answer, finalize):\n    \"\"\")\n    chain = LLMChain(llm=llm, prompt=prompt)\n    next_action = chain.run(messages=messages)\n    return {\"next_action\": next_action}\n\n# 定义 Node 函数 - 使用搜索工具\ndef search_tool_node(state):\n    messages = state.get(\"messages\", [])\n    query = messages[-1][\"content\"] # 假设最后一条消息是用户问题\n    search_tool = DuckDuckGoSearchRun()\n    tool_output = search_tool.run(query)\n    return {\"tool_output\": tool_output}\n\n# 定义 Node 函数 - 生成最终答案\ndef generate_answer_node(state):\n    messages = state.get(\"messages\", [])\n    tool_output = state.get(\"tool_output\")\n    llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n    prompt = PromptTemplate.from_template(\"\"\"\n    根据搜索结果回答用户问题。\n    用户问题：{user_question}\n    搜索结果：{tool_output}\n    请生成答案：\n    \"\"\")\n    chain = LLMChain(llm=llm, prompt=prompt)\n    answer = chain.run(user_question=messages[-1][\"content\"], tool_output=tool_output)\n    return {\"messages\": messages + [{\"role\": \"assistant\", \"content\": answer}]}\n\n# 创建状态图\ngraph = StateGraph(AgentState)\n\n# 添加 Node\ngraph.add_node(\"decision\", agent_decision_node)\ngraph.add_node(\"search\", search_tool_node)\ngraph.add_node(\"answer\", generate_answer_node)\n\n# 定义 Edge 和条件边 (简化示例)\ngraph.set_entry_point(\"decision\")\ngraph.add_edge(\"decision\", \"search\") # 假设 always 先 search\ngraph.add_edge(\"search\", \"answer\")\ngraph.set_finish_point(\"answer\")\n\n# 编译图\napp = graph.compile()\n\n# 运行图\ninputs = {\"messages\": [{\"role\": \"user\", \"content\": \"今天北京天气怎么样？\"}]}\nresult = app.invoke(inputs)\nprint(result)\n```\n\n### 5. 工具 (Tools) 相关\n\nLangchain 和 LangGraph 在工具的使用方式上基本相同，都使用 `langchain.tools` 模块提供的工具，或者自定义工具。\n\n**示例 (Langchain & LangGraph 类似):**\n\n```python\nfrom langchain.tools import DuckDuckGoSearchRun, CalculatorInput, CalculatorRun\n\n# 搜索工具\nsearch_tool = DuckDuckGoSearchRun()\nsearch_output = search_tool.run(\"最新的 Langchain 版本\")\nprint(f\"搜索结果: {search_output}\")\n\n# 计算器工具\ncalculator_tool = CalculatorRun()\ncalculator_input = CalculatorInput(expression=\"2 + 2\")\ncalculator_output = calculator_tool.run(calculator_input) # 或 calculator_tool.run(\"2 + 2\")\nprint(f\"计算结果: {calculator_output}\")\n```\n\n在 Langchain Agent 中，工具会传递给 `AgentExecutor`。在 LangGraph 中，工具可以在 Node 函数中直接调用。\n\n### 6. 状态管理 (State Management) (LangGraph 特有)\n\nLangGraph 通过 `StateGraph` 和自定义的 `AgentState` 类来显式地管理状态。状态在图的节点之间传递和更新。`AgentState` 可以自定义，包含任何需要跟踪的信息，例如对话历史、工具输出、中间结果、用户偏好等。状态管理是 LangGraph 的核心优势之一，使得构建复杂的、状态化的 Agent 和工作流程成为可能。\n\n**(在之前的 LangGraph Node 和 Agent 示例中已经展示了状态管理的使用，例如 `AgentState` 类和 Node 函数中对 `state` 参数的访问和更新。)**\n\n### 7. 图结构构建 (Graph Construction) (LangGraph 特有)\n\nLangGraph 通过 `StateGraph` 类提供的方法来构建图结构：`add_node()`, `add_edge()`, `set_entry_point()`, `set_conditional_edges()`, `set_finish_point()` 等方法用于定义节点、边和图的流程控制逻辑。这种显式的图结构构建方式使得 LangGraph 能够表达非常复杂的流程，例如循环、条件分支、并行处理等。\n\n## 三、`run` 与 `invoke` 的区别\n\n`run` 和 `invoke` 是 Langchain 和 LangGraph 中用于执行 Chain 或 Runnable 的两种方法，它们之间存在一些关键的区别：\n\n*   **输入类型:** `run` 主要接受单个字符串作为输入，而 `invoke` 接受一个字典 (dict) 作为输入，允许更结构化的输入数据。\n*   **输出类型:** `run` 主要返回单个字符串作为输出，而 `invoke` 返回一个字典 (dict) 作为输出，提供更结构化的输出数据。\n*   **底层机制:** `run` 是 Langchain 较早版本中使用的方法，相对较为简单直接。`invoke` 是 Langchain **Runnable 协议** 的一部分，是更现代、更推荐的方法。`invoke` 更加灵活，支持更复杂的功能，例如异步调用、流式输出、中间结果访问等，并且与 Langchain 的 **Runnable 接口** 更好地集成。\n*   **功能和灵活性:** `run` 功能相对有限，主要用于简单的字符串输入和输出场景。`invoke` 功能更强大，更灵活，支持更丰富的功能。\n*   **是否推荐使用:** `run` 在一些简单的 Langchain 代码示例中仍然可以看到，但 **不推荐在新代码中使用**。**强烈推荐使用 `invoke` (以及 `ainvoke`, `stream`, `astream`)**，它是 Langchain 官方推荐的执行 Runnable 的方法。\n\n**具体示例对比 (以 `LLMChain` 为例):**\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain\n\nllm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\nprompt = PromptTemplate.from_template(\"请用 {language} 向 {name} 问好\")\nchain = LLMChain(llm=llm, prompt=prompt)\n\n# 使用 run (不推荐)\n# output_run = chain.run(\"中文\", name=\"张三\") # 错误使用，run 无法处理多个参数\n# print(f\"run 输出: {output_run}\")\n\n# 使用 invoke (推荐)\noutput_invoke = chain.invoke({\"language\": \"中文\", \"name\": \"李四\"})\nprint(f\"invoke 输出: {output_invoke}\")\n```\n\n**总结表格:**\n\n| 特性           | `run`                                    | `invoke` (及 `ainvoke`, `stream`, `astream`)           |\n| -------------- | ---------------------------------------- | ------------------------------------------------------ |\n| **输入类型**   | 主要接受 **单个字符串**                  | 接受 **字典 (dict)**                                   |\n| **输出类型**   | 主要返回 **单个字符串**                  | 返回 **字典 (dict)**                                   |\n| **底层机制**   | 较早版本的方法，相对简单直接             | **Runnable 协议** 的一部分，更现代、更灵活             |\n| **功能**       | 相对有限                                 | 更强大，支持异步、流式输出、结构化数据、中间结果访问等 |\n| **灵活性**     | 较低，输入输出处理方式局限               | 较高，输入输出处理方式更结构化、更灵活                 |\n| **是否推荐**   | **不推荐**在新代码中使用                 | **强烈推荐** 使用                                      |\n| **异步支持**   | 间接异步 (可能依赖底层 LLM 库的异步支持) | 直接支持异步 (`ainvoke`)                               |\n| **流式输出**   | 不支持直接流式输出                       | 支持流式输出 (`stream`, `astream`)                     |\n| **结构化数据** | 处理结构化数据不方便                     | 更方便处理结构化数据 (通过字典输入输出)                |\n\n## 四、深入理解 Runnable\n\nRunnable 可以理解为 Langchain 框架的核心 **构建模块** 和 **统一接口**，用于构建复杂的 LLM 应用流程。Runnable 就像是乐高积木中的一块块标准砖，你可以将它们组合、连接，搭建出各种各样的应用。\n\n**关键点：**\n\n1. **Runnable 是一个接口 (Interface) 或协议 (Protocol):**  定义了一组标准方法，任何实现了这些方法的对象都可以被认为是 \"Runnable\"。\n2. **Runnable 的核心能力：执行和组合:**  提供了一种统一的方式来执行组件，并且可以方便地将它们组合成更复杂的流程。\n3. **Runnable 提供的标准方法:**  `invoke`, `ainvoke`, `stream`, `astream`, `batch`, `abatch`。\n4. **Runnable 的组合示例 (使用 `|` 管道操作符):**\n\n```python\nfrom langchain.prompts import PromptTemplate\nfrom langchain_openai import ChatOpenAI\nfrom langchain.output_parsers import StructuredOutputParser, ResponseSchema\n\n# 1. 定义 ResponseSchema，描述期望的输出结构\nresponse_schemas = [\n    ResponseSchema(name=\"summary\", description=\"对 {topic} 的中文总结\"),\n    ResponseSchema(name=\"keywords\", description=\"关于 {topic} 的三个中文关键词\"),\n]\n\n# 2. 创建 StructuredOutputParser\noutput_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n\n# 3. 创建 PromptTemplate，使用 ResponseSchema 的格式指令，并接受 topic 和 language 作为输入变量\nprompt_template = PromptTemplate(\n    template=\"请用 {language} 总结关于 {topic} 的内容，并提取关键词。\\n{format_instructions}\\n\",\n    input_variables=[\"language\", \"topic\"],\n    partial_variables={\"format_instructions\": output_parser.get_format_instructions()} # 注入格式指令\n)\n\n# 4. 创建 ChatOpenAI 实例\nllm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n\n# 5. 组装 Runnable Chain： PromptTemplate -> LLM -> OutputParser\nrunnable_chain = prompt_template | llm | output_parser\n\n# 6. 定义输入数据\ninput_data = {\"topic\": \"猫\", \"language\": \"中文\"}\n\n# 7. 使用 invoke 执行整个链\noutput = runnable_chain.invoke(input_data)\n\n# 8. 打印输出结果\nprint(output)\n```\n\n## 五、理解 `partial_variables`\n\n`partial_variables` 在 Langchain 的 `PromptTemplate` 中是一个非常有用的特性，它允许你 **预先填充 (partially fill)** Prompt 模板中的 **一部分变量**，而将 **另一部分变量** 留到 **后续使用时再动态填充**。\n\n**关键点：**\n\n*   **Partial (部分的):** 只填充 *一部分* 变量。\n*   **Variables (变量):** Prompt 模板中用花括号 `{}` 包裹的占位符。\n*   **Pre-set (预设的):**  `partial_variables` 的值在 **创建 `PromptTemplate` 对象时就被设定好**。\n\n**作用和用途：**\n\n1. **固定 Prompt 模板的一部分内容。**\n2. **简化 Prompt 模板的输入。**\n3. **注入上下文信息或指令。**\n\n**示例解释:**\n\n```python\nprompt_template = PromptTemplate(\n    template=\"请用 {language} 总结关于 {topic} 的内容，并提取关键词。\\n{format_instructions}\\n\",\n    input_variables=[\"language\", \"topic\"], # 声明了需要动态填充的变量\n    partial_variables={\"format_instructions\": output_parser.get_format_instructions()} # 预先填充 format_instructions\n)\n```\n\n在这个例子中，`format_instructions` 已经被预先设定，你只需要在 `format()` 或 `invoke()` 时 **提供 `language` 和 `topic` 的值** 即可。\n\n## 六、Agent 和 AgentExecutor 详解\n\n### 1. Agent vs. AgentExecutor\n\n*   **Agent (代理): 决策者和规划者 (The Brain)**\n    *   **核心职责:** **决定下一步应该做什么**。\n    *   **思考过程:** Agent 内部通常包含一个 **Prompt (提示词)** 和一个 **LLM (语言模型)**。\n    *   **输出:** **行动计划 (Action Plan)** 或 **下一步的指令**。\n    *   **比喻:**  `Agent` 就像一个 **架构师** 或 **项目经理**。\n\n*   **AgentExecutor (代理执行器): 执行者和协调者 (The Body)**\n    *   **核心职责:** **执行 Agent 制定的行动计划** 并 **协调整个 Agent 的运行流程**。\n    *   **执行流程:** `AgentExecutor` 的执行流程通常是一个 **循环 (Loop)**。\n    *   **状态管理:** `AgentExecutor` 通常还负责 **管理 Agent 的运行状态**。\n    *   **比喻:** `AgentExecutor` 就像一个 **施工队** 或 **执行团队**。\n\n**关键区别总结:**\n\n| 特性             | Agent (代理)                                      | AgentExecutor (代理执行器)                                  |\n| ---------------- | ------------------------------------------------- | ----------------------------------------------------------- |\n| **角色**         | **决策者、规划者 (The Brain)**                    | **执行者、协调者 (The Body)**                               |\n| **核心职责**     | **决定下一步做什么 (What to do next?)**           | **执行行动计划，协调流程 (How to execute the plan?)**       |\n| **内部组件**     | Prompt (提示词) + LLM (语言模型)                  | Agent + Tools (工具) +  (可选) Memory (记忆) + 执行循环逻辑 |\n| **输入**         | 用户输入、工具、有时记忆                          | Agent 的行动计划、工具、当前状态                            |\n| **输出**         | 行动计划 (Action Plan) 或 最终答案 (Final Answer) | 最终答案 (Final Answer)                                     |\n| **是否直接执行** | **否**，需要 `AgentExecutor` 来执行               | **是**，可以使用 `run` 或 `invoke` 方法直接执行             |\n\n### 2. `create_react_agent` vs. `create_with_tools_agent`\n\n这两个函数都是 Langchain 中用于 **创建 Agent 的辅助函数**，它们的主要区别在于它们 **创建的 Agent 类型** 和 **使用的 Prompt 策略** 不同：\n\n*   **`create_react_agent`:**\n    *   **创建类型:** **ReAct Agent** (implicitly)。\n    *   **Prompt 策略:** **ReAct (Reasoning and Acting)**。\n    *   **Prompt 定制:** 允许可选地传入自定义的 `prompt`，但应该遵循 ReAct 风格。\n    *   **适用场景:** 快速创建一个基于 ReAct 策略的 Agent。\n\n*   **`create_with_tools_agent`:**\n    *   **创建类型:** **通用 Agent** (explicitly controlled by prompt)。\n    *   **Prompt 策略:** **用户自定义 Prompt 策略**。\n    *   **Prompt 定制:** 强制要求提供 `prompt` 参数，完全定义 Agent 的行为。\n    *   **适用场景:** 需要对 Agent 的行为进行高度定制，或者尝试不同的 Prompt 策略。\n\n**关键区别总结:**\n\n| 特性                  | `create_react_agent`                                   | `create_with_tools_agent`                                    |\n| --------------------- | ------------------------------------------------------ | ------------------------------------------------------------ |\n| **Agent 类型**        | **ReAct Agent** (隐式创建)                             | **通用 Agent** (Prompt 完全控制)                             |\n| **Prompt 策略**       | **ReAct 策略** (默认或自定义 ReAct 风格 Prompt)        | **用户自定义 Prompt 策略** (可以是任何风格)                  |\n| **Prompt 定制程度**   | **较低**，主要使用预定义的 ReAct Prompt，可微调        | **较高**，完全由用户提供 Prompt，灵活度高                    |\n| **Prompt 参数**       | `prompt` 参数 **可选** (不提供则使用默认 ReAct Prompt) | `prompt` 参数 **必选** (用户必须提供 Prompt)                 |\n| **使用场景**          | **快速创建 ReAct Agent，简化 ReAct Agent 构建**        | **高度定制 Agent 行为，尝试不同 Prompt 策略，非 ReAct Agent** |\n| **上手难度 (Prompt)** | **较低**，ReAct Prompt 结构相对固定，易于理解和使用    | **较高**，需要用户自行设计 Prompt 策略，Prompt 工程难度较高  |\n\n### 3. Agent 中 `tools` 和 AgentExecutor 中 `tools` 的区别\n\n*   **`Agent` 中的 `tools`:**  让 `Agent` 知道有哪些工具可以使用，以及每个工具的功能和描述。Agent 实际上并不 *直接* 执行 `tools`，它只是 *思考* 和 *计划* 如何使用 `tools`。\n*   **`AgentExecutor` 中的 `tools`:** 让 `AgentExecutor` 拥有 *实际可执行的工具对象*。`AgentExecutor` 负责 *执行* `Agent` 输出的 Action (行动)。\n\n**总结来说，`tools` 传递两次，但目的是不同的：**\n\n*   **第一次传给 `Agent`：** 为了让 `Agent` **了解工具信息，用于思考和决策 (Planning & Reasoning)**。传递的是工具的 *描述性信息*。\n*   **第二次传给 `AgentExecutor`：** 为了让 `AgentExecutor` **拥有可执行的工具实例，用于实际执行 Agent 的行动计划 (Execution)**。传递的是工具的 *可执行对象*。\n\n### 4. AgentExecutor 的创建方法\n\n除了 `AgentExecutor.from_agent_and_tools`，还有以下几种创建方法：\n\n*   **`AgentExecutor(...)` (直接使用构造函数):**  更底层和灵活，可以直接控制 `AgentExecutor` 的各个参数。\n*   **`initialize_agent(...)` (更早版本的方法，逐渐被 `create_xxx_agent` + `from_agent_and_tools` 替代):**  一步创建 `Agent` 和 `AgentExecutor`，灵活性稍差，逐渐被替代。\n\n**最佳实践建议:**\n\n**对于新的 Langchain Agent 项目，强烈推荐使用 `create_xxx_agent` (例如 `create_react_agent`, `create_conversational_agent` 等) 创建 `Agent`，然后使用 `AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, ...)` 创建 `AgentExecutor` 的方式。**\n\n### 5. Agent 和 LLM 的分离\n\n将 `Agent` 和 `LLM` 分开设计带来了 **模块化、灵活性、可配置性、概念清晰** 等多方面的优势。这种分离使得 Langchain 能够更好地利用 LLM 的强大能力，构建出更强大、更灵活、更易于维护的 Agent 应用。\n\n### 6. `AgentExecutor.from_agent_and_tools` 不接受 `llm` 参数\n\n这是因为 `Agent` 对象在创建时就已经被赋予了 `llm`，`AgentExecutor` 通过 `agent` 参数间接获得了所需的 `llm`，无需再次传入。这种参数设计体现了 Langchain Agent 架构的职责分离和模块化原则。\n\n## 七、Memory vs. StateGraph\n\n| 特性              | Langchain `Memory`                                           | LangGraph `StateGraph` / `AgentState`                        |\n| ----------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| **核心目的**      | **管理对话历史 (Conversational History)**，主要用于记住对话上下文，使 Chain 或 Agent 具有对话记忆能力。 | **管理更广泛的应用状态 (Application State)**，不仅限于对话历史，可以跟踪任何需要在图执行过程中维护和传递的数据。 |\n| **状态类型**      | 主要关注 **消息 (Messages)** 序列，通常是对话的 `HumanMessage` 和 `AIMessage`。 | **自定义的状态类 `AgentState`**，可以包含任何类型的数据，例如消息历史、工具输出、中间结果、用户偏好、计数器等。 |\n| **作用范围**      | 主要作用于 **Chain 或 AgentExecutor 级别**，为单个 Chain 或 Agent 提供记忆能力。 | 作用于 **整个 LangGraph 图级别**，状态在图中的各个节点之间传递和更新，影响整个图的执行流程。 |\n| **数据结构**      | 通常使用 **列表 (List)** 来存储消息历史，例如 `ConversationBufferMemory` 使用消息列表。 | 使用 **自定义的 `AgentState` 类**，其内部数据结构可以根据需求灵活定义，例如可以使用字典、列表、对象属性等。 |\n| **灵活性/定制性** | 相对 **较低**，主要关注对话历史管理，提供的 Memory 类型和配置选项相对有限。 | 相对 **较高**，`AgentState` 是完全自定义的，可以根据应用需求设计任何复杂的状态结构和管理逻辑。 |\n| **复杂性**        | 相对 **简单**，易于上手和使用，配置和集成比较直接。          | 相对 **复杂**，需要理解图的概念、状态类定义、节点间状态传递等，学习曲线稍高。 |\n| **适用场景**      | **对话型应用 (Chatbots, Conversational Agents)**，需要记住对话上下文以实现连贯对话。 | **复杂状态化应用 (Complex Agents, Multi-step Workflows, Decision Processes)**，需要跟踪和管理多维度的状态信息，并基于状态控制流程。 |\n| **与流程控制**    | 主要影响 Chain 或 Agent 的输入，间接影响流程，但本身不直接控制流程。 | **直接控制图的执行流程**，状态变化可以决定图的下一步走向 (通过条件边等)，是图流程控制的核心组成部分。 |\n\n**如何选择:**\n\n*   如果你的应用是简单的对话型应用，只需要基本的对话记忆功能，并且希望快速上手，那么 Langchain `Memory` 通常是更合适的选择。\n*   如果你的应用是复杂的 Agent 或工作流程，需要管理多维度的状态信息，并需要基于状态进行流程控制，那么 LangGraph `StateGraph` / `AgentState` 是更强大的选择。\n\n## 八、Agent 嵌套\n\nAgent 中可以嵌套 Agent，实现的核心思路是： **将一个 Agent 封装成一个 Langchain Tool，然后让另一个 Agent 可以使用这个 Tool**。\n\n**代码示例 (概念性示例):**\n\n```python\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import create_react_agent, AgentExecutor, Tool\nfrom langchain.prompts import PromptTemplate\n\n# 1. 创建内层 Agent (Inner Agent) - 假设是一个简单的总结 Agent\ninner_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.0)\ninner_prompt_template = PromptTemplate.from_template(\"请总结以下内容：{text}\")\ninner_agent = create_react_agent(llm=inner_llm, tools=[], prompt=inner_prompt_template) # Inner Agent 没有 tools\ninner_agent_executor = AgentExecutor.from_agent_and_tools(agent=inner_agent, tools=[], verbose=False)\n\n# 2. 创建 Agent Tool - 封装内层 Agent\nclass SummaryAgentTool(Tool):\n    name = \"SummaryAgent\"\n    description = \"用于总结文本内容的 Agent. 输入应该是要总结的文本。\"\n\n    def _run(self, text: str) -> str:\n        \"\"\"使用 Summary Agent\n","tags":["Agent","LLMs","LangChain","LangGraph"],"categories":["LLM"]},{"title":"Feature Engineering by Python and Pyspark","url":"/2025/01/24/Feature_Engineering_by_Python_and_Pyspark/","content":"\n特征工程是机器学习流程中至关重要的环节，直接影响模型性能。以下是详细的步骤说明及对应的Python/PySpark实现代码：\n\n---\n\n### **一、数据理解与探索**\n#### 1. 数据概览\n```python\n# Python (Pandas)\nimport pandas as pd\ndf = pd.read_csv('data.csv')\nprint(df.head())        # 查看前5行\nprint(df.info())        # 数据类型和缺失值统计\nprint(df.describe())    # 数值型特征统计分布\n\n# PySpark\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\ndf = spark.read.csv('data.csv', header=True, inferSchema=True)\ndf.printSchema()        # 查看数据模式\ndf.show(5)              # 显示前5行\ndf.describe().show()    # 统计分布\n```\n\n#### 2. 可视化分析\n```python\n# Python (Matplotlib/Seaborn)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 数值型特征分布\nsns.histplot(df['age'], kde=True)\nplt.show()\n\n# 类别型特征分布\nsns.countplot(x='gender', data=df)\nplt.show()\n\n# 相关性矩阵\ncorr_matrix = df.corr()\nsns.heatmap(corr_matrix, annot=True)\nplt.show()\n```\n\n---\n\n### **二、数据清洗**\n#### 1. 处理缺失值\n```python\n# Python (Pandas)\n# 删除缺失值\ndf.dropna(subset=['age'], inplace=True)\n\n# 填充缺失值（均值/众数）\ndf['income'].fillna(df['income'].mean(), inplace=True)\ndf['gender'].fillna(df['gender'].mode()[0], inplace=True)\n\n# PySpark\nfrom pyspark.sql.functions import mean, col\n\n# 计算均值\nmean_income = df.select(mean(col('income'))).collect()[0][0]\ndf = df.na.fill({'income': mean_income})\n\n# 删除缺失值\ndf = df.na.drop(subset=['age'])\n```\n\n#### 2. 处理异常值\n```python\n# Python (Pandas)\n# Z-Score方法\nfrom scipy import stats\nz_scores = stats.zscore(df['income'])\ndf = df[(z_scores < 3) & (z_scores > -3)]\n\n# IQR方法\nQ1 = df['age'].quantile(0.25)\nQ3 = df['age'].quantile(0.75)\nIQR = Q3 - Q1\ndf = df[~((df['age'] < (Q1 - 1.5*IQR)) | (df['age'] > (Q3 + 1.5*IQR)))]\n\n# PySpark\nfrom pyspark.sql.functions import avg, stddev\n\n# 计算统计量\nstats = df.select(avg('age'), stddev('age')).collect()\nmean_age = stats[0][0]\nstd_age = stats[0][1]\ndf = df.filter((col('age') > mean_age - 3*std_age) & (col('age') < mean_age + 3*std_age))\n```\n\n---\n\n### **三、特征构建**\n#### 1. 数值型特征处理\n```python\n# Python (Pandas)\n# 分箱（离散化）\ndf['age_bin'] = pd.cut(df['age'], bins=[0, 18, 35, 60, 100], labels=['child', 'youth', 'adult', 'senior'])\n\n# 多项式特征\nfrom sklearn.preprocessing import PolynomialFeatures\npoly = PolynomialFeatures(degree=2, interaction_only=True)\npoly_features = poly.fit_transform(df[['age', 'income']])\n\n# PySpark\nfrom pyspark.ml.feature import Bucketizer\n\n# 分箱\nbucketizer = Bucketizer(splits=[0, 18, 35, 60, 100], inputCol='age', outputCol='age_bin')\ndf = bucketizer.transform(df)\n```\n\n#### 2. 类别型特征编码\n```python\n# Python (Pandas)\n# One-Hot Encoding\ndf = pd.get_dummies(df, columns=['gender'])\n\n# Label Encoding\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndf['city'] = le.fit_transform(df['city'])\n\n# PySpark\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder\n\n# StringIndexer\nindexer = StringIndexer(inputCol='gender', outputCol='gender_index')\ndf = indexer.fit(df).transform(df)\n\n# OneHotEncoder\nencoder = OneHotEncoder(inputCol='gender_index', outputCol='gender_vec')\ndf = encoder.fit(df).transform(df)\n```\n\n#### 3. 时间序列特征\n```python\n# Python (Pandas)\ndf['date'] = pd.to_datetime(df['date'])\ndf['year'] = df['date'].dt.year\ndf['day_of_week'] = df['date'].dt.dayofweek\n\n# 滑动窗口统计\ndf['7d_avg'] = df['sales'].rolling(window=7).mean()\n\n# PySpark\nfrom pyspark.sql.functions import year, dayofweek, window\n\ndf = df.withColumn('year', year('date'))\ndf = df.withColumn('day_of_week', dayofweek('date'))\n\n# 窗口函数\nfrom pyspark.sql.window import Window\nwindow_spec = Window.orderBy('date').rowsBetween(-6, 0)\ndf = df.withColumn('7d_avg', avg('sales').over(window_spec))\n```\n\n---\n\n### **四、特征变换**\n#### 1. 标准化/归一化\n```python\n# Python (Scikit-Learn)\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nscaler = StandardScaler()\ndf['income_scaled'] = scaler.fit_transform(df[['income']])\n\nminmax = MinMaxScaler()\ndf['age_normalized'] = minmax.fit_transform(df[['age']])\n\n# PySpark\nfrom pyspark.ml.feature import StandardScaler, MinMaxScaler\nfrom pyspark.ml import Pipeline\n\nscaler = StandardScaler(inputCol='income', outputCol='income_scaled')\npipeline = Pipeline(stages=[scaler])\ndf = pipeline.fit(df).transform(df)\n```\n\n#### 2. 非线性变换\n```python\n# Python (Pandas)\nimport numpy as np\n\ndf['log_income'] = np.log1p(df['income'])\ndf['sqrt_age'] = np.sqrt(df['age'])\n\n# PySpark\nfrom pyspark.sql.functions import log1p, sqrt\n\ndf = df.withColumn('log_income', log1p('income'))\ndf = df.withColumn('sqrt_age', sqrt('age'))\n```\n\n---\n\n### **五、特征选择**\n#### 1. 过滤法\n```python\n# Python (Scikit-Learn)\nfrom sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif\n\n# 低方差过滤\nselector = VarianceThreshold(threshold=0.1)\ndf_selected = selector.fit_transform(df)\n\n# 基于统计检验\nselector = SelectKBest(f_classif, k=10)\nX_new = selector.fit_transform(X, y)\n\n# PySpark\nfrom pyspark.ml.feature import ChiSqSelector\n\nselector = ChiSqSelector(numTopFeatures=10, featuresCol='features', outputCol='selected', labelCol='label')\nmodel = selector.fit(df)\ndf = model.transform(df)\n```\n\n#### 2. 包裹法（递归特征消除）\n```python\n# Python (Scikit-Learn)\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\nrfe = RFE(estimator=LogisticRegression(), n_features_to_select=5)\nX_rfe = rfe.fit_transform(X, y)\n```\n\n#### 3. 嵌入法（基于模型）\n```python\n# Python (Scikit-Learn)\nfrom sklearn.ensemble import RandomForestClassifier\n\nmodel = RandomForestClassifier()\nmodel.fit(X, y)\nimportances = model.feature_importances_\n```\n\n---\n\n### **六、特征降维**\n#### 1. PCA\n```python\n# Python (Scikit-Learn)\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=0.95)  # 保留95%方差\nX_pca = pca.fit_transform(X)\n\n# PySpark\nfrom pyspark.ml.feature import PCA\n\npca = PCA(k=3, inputCol='features', outputCol='pca_features')\nmodel = pca.fit(df)\ndf = model.transform(df)\n```\n\n#### 2. t-SNE\n```python\n# Python (Scikit-Learn)\nfrom sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=2)\nX_tsne = tsne.fit_transform(X)\n```\n\n---\n\n### **七、特征存储**\n#### 保存处理后的数据\n```python\n# Python (Pandas)\ndf.to_parquet('processed_data.parquet')\n\n# PySpark\ndf.write.parquet('hdfs://path/to/processed_data')\n```\n\n---\n\n### **八、高级技巧**\n#### 1. 自动化特征工程\n```python\n# 使用FeatureTools\nimport featuretools as ft\n\nes = ft.EntitySet(id='data')\nes = es.entity_from_dataframe(entity_id='main', dataframe=df, index='id')\n\n# 自动生成特征\nfeature_matrix, features = ft.dfs(entityset=es, target_entity='main', max_depth=2)\n```\n\n#### 2. 处理高基数类别特征\n```python\n# 目标编码（Target Encoding）\nfrom category_encoders import TargetEncoder\n\nencoder = TargetEncoder()\ndf['city_encoded'] = encoder.fit_transform(df['city'], y)\n\n# PySpark实现需手动编码\nfrom pyspark.sql.functions import mean\n\ntarget_mean = df.groupBy('city').agg(mean('label').alias('city_target_encoded'))\ndf = df.join(target_mean, on='city', how='left')\n```\n\n---\n\n### **总结**\n特征工程需要结合具体业务场景灵活调整，核心原则包括：\n1. 充分理解数据分布和业务含义\n2. 优先处理数据质量问题（缺失值、异常值）\n3. 通过特征交叉、变换挖掘隐藏信息\n4. 使用自动化工具加速实验过程\n5. 持续监控特征在生产环境的表现\n\n建议将特征工程代码封装为可复用的Pipeline，便于后续维护和自动化部署。\n","tags":["Python","PySpark"],"categories":["ML"]},{"title":"Machine Leaning (lightGBM)","url":"/2025/01/24/machine_leaning_lightgbm/","content":"\nOne of the go-to algorithms for handling large datasets efficiently and accurately is **LightGBM**. \n\n### **What is LightGBM?**\n\nLightGBM, which stands for **Light Gradient Boosting Machine**, is a powerful and popular gradient boosting framework developed by Microsoft. It's designed for distributed and efficient training of decision tree-based models, especially when dealing with:\n\n*   **Large datasets:**  It excels at handling datasets with millions of rows and numerous features.\n*   **High dimensionality:** It can effectively work with datasets having a high number of features.\n*   **Speed and efficiency:** It's known for its fast training speed and low memory consumption compared to other gradient boosting libraries like XGBoost.\n\n### **How LightGBM Works (Conceptual Overview):**\n\nLike other gradient boosting algorithms, LightGBM builds an **ensemble of decision trees sequentially**. Here's a simplified breakdown:\n\n1. **Boosting:** It combines multiple \"weak\" learners (decision trees in this case) to create a strong predictive model.\n2. **Gradient Descent:** Each tree is trained to correct the errors made by the previous trees. It does this by focusing on the *gradient* of the loss function, which indicates the direction of steepest descent towards minimizing the error. (it leverages **both first-order and second-order gradients** to improve the optimization process)\n3. **Sequential Tree Building:** Trees are added one at a time. Each new tree tries to minimize the residual errors from the previous combination of trees.\n\n### **Key Innovations and Advantages of LightGBM:**\n\nLightGBM's efficiency and performance stem from several key innovations:\n\n#### **Gradient-based One-Side Sampling (GOSS):**\n\n*   **Problem:** In traditional gradient boosting, all data instances are used to calculate information gain when splitting nodes. This is computationally expensive.\n*   **Solution:** GOSS keeps instances with large gradients (meaning they have larger errors and are more important for learning) and randomly samples instances with small gradients. This reduces the number of data instances used for calculating gain without sacrificing accuracy significantly.\n*   **Analogy:** Imagine analyzing our app user data. GOSS would prioritize users who show unusual behavior (e.g., high churn risk or unexpectedly high engagement), while still considering a sample of the \"regular\" users. This allows us to focus on the most informative data points.\n\n**The Problem GOSS Solves:**\n\nImagine we're trying to predict user churn for our app. We have a massive dataset with millions of users and their corresponding features (usage patterns, demographics, etc.). In traditional gradient boosting, when building each decision tree, the algorithm needs to evaluate the *information gain* for every possible split point across all features and *all users*. This is computationally expensive and time-consuming.\n\nWhy is evaluating all users a problem? Because not all users are equally informative for learning. Some users' behavior might be perfectly predicted by the existing trees (small errors/gradients), while others might be poorly predicted (large errors/gradients). Spending equal computational effort on both groups is inefficient.\n\n**GOSS to the Rescue: Focus on the Important Users**\n\nGOSS provides a clever solution by intelligently sampling users during the tree-building process. It focuses on the users that are most informative for learning—those with larger gradients. Here's how it works:\n\n**Steps in GOSS:**\n\n1. **Calculate Gradients:** For each user, we calculate the gradient of the loss function. This gradient essentially represents how \"wrong\" the current model's prediction is for that user. A larger gradient means a larger error and a greater need for the next tree to correct it.\n   *   **Analogy:** Think of the gradient as the \"degree of surprise\" for each user. A user who consistently uses the app as predicted has a small gradient (low surprise). A user whose behavior deviates significantly from the prediction has a large gradient (high surprise).\n\n2. **Sort by Gradients:** We sort the users in descending order based on the absolute value of their gradients. This puts the users with the largest errors (and thus, most informative for learning) at the top.\n\n3. **Keep Top `a%`:** We select the top `a%` of users with the largest gradients. These are the users whose behavior is currently poorly predicted and most crucial for improvement.\n   *   **Example:** Let's say `a = 10`. We keep the top 10% of users with the largest gradients. If we have 1 million users, we keep 100,000 of them.\n\n4. **Randomly Sample `b%` from the Rest:** From the remaining users (those with smaller gradients), we randomly sample `b%` of them. This ensures that we don't completely ignore the users with smaller errors. We might still learn something from them, just not as much as from those with large errors.\n   *   **Example:** Let's say `b = 20`. From the remaining 900,000 users (1,000,000 - 100,000), we randomly sample 180,000 (900,000 * 0.20).\n\n5. **Amplify the Sampled Small Gradients:** To compensate for the under-sampling of users with small gradients, we amplify their weights when calculating information gain during tree splitting. We multiply their gradients by a constant factor `(1-a)/b`. This ensures that their contribution to the overall learning process is not diminished.\n   *   **Example:** Continuing with `a = 10` and `b = 20`, the amplification factor would be (1 - 0.1) / 0.2 = 4.5. So, when calculating information gain, the gradients of the 180,000 sampled users with small gradients are multiplied by 4.5.\n\n**Why This Works:**\n\n*   **Focus on the Most Informative Data:** By prioritizing users with large gradients, GOSS concentrates on the areas where the model is currently weakest, leading to faster learning and improvement.\n*   **Reduced Computation:**  Instead of using all users, we're now using only `a% + (1-a%)*b%` of them for calculating information gain. In our example, that's 100,000 + 180,000 = 280,000 users instead of 1,000,000. This significantly reduces computational cost.\n*   **Accuracy Maintained:** While we are sampling, the amplification of small gradients ensures that we don't lose too much information from the less \"surprising\" users. The balance between keeping top gradients and sampling the rest, combined with amplification, maintains accuracy while boosting efficiency.\n\n**Analogy in our App Context:**\n\nLet's say we have users who:\n\n*   **Group A (Large Gradients):** Users who are exhibiting behavior that strongly suggests they might churn (e.g., declining usage, infrequent logins, negative feedback). These are our \"high surprise\" users.\n*   **Group B (Small Gradients):** Users who are behaving as expected (e.g., regular usage, consistent engagement). These are our \"low surprise\" users.\n\nGOSS would:\n\n1. **Prioritize Group A:** Keep all users in Group A for analysis because their behavior needs to be better understood to prevent churn.\n2. **Sample Group B:** Take a random sample from Group B, as their behavior is already well-modeled.\n3. **Amplify Group B's Influence:** When calculating how good a potential split is in a decision tree, give the sampled users from Group B more weight to compensate for the fact that we didn't include all of them.\n\n#### **Exclusive Feature Bundling (EFB):**\n\n*   **Problem:** High-dimensional datasets often have many sparse features (features with many zero values). This sparsity wastes computational resources.\n*   **Solution:** EFB bundles mutually exclusive features (features that rarely take non-zero values simultaneously) into a single feature automatically. This reduces the number of features without significant information loss.\n*   **Analogy:** In our app, features like \"clicked on ad A\" and \"clicked on ad B\" might be mutually exclusive if a user can only click on one ad at a time. EFB would bundle them into a single feature like \"clicked on an ad\".\n\n#### **Leaf-wise (Best-first) Tree Growth:**\n\n*   **Problem:** Traditional level-wise tree growth (like in XGBoost) can be inefficient as it splits nodes at the same level even if some nodes don't contribute much to reducing the error.\n*   **Solution:** LightGBM uses leaf-wise growth. It chooses the leaf node with the highest gain to split, regardless of its level. This leads to faster convergence and potentially more accurate trees.\n*   **Analogy:** Instead of analyzing all user segments equally, we prioritize analyzing the segment that shows the most significant potential for improvement (e.g., the segment with the highest churn rate).\n\n#### Leaf-wise vs level-wise\n\nThe core difference lies in *how* the decision tree expands, both of them use the Gain calculated by gradient and second-order gradient to determine the split node and feature:\n\nthe main difference in the Gain calcualtion:\n\n| 特性             | LightGBM                              | XGBoost                        |\n| :--------------- | :------------------------------------ | :----------------------------- |\n| **树生长策略**   | 默认使用 Leaf-wise 生长策略           | 默认使用 Level-wise 生长策略   |\n| **特征分裂算法** | 基于直方图的近似算法                  | 支持精确贪心算法和近似算法     |\n| **处理稀疏数据** | 使用 Exclusive Feature Bundling (EFB) | 使用稀疏感知算法               |\n| **并行化**       | 基于特征的并行化                      | 基于特征的并行化和数据的并行化 |\n\n**1. Level-wise (Traditional Approach - e.g., XGBoost):**\n\n*   **How it grows:** The tree grows level by level. In each round, it splits *all* the nodes at the current level before moving to the next level. It's a breadth-first approach.\n*   **Analogy:** Imagine analyzing all user segments (e.g., new users, active users, dormant users) simultaneously in each round of analysis, regardless of whether some segments are already well-understood or less critical to analyze at that point.\n*   **Drawback:** This can be inefficient because some nodes at a given level might not contribute much to reducing the error. Splitting them is computationally wasteful and can lead to overfitting, especially in deeper levels where data becomes scarce.\n\n```\n        Level 0: [A]\n        /         \\\nLevel 1: [B]       [C]\n      /   \\       /   \\\nLevel 2: [D] [E] [F] [G]\n```\n\n**2. Leaf-wise (LightGBM's Approach):**\n\n*   **How it grows:** The tree grows by splitting the leaf node with the *highest delta loss* (or *maximum gain*) *regardless of its level*. It's a best-first approach, driven by maximizing improvement.\n*   **Analogy:**  Instead of analyzing all user segments equally, we focus our attention on the segment that currently exhibits the most significant potential for improvement. For example, if our analysis shows that the segment with the highest churn rate is \"users who signed up last week but haven't used a key feature,\" we would prioritize analyzing this segment further in the next round to understand the reasons behind their churn and develop targeted interventions.\n*   **Advantages:**\n    *   **Faster Convergence:** By focusing on the most promising splits, leaf-wise growth often leads to faster convergence, meaning we reach a good model with fewer splits.\n    *   **Potentially More Accurate Trees:**  It tends to create deeper, more asymmetric trees that can capture complex patterns in the data more effectively.\n    *   **Reduced Overfitting (with proper regularization):** By prioritizing high-gain splits, it's less likely to make unnecessary splits in less informative regions of the data, which can help reduce overfitting.\n\n**Leaf-wise Growth in Detail:**\n\n1. **Initialization:** The tree starts with a single root node containing all training data.\n2. **Find Best Leaf to Split:** In each iteration, LightGBM examines all current leaf nodes (nodes without further splits). For each leaf, it calculates the potential *gain* that would result from splitting it further. The gain represents the improvement in the model's accuracy (or reduction in the loss function) if that leaf were split.\n3. **Split the Best Leaf:** LightGBM selects the leaf node with the *highest* gain and splits it. This is the \"best-first\" aspect.\n4. **Repeat:** Steps 2 and 3 are repeated until a stopping criterion is met (e.g., maximum depth, minimum number of samples in a leaf, or no further significant gain).\n\n```\n        Level 0: [A]\n        /\nLevel 1: [B]\n      /\nLevel 2: [C]\n      \\\nLevel 3: [D]\n```\n\n**Analogy: User Churn Prediction**\n\nLet's make the analogy even more concrete:\n\n*   **Level-wise:**\n    *   **Round 1:** Analyze all users (root node). Split them based on, say, \"days since last login\" (level 1).\n    *   **Round 2:** Now, we have two segments. We split *both* based on, say, \"number of sessions\" (level 2), even if the \"recently logged in\" segment is already showing very low churn and further splitting might not be beneficial.\n    *   **Round 3:** We continue splitting *all* resulting segments at the next level, and so on.\n\n*   **Leaf-wise:**\n    *   **Round 1:** Analyze all users (root node). Split them based on \"days since last login.\" Let's say this gives us two segments: \"recent\" and \"dormant.\"\n    *   **Round 2:**  We calculate the potential gain from splitting each of these segments. Let's say splitting the \"dormant\" segment based on \"number of sessions\" has a much higher gain than any split for the \"recent\" segment (because dormant users with few sessions are very likely to churn). LightGBM chooses to split the \"dormant\" segment *only* in this round.\n    *   **Round 3:** Now we have three leaf nodes: \"recent,\" \"dormant & few sessions,\" and \"dormant & many sessions.\" We again calculate the potential gain for each. Perhaps splitting \"dormant & few sessions\" based on \"used feature X\" yields the highest gain. We split that node.\n    *   **And so on...** We keep prioritizing the leaf node that offers the most significant improvement in predicting churn.\n\n**Important Considerations:**\n\n*   **Overfitting:** While leaf-wise growth can be more accurate, it can also be more prone to overfitting if not properly controlled.\n*   **Regularization:** LightGBM provides parameters like `max_depth`, `min_data_in_leaf`, and `lambda_l1`/`lambda_l2` to regularize the tree growth and prevent overfitting. These parameters limit the tree's depth, ensure a minimum number of samples in each leaf, and add penalties to the loss function for complex trees, respectively. Using these is crucial in practice.\n\n**In Conclusion**\n\nLeaf-wise tree growth is a core innovation in LightGBM that contributes significantly to its efficiency and often leads to more accurate models. By strategically focusing on the most informative splits, it allows us to build powerful models for tasks like user churn prediction, enabling us to derive valuable insights and take proactive measures to improve user retention in our app. The analogy of prioritizing user segments based on their potential for improvement clearly illustrates the power of this \"best-first\" approach.\n\n> - **Each round involves evaluating all current leaf nodes.**\n> - **The leaf with the highest potential gain is split.**\n> - **Unsplit nodes are not discarded but are carried over to the next round for re-evaluation.**\n\n#### **Histogram-based Algorithm:**\n\n*   **Problem:** Finding the optimal split point for a feature can be time-consuming, especially for continuous features.\n*   **Solution:** LightGBM discretizes continuous features into bins (histograms). This speeds up the process of finding the best split point by working with a smaller number of discrete values.\n*   **Less Sensitivity to Outliers:** Outliers will be grouped into the extreme bins, reducing their undue influence on the splitting process.\n*   **Data Compression:** Binning effectively compresses the data, leading to lower memory usage, which is especially beneficial for large datasets.\n\n#### **Support for Categorical Features:**\n\n*   **Advantage:** LightGBM can directly handle categorical features without requiring one-hot encoding, which can save time and reduce memory usage, especially when dealing with high-cardinality categorical features. It uses a specialized splitting strategy for categorical features.\n*   **Analogy:** In our user data, features like \"device type\" or \"country\" can be directly processed by LightGBM without needing to create many dummy variables.\n\n### **How LightGBM Can Help Us Improve User Engagement and Retention:**\n\nBy leveraging LightGBM's strengths, we can build powerful models to:\n\n1. **Predict Churn:** By analyzing user behavior patterns (e.g., session duration, feature usage, inactivity periods), we can train a LightGBM model to predict which users are likely to churn. This allows us to proactively target these users with personalized interventions, such as push notifications, special offers, or in-app support.\n2. **Personalize Recommendations:** We can use LightGBM to build a recommendation system that suggests relevant content, features, or products to users based on their past behavior and preferences. This can enhance user engagement and satisfaction.\n3. **Optimize User Interface/User Experience (UI/UX):**  By analyzing user interactions with different UI elements, we can identify areas for improvement. LightGBM can help us build models to predict which UI designs lead to higher engagement and conversion rates.\n4. **Identify Key Drivers of Engagement:** LightGBM's feature importance scores can help us understand which factors are most influential in driving user engagement. This allows us to focus our efforts on optimizing these key drivers.\n5. **Segment Users:** We can use clustering techniques in conjunction with LightGBM to segment users into different groups based on their behavior and characteristics. This enables us to tailor our marketing and product development strategies to each segment's specific needs.\n\n### **LightGBM Parameters (Important Ones):**\n\nTuning these parameters is crucial for optimal performance:\n\n*   **`objective`:** Defines the learning task (e.g., `regression`, `binary`, `multiclass`).\n*   **`boosting_type`:** Specifies the boosting algorithm (e.g., `gbdt`, `goss`, `dart`).\n*   **`num_leaves`:** Controls the complexity of the tree (higher value means more complex trees).\n*   **`learning_rate`:** Determines the step size at each iteration (lower value means slower but potentially more accurate learning).\n*   **`max_depth`:** Limits the maximum depth of a tree (prevents overfitting).\n*   **`feature_fraction`:** Controls the fraction of features used for each tree (prevents overfitting).\n*   `min_data_in_leaf`: Specifies the minimum number of samples in a leaf node, controls the complexity of the tree, helping to prevent over-fitting.\n*   `lambda_l1`, `lambda_l2`: Regularization terms that help reduce overfitting.\n\n### **Code Example (Python):**\n\n```python\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n# Assuming you have your data in X (features) and y (target variable)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create LightGBM datasets\ntrain_data = lgb.Dataset(X_train, label=y_train)\ntest_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n\n# Set parameters\nparams = {\n    'objective': 'binary',  # For binary classification\n    'boosting_type': 'gbdt',\n    'num_leaves': 31,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.9,\n    'min_data_in_leaf': 20,\n    'metric': 'binary_logloss' # Or 'auc'\n}\n\n# Train the model\nnum_round = 100\nbst = lgb.train(params, train_data, num_round, valid_sets=[test_data])\n\n# Make predictions\ny_pred = bst.predict(X_test)\ny_pred_binary = [1 if x > 0.5 else 0 for x in y_pred] # Convert probabilities to binary predictions\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred_binary)\nprint(f\"Accuracy: {accuracy}\")\n\n# Get feature importances\nfeature_importance = bst.feature_importance(importance_type='gain')\nprint(feature_importance)\n```\n\n### **In Conclusion:**\n\nLightGBM is a powerful and versatile tool in a data scientist's arsenal, especially when dealing with large and complex datasets like user behavior data. Its speed, efficiency, and accuracy make it ideal for building predictive models that can provide actionable insights to improve user engagement, retention, and ultimately, the success of our new app. I'm excited to apply it to our project and see what valuable insights we can uncover!\n\n### Holistic Comparsion: LightGBM vs. XGBoost\n\nA comprehensive comparison of LightGBM and XGBoost, two of the most popular and powerful gradient boosting libraries. While they share a common foundation, there are key differences in their algorithms, performance characteristics, and features.\n\n| Feature                          | LightGBM                                                     | XGBoost                                                      |\n| :------------------------------- | :----------------------------------------------------------- | :----------------------------------------------------------- |\n| **Tree Growth**                  | **Leaf-wise (best-first)**                                   | **Level-wise (depth-wise)**                                  |\n| **Splitting Strategy**           | Histogram-based, with binning                                | Pre-sorted (exact) and Approximate (quantile sketch/histogram) |\n| **Speed**                        | **Generally faster, especially on large datasets**           | **Can be slower, especially on large datasets with the exact method** |\n| **Memory Usage**                 | **Lower memory footprint**                                   | **Higher memory footprint (especially with pre-sorted method)** |\n| **Overfitting**                  | Can be more prone to overfitting on smaller datasets, but has effective regularization parameters | Generally more robust to overfitting on smaller datasets     |\n| **Categorical Feature Handling** | Built-in, optimized handling                                 | Requires one-hot encoding (or other encoding) or can use its own optimal split method |\n| **Missing Values**               | Handles missing values natively                              | Handles missing values natively, assign the value to left or right child by comparsion |\n| **GPU Support**                  | Excellent GPU support                                        | Good GPU support (but historically LightGBM was ahead)       |\n| **Scalability**                  | Highly scalable to very large datasets                       | Also scalable, but performance difference becomes more pronounced on massive data |\n| **Parameters**                   | Many parameters to tune                                      | Many parameters to tune, some are similar with lightGBM      |\n| **API/Ecosystem**                | Python, R, C++                                               | Python, R, Java, Scala, C++, Julia, etc.                     |\n| **Popularity**                   | Extremely popular, especially for very large datasets or low latency requirement. | Extremely popular, wide adoption, and a strong community     |\n| **Sparse Data**                  | Optimized for sparse data                                    | Handle sparse data with sparse aware split finding           |\n| **Custom Loss Functions**        | Supported                                                    | Supported                                                    |\n| **Early Stopping**               | Supported                                                    | Supported                                                    |\n| **Parallel Learning**            | Supports parallel and distributed learning                   | Supports parallel and distributed learning                   |\n\n**Detailed Explanation of Key Differences:**\n\n1. **Tree Growth Strategy:**\n\n    *   **LightGBM (Leaf-wise):**  Grows the tree by repeatedly splitting the leaf node that promises the largest reduction in the loss function. This leads to deeper, more asymmetric trees.\n        *   **Pros:** Often converges faster, can achieve lower loss.\n        *   **Cons:** More prone to overfitting, especially on smaller datasets, if not properly regularized.\n    *   **XGBoost (Level-wise):** Grows the tree level by level, splitting all nodes at the same depth before moving to the next level. This results in more balanced trees.\n        *   **Pros:** More robust to overfitting, easier to control tree complexity.\n        *   **Cons:** Can be slower to converge, might not achieve the same level of accuracy as leaf-wise growth on some datasets.\n\n2. **Splitting Algorithm:**\n\n    *   **LightGBM (Histogram-based):** Discretizes continuous features into bins and uses histograms to efficiently find the best split points.\n        *   **Pros:** Significant speed and memory improvements, especially on large datasets.\n        *   **Cons:** Potential loss of precision due to binning (though often negligible in practice).\n    *   **XGBoost:**\n        *   **Pre-sorted (Exact):**  Sorts the feature values and iterates through all possible split points.\n            *   **Pros:** Finds the exact best split.\n            *   **Cons:** Very computationally expensive and memory-intensive, especially for large datasets.\n        *   **Approximate:** Uses quantile sketches or histograms to approximate the best split.\n            *   **Pros:** Faster than the exact method, more scalable.\n            *   **Cons:** The split might not be as precise as the exact method.\n\n3. **Categorical Feature Handling:**\n\n    *   **LightGBM:**  Has a native, optimized way of handling categorical features without requiring one-hot encoding. It finds the optimal way to split based on categories by considering different groupings.\n    *   **XGBoost:**\n        *   Originally, it required users to perform one-hot encoding (or other encoding methods) for categorical features.\n        *   However, now it offers experimental support for finding optimal splits for categorical features without one-hot encoding.\n\n4. **Missing Values:**\n    *   **LightGBM:** Intelligently handles missing values during training by assigning them to the branch that yields the best gain in each split.\n    *   **XGBoost:** Similar strategy to LightGBM. In each node, it tries to send missing value data points to both the left and right child, calculating the gain for each, then ultimately send it to the child with the higher gain.\n\n**When to Choose LightGBM:**\n\n*   **Very large datasets:** When training speed and memory efficiency are critical.\n*   **Low-latency requirements:** When you need fast predictions.\n*   **High-cardinality categorical features:** When you have categorical features with many unique values.\n*   **Sparse datasets**\n\n**When to Choose XGBoost:**\n\n*   **Smaller datasets:** When overfitting is a primary concern.\n*   **Focus on interpretability:** When you want more balanced trees that are easier to interpret (although both offer model interpretation tools).\n*   **Established workflow:** When you have an existing pipeline or team already familiar with XGBoost.\n\n**Important Notes:**\n\n*   **Performance is dataset-dependent:** The \"best\" library often depends on the specific dataset and problem. It's highly recommended to try both and compare performance using cross-validation.\n*   **Tuning matters:** Both libraries have many parameters. Proper tuning is essential to achieve optimal performance.\n*   **Evolving landscape:** Both LightGBM and XGBoost are actively developed. New features and optimizations are constantly being added, so it's important to stay up-to-date.\n\n","tags":["lightGBM"],"categories":["ML"]},{"title":"Machine Leaning (XGBoost)","url":"/2025/01/23/ML_XGboosting/","content":"\n## XGBoost\n\nXGBoost（Extreme Gradient Boosting）是一个高效的、可扩展的树模型实现，广泛用于结构化数据（表格数据）场景的机器学习任务，如分类和回归问题。它以其高性能、灵活性和优化的实现赢得了广泛使用，尤其是在Kaggle竞赛和工业应用中。\n\n### 1. 什么是XGBoost？\nXGBoost是基于梯度提升（Gradient Boosting）算法优化后的提升树（Boosted Trees）模型。梯度提升是一种集成学习方法，它通过将多个弱学习器（比如决策树）串联起来，从而构建出一个强学习器。XGBoost对梯度提升进行了许多工程化、数学优化和高效实现，使其具有以下特点：\n- **更高效的计算**：通过增量式的优化，充分利用多线程和存储资源。\n- **正则化**：引入了L1和L2正则化，增加了控制过拟合的能力。\n- **处理缺失值**：自动处理缺失值，使其适用于更复杂的数据场景。\n- **可并行计算**：支持分布式计算，适合大规模数据集。\n- **灵活性强**：可用于分类、回归、排序（Ranking）等任务。\n\n---\n\n### 2. 核心思路\nXGBoost基于梯度提升树（Gradient Boosting Decision Trees, GBDT），而梯度提升树的主要思想是通过迭代地优化一个损失函数，逐步构建出一组弱学习器（通常是决策树），每一棵树用于修正上一棵树的残差（预测误差）。XGBoost在此基础上进行了一系列增强：\n\n1. **加速计算**：通过内存感知的块存储结构、高效分裂点查找、支持多线程等手段，提升训练速度。\n2. **自定义损失函数**：可使用一阶梯度（原始梯度）和二阶梯度（Hessian矩阵）来优化目标函数，提高拟合准确性。\n3. **正则化**：在目标函数中加入正则化项，从而控制模型复杂度。\n4. **独特的分裂节点优化**：优先选择总增益最大的分裂点，同时适配加权数据。\n5. **处理稀疏数据**：通过启发式的算法处理缺失值，同时支持稀疏输入矩阵。\n\n---\n\n### 3. 数学公式\n\n#### (1) 模型目标函数\nXGBoost的核心在于最小化目标函数 \\( \\mathcal{L} \\)，包括损失函数和正则化项：\n$$\n\\mathcal{L} = \\sum_{i=1}^n l(y_i, \\hat{y}_i) + \\sum_{k=1}^K \\Omega(f_k)\n$$\n\n- $l(y_i, \\hat{y}_i)$ ：损失函数，用于衡量预测值与真实值之间的差距。\n-  $\\Omega(f_k)$ ：正则化项，用于控制树的复杂度。\n  $$\n  \\Omega(f_k) = \\gamma T + \\frac{1}{2} \\lambda \\| w \\|^2\n  $$\n  \n  - \\( $T$ \\)：树的叶节点数量。\n  - \\( $w$ \\)：叶节点的权重。\n  - \\( $\\gamma, \\lambda$ \\)：正则化系数，控制树的复杂度。\n\n#### (2) 二阶近似优化\nXGBoost使用泰勒展开公式，对损失函数在当前模型处取二阶展开，从而更高效地找到最优分裂点：\n$$\n\\mathcal{L}^{(t)} \\approx \\sum_{i=1}^n \\big[ g_i w_i + \\frac{1}{2} h_i w_i^2 \\big] + \\Omega(f)\n$$\n\n- \\( $g_i = \\partial_{\\hat{y}^{(t-1)}} l(y_i, \\hat{y}^{(t-1)})$ \\)：一阶梯度 （梯度下降的方向）。\n- \\( $h_i = \\partial^2_{\\hat{y}^{(t-1)}} l(y_i, \\hat{y}^{(t-1)})$ \\)：二阶梯度（梯度下降的变化率）。\n\n#### (3) 分裂增益计算\n伪装节点的分裂增益公式为：\n\n$Gain=(分裂后损失的减少)=(左子树的损失 + 右子树的损失) - 分裂前的总损失$\n$$\n\\text{Gain} = \\frac{1}{2} \\bigg[ \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{(G_L+G_R)^2}{H_L + H_R + \\lambda} \\bigg] - \\gamma\n$$\n\n- \\( G_L, G_R \\)：左、右子节点的一阶梯度和。\n- \\( H_L, H_R \\)：左、右子节点的二阶梯度和。\n- \\( $\\lambda, \\gamma$ \\)：正则化参数。\n\n​\t**实际操作过程**：\n\n​\t遍历所有特征的所有可能分裂点。\n\n​\t在每个分裂点上计算分裂增益。\n\n​\t选择增益最大的分裂点作为最佳分裂点。\n\n​\t如果最大的增益 GainGain 小于 𝛾*γ*，则停止分裂。\n\n#### (4) 叶子节点的权重计算\n\n- XGBoost 为了计算各叶子节点的最终权重，使用以下公式：\n\n  $$\n  w_j = -\\frac{\\sum_{i \\in j} g_i}{\\sum_{i \\in j} h_i + \\lambda}\n  $$\n\n  - \\($j$\\)：表示某一个具体的叶子节点。\n  - \\($i \\in j$\\)：指代所有分配到节点 \\(j\\) 的样本索引。\n  - \\($g_i$\\)：样本 \\(i\\) 对应的一阶梯度，代表模型的误差方向。\n  - \\($h_i$\\)：样本 \\(i\\) 对应的二阶梯度，衡量误差的曲率。\n  - \\($\\lambda$\\)：L2 正则化参数，用于控制权重的大小，防止模型过拟合。\n\n  这公式的含义是：通过优化一阶梯度（误差方向）的和，同时结合样本的二阶梯度（曲率），来决定当前叶子节点的最佳输出值。\n\n---\n\n### 4. XGBoost的优点\n\n1. **速度快**：利用多核、多线程、增量式的计算方法，训练效率极高。\n2. **性能优**：通过正则化和对目标函数的高级优化，能有效控制过拟合。\n3. **灵活性高**：支持分类、回归、排序、特定的自定义目标函数。\n4. **处理缺失值**：可**自动选择最优方向完成缺失值**填充。\n5. **特征选择能力强**：通过树模型的结构，能够有效识别重要特征。\n6. **支持分布式计算**：具备良好的扩展性，适用大规模数据集。\n\n### 5. XGboost Summary\n\n1. **Start with an initial prediction (e.g., mean or prior values):**\n    Initialize the model predictions $𝑦^𝑖*y*^*i*$ with a base value, such as the average of the target variable in a regression task or the logarithmic odds in a classification task.\n2. **Calculate residuals:**\n    For each iteration, calculate the residuals or gradients that represent the model's errors with respect to the current predictions.\n3. **Train a new tree to minimize residuals:**\n    Fit a new decision tree using the residuals/gradients as the target variable. The tree learns how to correct the current errors in the prediction.\n4. **Adjust leaf weights based on gradients and second-order gradients:**\n    Once the structure of the tree is determined, calculate the weights for each leaf based on the gradients and second-order gradients for the samples falling into that leaf. The tree's predictions are scaled by the learning rate and added to the cumulative predictions.\n5. **Update predictions iteratively:**\n    Repeat steps 2–4 iteratively, so that each new tree gradually improves the model's overall prediction by correcting the previous residuals.\n\n---\n\n### 6. XGBoost的参数详解\n\nXGBoost的参数分为三类：**通用参数**、**树参数**、**学习目标参数**。\n\n#### (1) 通用参数\n- `booster`：选择基础模型（默认值为`gbtree`），支持`gbtree`（树模型）、`gblinear`（线性模型）、`dart`（带Dropout的树模型）。\n- `nthread`：设置线程数。\n\n#### (2) 树参数\n- `max_depth`：树的最大深度，控制模型复杂性，默认值为6。\n- `eta`（Learning Rate）：学习率，控制每棵树对最终模型的贡献，默认值为`0.3`。\n- `min_child_weight`：最小叶节点样本权重和，用于防止过拟合，默认值为`1`。\n- `subsample`：训练数据的采样比例（默认值`1`）。\n- `colsample_bytree`：每棵树随机采样的特征比例，防止过拟合。\n- `lambda`：L2正则化系数，默认值`1`。\n- `gamma`：最小分裂增益，默认值`0`。\n\n#### (3) 学习目标参数\n- `objective`：定义学习任务（如分类、回归、排序等）。\n  - `reg:linear`：线性回归。\n  - `binary:logistic`：二分类问题。\n  - `multi:softmax`：多分类，返回类别。\n  - `multi:softprob`：多分类，返回类别概率。\n\n### 7. 使用XGBoost的步骤\n\n以下是典型的XGBoost建模流程：\n\n#### (1) 数据预处理\n- 清洗、处理缺失值和异常值。\n- 分割训练集和测试集。\n- 特征工程（如编码、归一化、特征选择）。\n\n#### (2) 模型构建\n- 导入XGBoost库（如`xgboost`或`sklearn`接口）。\n- 定义超参数（包括树深、学习率等）。\n- 使用DMatrix数据格式加载输入特征和目标。\n\n#### (3) 模型训练\n- 使用`xgb.train()`或`XGBClassifier`训练模型。\n- 定义早停条件（如`early_stopping_rounds`）。\n\n#### (4) 模型评估\n- 使用交叉验证或测试集，评估性能指标（如AUC、F1-score）。\n\n#### (5) 模型调优\n- 调整超参数（如`max_depth`、`min_child_weight`、`colsample_bytree`等）。\n- 结合网格搜索（Grid Search）或贝叶斯优化。\n\n---\n\n### 8. 代码示例\n\n以下是一个简单的二分类任务的XGBoost示例：\n\n```python\nimport xgboost as xgb\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\n# 加载数据\ndata = load_breast_cancer()\nX, y = data.data, data.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 转换为XGBoost专用的DMatrix数据格式\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)\n\n# 定义参数\nparams = {\n    'objective': 'binary:logistic',\n    'max_depth': 6,\n    'eta': 0.1,\n    'eval_metric': 'auc'\n}\n\n# 训练模型\nevals = [(dtrain, 'train'), (dtest, 'eval')]\nmodel = xgb.train(params, dtrain, num_boost_round=100, evals=evals, early_stopping_rounds=10)\n\n# 预测\ny_pred = model.predict(dtest)\nauc_score = roc_auc_score(y_test, y_pred)\nprint(f\"AUC Score: {auc_score}\")\n```\n\n---\n\n### 9. 结论\nXGBoost是一种强大的机器学习工具，特别适合梯度提升树模型的场景。通过优化计算方法、自定义目标函数、正则化和扩展性等特点，XGBoost在工业界与学术界均有卓越应用。掌握其原理和参数调优，能够有效提升数据科学项目的性能和效率。\n\n### 10 GBDT 和 XGboost\n\nXGBoost 是众多实现 **Gradient Boosting Decision Tree** (GBDT，梯度提升决策树) 的一种，但它在 GBDT 的基础上引入了多种优化和改进，使其在工程高效性、性能表现和鲁棒性方面超越了传统的 GBDT。下面我们来详细讲解 **XGBoost 和传统 GBDT 的主要区别**。\n\n#### **损失函数优化：引入二阶导数信息**\n\n**传统 GBDT**：\n\n- 仅使用目标函数的一阶梯度（即残差）来指导新树的生成。\n- 目标是基于一阶梯度，使模型不断逼近最优解。\n\n**XGBoost**：\n\n- **引入了二阶梯度（Hessian，目标函数的曲率）进行优化。**\n- 树节点的划分不仅依赖于一阶梯度，还考虑了二阶梯度（即损失函数的曲率特性），引入了更精确的梯度信息。\n- 二阶梯度进一步提升了分裂点计算的准确性和优化性，帮助模型更好地拟合复杂数据。\n\n**二阶信息的改进：**\n\nXGBoost 在目标函数中加入二阶梯度的累加项，分裂点决策时的公式为：\n$\\text{Gain} = \\frac{1}{2} \\left( \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{(G_P)^2}{H_P + \\lambda} \\right) - \\gamma$\n\n- \\( G \\)：一阶梯度的累加。\n- \\( H \\)：二阶梯度的累加。\n- **效果**：比单纯基于一阶残差的传统 GBDT 提供更准确的分裂评估，同时二阶信息能更好地避免梯度方向过冲（Overstepping）。\n\n#### **正则化：控制模型复杂度（防止过拟合）**\n\n**传统 GBDT：**\n\n- 通常没有明确的正则化项，较难控制模型的复杂度。\n- 因为迭代训练中如果树的深度很深或切分过多，传统 GBDT 往往容易过拟合。\n\n**XGBoost：**\n\n- 在目标函数中显式引入了正则化项，通过控制模型复杂度来防止过拟合：\n  \\[\n  $\\mathcal{L}(q) = -\\frac{1}{2}(\\text{目标函数中的一阶和二阶梯度优化}) + \\Omega(f)$\n  \\]\n  正则化项：\n  \\[\n  $\\Omega(f) = \\gamma T + \\frac{\\lambda}{2} \\sum_j w_j^2$\n  \\]\n  - \\( \\gamma T \\)：控制叶子节点个数 \\(T\\) 的惩罚，用于限制树的深度；\n  - \\( \\lambda \\sum w_j^2 \\)：控制叶子节点权重 \\(w_j\\) 的惩罚，用于避免过大的权重值。\n  - **效果**：正则化项可以防止模型在训练时过拟合，同时提升泛化能力。\n\n#### **并行计算：优化训练效率**\n\n**传统 GBDT：**\n\n- 传统 GBDT 是 **串行** 的方法，即每棵树的训练必须依赖于上一棵树的输出，因此无法并行化。\n- 效率相对较低，尤其是在大规模数据集上，耗时较长。\n\n**XGBoost：**\n\n- **全局搜索优化加并行化计算：**\n  1. 引入了 **Block-wise 的分裂模型**：\n     - 对于特定节点，XGBoost 使用直方图算法（Histogram）对特征值排序并离散化，计算每个分裂点的候选增益。\n     - 一旦节点划分结束，其增益计算可以在多个线程间并行化。\n  2. 将并行化落地到节点的分裂计算中：\n     - 特征按区块划分，增益计算可以在每个区块中同时完成，显著提高效率。\n  3. 对特征子集或样本做随机分割（列采样和行采样），进一步提升计算效率。\n\n- **效果**：通过支持并行化，XGBoost 在大量数据和高维场景中计算速度远超传统 GBDT。\n\n#### **支持行/列采样**\n\n**传统 GBDT：**\n\n- 大多数 GBDT 实现（如 sklearn 中的 GradientBoosting）对特征不支持列采样，对数据行采样的支持也有限。\n- 只能依赖全部数据进行树分裂，容易导致过拟合，且计算成本高（因为所有列都要用于分裂）。\n\n**XGBoost：**\n\n- 支持 **列采样（feature sampling）**：随机选择部分特征用于分裂。\n  - 类似于随机森林的特征子集选择方法（列采样）。\n  - 帮助进行降维，缓解数据中冗余特征的干扰，同时提升效率并增强模型的鲁棒性。\n- 支持 **行采样（row sampling）**：对数据进行随机采样，这种方法类似于传统 Bagging 的原理，进一步减轻了过拟合的风险。\n\n#### **基于分裂的停止策略**\n\n**传统 GBDT：**\n\n- 传统 GBDT 通常没有细化的停止策略，通常用树的固定深度或样本个数等简单规则来决定分裂是否停止。\n- 在某些场景下效果欠佳（可能过度分裂导致过拟合）。\n\n**XGBoost：**\n\n- XGBoost 的分裂停止标准更加严格：\n  - 如果在某个节点进行分裂后的增益 \\( \\text{Gain} \\) 小于阈值 \\( \\gamma \\)，则停止分裂，当前节点变为叶子节点。\n  - 结合正则化项 \\( \\mathcal{L}(q) + \\Omega(f) \\)，对分类叶子节点和树结构进行复杂度限制，从而显著减少树的冗余分裂。\n\n#### **支持稀疏特征的处理**\n\n**传统 GBDT：**\n\n- 传统实现难以很好地处理稀疏数据（例如缺失值或很多特征值为零的场景），可能需要手动预处理。\n\n**XGBoost：**\n\n- 原生支持稀疏特征，通过使用默认方向（default direction）处理缺失值。\n  - 每次分裂时，模型会学习在特征缺失情况下样本的默认去向（是分到左子节点还是右子节点）。\n- 提供对稀疏特征计算效率的优化，同时提升模型的鲁棒性。\n\n#### **支持自定义目标函数**\n\n**传统 GBDT：**\n\n- 大多数传统 GBDT 实现只支持少量固定的目标函数（如均方误差、二元对数损失）。\n\n**XGBoost：**\n\n- 提供接口支持自定义任意可微目标函数。\n  - 用户只需要提供 **一阶导数** 和 **二阶导数**，即可将不同任务（如排序、目标优化、回归等）套用到 XGBoost 中。\n  - 如排序任务的 LambdaRank、Pairwise Ranking 等可以轻松整合到 XGBoost。\n\n#### 总结：XGBoost 对 GBDT 的优越性\n\n| **特性**           | **传统 GBDT**          | **XGBoost**                                  |\n| ------------------ | ---------------------- | -------------------------------------------- |\n| **优化目标**       | 一阶梯度               | 一阶梯度 + 二阶梯度（目标函数曲率）          |\n| **正则化**         | 无显式正则化           | 显式正则化（叶节点和树深度的惩罚）           |\n| **并行化**         | 不支持                 | 支持特征和样本的并行化处理                   |\n| **列/行采样**      | 不支持                 | 支持列采样和行采样，缓解过拟合并提升效率     |\n| **稀疏数据处理**   | 不支持                 | 支持稀疏特征，自动处理缺失值                 |\n| **分裂停止策略**   | 简单停深条件           | 自适应增益阈值 \\( \\gamma \\) 用于分裂决策     |\n| **自定义目标函数** | 不支持                 | 支持任意可微目标函数                         |\n| **工程优化和效率** | 慢（因为无并行和优化） | 快（支持并行化、缓存优化、大规模分布式训练） |\n\n可以总结为 **XGBoost 是工程优化、正则化处理更强的 GBDT 版本**，尤其在大规模数据、稀疏特征、有复杂任务需求时表现尤为突出。\n","tags":["xgboost"],"categories":["ML"]},{"title":"Data Warehouse Architecture Knowledge","url":"/2025/01/19/data_warehouse_architecture/","content":"\n## 数仓建模的意义\n\n### 分层的意义\n\n- 清晰数据结构：每一层都有各自的作用域\n- 数据血缘关系追踪：快速定位数据问题\n- 数据复用，减少重复开发：通过中间层极大减少重复计算，统一口径\n- 复杂问题简化：每一层只做单一处理，且便于数据维护当口径调整仅需调整底层字段\n- 屏蔽原始数据影响，屏蔽业务的影响：业务系统发生调整，不必改一次系统就需要重新接入数据\n- 屏蔽源头业务系统的复杂性：源头系统可能极为复杂，通过`DW` 来实现规范化\n- 数据仓库的可维护性：分层使得某层的问题只需解决该层问题，不会影响到下层的代码和逻辑\n\n### 数据仓库的ETL操作\n\nETL(Extraction-Transformation-Loading) 负责将分散的、异构数据源中的数据抽取到临时中间层后进行操作加载到目标仓库和数据集市中。\n\n- 数据抽取：包括数据初始化装载和数据刷新，初始化装载即如何基于业务系统来构建维度表和事实表，并将数据装载在这些表中；数据刷新即设定定时任务来监听和触发源数据的变动从而对相应的数据表进行更新维护\n- 数据清洗：针对源系统数据库中出现的重复、不完整、违反业务逻辑等问题数据进行统一处理\n- 数据转换：主要是将数据清洗后的数据转换成数仓所需的数据：规范统一字段命名提供统一的数据字典和格式\n- 数据加载：将处理完后的数据导入对应存储空间中\n\n| 步骤     | 组件           | 功能描述                                                     |\n| -------- | -------------- | ------------------------------------------------------------ |\n| 数据抽取 | Flume          | 收集、聚合和移动大量日志数据。                               |\n|          | Sqoop          | 在Hadoop和关系型数据库之间传输数据。                         |\n| 数据清洗 | Hive           | 提供SQL-like查询语言，进行数据清洗和转换。                   |\n|          | Pig            | 提供类似SQL的语言（Pig Latin），编写复杂的ETL流程。          |\n| 数据转换 | Spark          | 高效地进行数据转换。                                         |\n|          | MapReduce      | 批处理数据转换任务。                                         |\n| 数据加载 | HDFS           | 存储处理后的数据。                                           |\n|          | HBase          | 对于需要实时访问的数据，使用HBase进行存储。                  |\n| 任务调度 | Oozie          | 流程编排和工作流管理，支持Flume、Sqoop、Hive、Pig、Spark等任务调度。 |\n|          | Apache Airflow | 强大的工作流管理平台，支持复杂的工作流编排和监控。           |\n\n### 分层的误区\n\n数仓的划分不是为了分层而分层，更多是为了解决ETL任务及工作流的组织、数据的流向、读取权限的控制。\n\n通常基础分层为三层 `ODS-DW-AD` 其中`DW` 可根据公司业务需求进行细分为 `DWD` `DWT` `DWS` `DIM` `DM` \n\n## 技术架构\n\n![image-20250119143046124](http://hexo.kygoho.win/upload/uploads/2a070c70-b57d-4e82-8b1c-2d8b8e4faca8.png)\n\n![image-20250119143129904](http://hexo.kygoho.win/upload/uploads/dfb6d6a6-b9ad-4a28-9661-a892eb408287.png)\n\n数据中台包含的内容：\n\n- 系统架构：以Hadoop、Spark等组件为中心的架构体系\n- 数据架构：顶层设计、主题域划分、分层设计\n- 数据建模：维度建模，业务过程-确定粒度-维度-事实表\n- 数据管理：资产管理，元数据管理、质量管理、主数据管理、数据标准、数据安全管理\n- 辅助系统：调度系统、ETL系统、监控系统\n- 数据服务：数据门户、机器学习数据挖掘、可视化系统、数据交换分享下载\n\n## 数据分层架构\n\n一些架构：\n\n![image-20250119151429188](http://hexo.kygoho.win/upload/uploads/10f093ec-73ac-41c9-96f1-47b472672857.png)\n\n![image-20250119151443257](http://hexo.kygoho.win/upload/uploads/6f450469-c0f1-4bb5-aea6-c0f58136f388.png)\n\n![image-20250119151459600](http://hexo.kygoho.win/upload/uploads/4674bcb9-e92c-4273-966a-6ebd5e9923fd.png)\n\n![image-20250119151905866](http://hexo.kygoho.win/upload/uploads/13ec3f56-ffca-4310-a02d-72a2f80345a5.png)\n\n#### 贴源层（ODS, Operational Data Store)\n\n![image-20250119152504650](http://hexo.kygoho.win/upload/uploads/5656dcd6-de83-482b-8191-efd739ec894f.png)\n\n数据引入层又称数据基础层，将原始数据几乎无处理存放在该层，结构与源系统保持一致，是准备区，主要职责是将基础**数据同步和存储**。\n\n一般来说`ODS` 和源系统数据是同构的，主要是简化后续数据加工处理工作。从数据粒度上来说是最细的。对于历史数据的存留一般保存3-6个月后需要清楚，以节省空间。根据不同的需要采取不同生命周期管理措施。\n\n注意：这一层除数据接入，还会考虑一定的数据清洗，比如异常字段处理、字段命名规范花、时间字段的统一。\n\n - 数据分区：按照时间进行分区，维度表按照快照时间、事实表按照业务时间\n\n - 数据处理：对于离线数据，每日定时任务跑批，通过`Sqoop` 来进行数据抽取；对于实时数据一般利用Spark streaming，Flink 来进行处理入库\n\n - 数据存储策略：\n\n   1） 增量存储\n\n   2） 全量存储\n\n   3） 拉链存储 \n\n- Hive外部表：存放数据的文件可以不是在hive的hdfs默认的位置，并且hive对应的表删除时相应的数据文件并不会删除，可以防止误删除而将数据删除。\n\n#### 数仓层（DW, data warehouse）\n\n数据仓库层时核心层，将从`ODS` 层中的数据按照主题建立数据模型，每一个数据模型对应一个宏观分析的领域，DW层会保存永久的存量的数据。\n\nDW层存放明细事实数据，维表数据以及公共指标汇总数据。其中，明细事实数据、维表数据一般根据ODS层数据加工生成。公共指标数据汇总一般根据维表数据和明细事实数据加工生成。\n\nDW层又可以细分维度层`DIM`  ，明细数据层`DWD` 汇总数据层 `DWS` ，采用维度建模方法作为理论基础，可以定义为度模型主键与事实模型中外键关系，减少数据冗余，提高明细数据表的易用性。在汇总数据层同样可以关联复用统计粒度中的为度，采取更多的宽表化手段来构建公共指标数据层，提升指标复用性。\n\n维度层（DIM，Dimension）：以维度作为建模驱动，基于每个维度的业务含义，通过添加维度属性，关联维度等。为了避免在维度模型中冗余关联维度的属性，基于雪花模型构建维度表。\n\n明细数据层（DWD，Data Warehouse Detail）：以业务过程作为建模驱动，基于每个具体业务过程特点，构建最细粒度的明细事实表。可将某些重要属性字段适当冗余，也即宽表化处理。\n\n汇总数据层（DWD，Data Warehouse Summary）：以分析的主题对象作为建模驱动，基于上层的应用和产品的指标需求，构建公共粒度的汇总指标表。以宽表化手段物理化模型，构建统一命名规范、口径统一的统计指标，为上层提供公共指标，建立汇总宽表、明细事实表。\n\n主题域：面向业务过程，将业务活动事件进行抽象集合。针对DWD层\n\n数据域：面向业务分析，将业务过程或者维度进行抽象集合。针对DWS层\n\n**DWD 层是以业务过程为驱动；DWS是以需求为驱动。**\n\n- 公共维度层（DIM）\n\n由维表构成。维度是逻辑概念，是衡量和观察业务的角度。维表是根据维度及其物理属性来物理化的表，采用宽表设计原则。因此，构建公共维度汇总层首先要定义维度。他贯穿整个DW层\n\n高基数维度数据：用户信息、商品信息，数量级为千万或者上亿级别。\n\n低基数维度数据：一般是配置表，如日期维表。\n\n维表设计：\n\n避免过于频繁的更新维表的数据：缓慢变化维-拉链表\n\n维度建模步骤：\n\n**维度建模**是数据仓库设计中的核心方法之一，它通过将业务数据组织为事实表和维度表来支持高效的查询和分析。常见的维度建模方式有**星型模型（Star Schema）**和**雪花模型（Snowflake Schema）** \n\n1）确定业务过程\n\n2）选择与业务事实相关的主维度\n\n3）识别主维度中可以进一步拆分的子维度\n\n关于主题：\n\n数据仓库都是面向主题组织的，主题在一个较高层次上将企业信息数据进行综合、归类和分析利用。每一个主题对应一个宏观的分析领域。\n\n- 数据明细层（DWD，Data Warehouse detail）\n\n![image-20250119183318981](http://hexo.kygoho.win/upload/uploads/35dd50ec-33aa-4c5c-ad90-ddcdb891a25c.png)\n\nDWD是业务层与数据仓库的隔离层， 这⼀层主要解决⼀些数据质量问题和数据的完整度问题。\n\n明细表用于存储ODS层原始表转换过来的明细数据，DWD层的数据是经过清洗、规范、维度退化后的数据。\n\n明细粒度事实层（DWD）：**以业务过程作为建模驱动**，基于每个具体的业务过程特点，构建最细粒度的明细层事实表。可以结合企业的数据使⽤特点，将明细事实表的某些重要维度属性字段做适当冗余，即宽表化处理。明细粒度事实层的表通常也被称为逻辑事实表。\n\n维度退化：对于一些特殊维度如订单ID无法在仓库中找出与其相关联的其他内容则可对他进行维度退化将该维度退化至事实表中。\n\nDWD层做了哪些事：\n\n数据清洗过滤-数据映射转换-数据规范-维度退化-日期规范化\n\n明细表设计原则：\n\n**一个明细粒度事实表仅和一个维度关联。**\n**尽可能包含所有与业务过程相关的事实。**\n**只选择与业务过程相关的事实。**\n**解不可加性事实为可加的组件。**\n**在选择维度和事实之前必须先声明粒度。**\n**在同一个事实表中不能有多种不同粒度的事实。**\n**事实的单位要保持一致。粒度**\n**谨慎处理Null值。**\n**使用退化维度提高事实表的易用性。**\n\n- 数据汇总层（DWS，Data Warehouse Summary）\n\n![image-20250119190812666](http://hexo.kygoho.win/upload/uploads/2c194214-a4f6-464d-96b6-2a49b2719dda.png)\n\n基于DWD层的明细数据，可以按照一些场景、分析实体去组织数据，构成一些分主题的汇总层数据层DWS\n\n明细粒度 ====》汇总粒度\n\nDWS层（数据汇总层）宽表，面向主题汇总，维度相对来说比较少，DWS根据DWD层的基础数据按照各个维度ID进行粗粒度汇总聚合，如按照交易来源、交易类型汇总。整合汇总成分析某一个主题域的服务数据，一般是宽表。\n\n> 宽表建模 (Wide Table Modeling)，也称为扁平化建模或反规范化建模，是一种数据建模技术，旨在通过将多个相关表中的数据合并到一个单一的、宽大的表中来简化数据查询和分析。 这个宽表通常包含大量列，其中每一列代表来自不同源表中的一个属性。\n\n以DWD为基础，按天轻度汇总。统计各个主题对象的当天行为，（例如，购买行为，统计商品复购率）\n\n该层数据表会相对比较少，基本都是宽表（一张表会涵盖比较多的业务内容，表中的字段比较多）按照主题划分，如订单、用户等，用于后续的业务查询，OLAP分析，数据分发等。\n\nDWS层做了哪些：\n\nDWS将DWD层的数据按照主题进行汇总，每个主题构建1-3张宽表：按照主题建模、维度建模\n\n1）DWS层每个主题1-3张宽表（处理100-200个指标覆盖70%以上的需求）\n\n2）最大的宽表：用户行为宽表。大概60-200个字段\n\n3）行为宽表包括的指标：评论、打赏、收藏、关注、分享\n\n4）分析过的指标：日活、月活、新增、留存、转化率、流失、活跃\n\n- 数据应用层（ADS，Application Data Store）\n\n![image-20250120161541380](http://hexo.kygoho.win/upload/uploads/16b18695-9c6a-45ad-8c07-fd683a05839c.png)\n\nADS存放数据产品个性化的统计指标数据，报表数据。主要是提供给数据产品和数据分析使用的数据，通常根据业务需求划分成流量、订单、用户等，生成字段较多的宽表，用于提供后续的业务查询主要目的是满足用户分析的需求。所以数据存储的一般是近几年，但在数据广度上来说覆盖了所有的业务数据。\n\n业务⽅或者部⻔基于DWD和DWS建⽴的数据集市(Data Market, DM)，⼀般来说应⽤层的数据来源于DW层，⽽且相对于DW层，应⽤层只包含部⻔或者业务⽅⾯⾃⼰关⼼的明细层和汇总层的数据。\n\n","tags":["Architecture"],"categories":["DataWarehouse"]},{"title":"Null Value in SQL","url":"/2024/12/13/Null_Value_in_SQL/","content":"\n在 SQL 中处理 `NULL` 值需要格外小心，因为 `NULL` 代表“未知”或“缺失”，它的行为与其他值不同，可能会导致意想不到的结果。以下是一些在 SQL 中处理 `NULL` 值时需要注意的关键事项：\n\n**1. `NULL` 值与比较运算符：**\n\n   - **`NULL` 与任何值的比较（包括 `NULL` 本身）都返回 `UNKNOWN`，而不是 `TRUE` 或 `FALSE`。**\n   - 这意味着：\n      - `NULL = NULL`  返回 `UNKNOWN`\n      - `NULL <> NULL` 返回 `UNKNOWN`\n      - `NULL > 10`  返回 `UNKNOWN`\n      - `NULL < 10`  返回 `UNKNOWN`\n      - `NULL = 'abc'` 返回 `UNKNOWN`\n   - 因此，**不能使用 `=` 或 `<>` 来直接判断一个值是否为 `NULL`。**\n\n**2. `IS NULL` 和 `IS NOT NULL` 操作符：**\n\n   - **使用 `IS NULL` 来判断一个值是否为 `NULL`：**\n      ```sql\n      SELECT * FROM your_table WHERE your_column IS NULL;\n      ```\n   - **使用 `IS NOT NULL` 来判断一个值是否不为 `NULL`：**\n      ```sql\n      SELECT * FROM your_table WHERE your_column IS NOT NULL;\n      ```\n\n**3. `NULL` 值在聚合函数中的行为：**\n\n   - **大多数聚合函数（如 `COUNT`, `SUM`, `AVG`, `MAX`, `MIN`）都会忽略 `NULL` 值。**\n   - **`COUNT(*)` 会计算所有行数，包括包含 `NULL` 值的行，而 `COUNT(column_name)` 只计算非 `NULL` 值的行。**\n   - 例如：\n     - `SUM(your_column)` 只会对 `your_column` 中非 `NULL` 的值求和。\n     - `AVG(your_column)` 只会对 `your_column` 中非 `NULL` 的值求平均值。\n   - **如果 `MAX` 或 `MIN` 的所有值都是 `NULL`，则结果也是 `NULL`。**\n   - **`COUNT(DISTINCT column_name)` 不会统计 `NULL` 值。**\n\n**4. `NULL` 值在 `WHERE` 子句中的影响：**\n\n   - **`WHERE` 子句的条件必须返回 `TRUE` 才能选择对应的行。由于 `NULL` 与比较运算符的结果为 `UNKNOWN`，因此 `WHERE` 条件中包含 `NULL` 的表达式通常不会返回 `TRUE`，导致对应的行被过滤掉。**\n   - 例如：\n      ```sql\n      -- 假设 your_column 中存在 NULL 值\n      SELECT * FROM your_table WHERE your_column = 10;  -- 不会选择 your_column 为 NULL 的行\n      SELECT * FROM your_table WHERE your_column <> 10; -- 同样不会选择 your_column 为 NULL 的行\n      SELECT * FROM your_table WHERE your_column = NULL; -- 不会选择任何行\n      ```\n   - **要选择 `NULL` 值的行，必须使用 `IS NULL`。**\n\n**5. `COALESCE` 和 `IFNULL` 函数：**\n\n   - **`COALESCE(value1, value2, value3, ...)`：返回第一个非 `NULL` 值。**\n   - **`IFNULL(value1, value2)` (MySQL, MariaDB): 如果 `value1` 为 `NULL`，则返回 `value2`，否则返回 `value1`。**\n   - 这两个函数可以用来替换 `NULL` 值，例如：\n      ```sql\n      SELECT COALESCE(your_column, 0) AS column_with_default FROM your_table; -- 将 NULL 替换为 0\n      SELECT IFNULL(your_column, 'N/A') AS column_with_na FROM your_table; -- 将 NULL 替换为 'N/A'\n      ```\n   - **`COALESCE` 更通用，因为它接受多个参数。**\n\n**6. `NULL` 值在连接操作中的影响：**\n\n   - **`JOIN` 操作默认情况下会排除连接列中包含 `NULL` 值的行。**\n   - **可以使用 `LEFT JOIN`, `RIGHT JOIN` 或 `FULL OUTER JOIN` 来包含 `NULL` 值相关的行。**\n   - 例如，如果两个表连接的列中存在 `NULL` 值，`INNER JOIN` 将会排除这些行，而 `LEFT JOIN` 会保留左表的行，右表不匹配的行则填充为 `NULL`。\n\n**7. 数据库特定的 `NULL` 值处理：**\n\n   - 不同的数据库系统可能在某些细节上对 `NULL` 值的处理有所不同，请参考具体的数据库文档。\n\n**8. 设计数据库时对 `NULL` 值的考虑：**\n\n   - **应该仔细考虑哪些列允许 `NULL` 值，哪些列必须有值。**\n   - **尽量避免在关键列中使用 `NULL` 值，因为这可能会导致数据不一致和查询问题。**\n   - **可以使用约束 (constraints) 来强制某些列不允许 `NULL` 值。**\n   - **在文档中明确说明哪些列可能包含 `NULL` 值，以及 `NULL` 值在业务逻辑中的含义。**\n\n**总结：**\n\n- `NULL` 值在 SQL 中是一种特殊的值，代表未知或缺失。\n- 不能使用 `=` 或 `<>` 直接与 `NULL` 值进行比较，应使用 `IS NULL` 或 `IS NOT NULL`。\n- 大多数聚合函数会忽略 `NULL` 值。\n- `WHERE` 子句中包含 `NULL` 值的表达式通常不会返回 `TRUE`。\n- 可以使用 `COALESCE` 或 `IFNULL` 函数来处理 `NULL` 值。\n- 在设计数据库时，需要仔细考虑 `NULL` 值的用法。\n\n理解 `NULL` 值的行为并正确处理它们对于编写可靠的 SQL 查询至关重要。\n","tags":["sql"],"categories":["SQL"]},{"title":"Automate to Create a New Post Template","url":"/2024/12/13/automate_to_create_a_new_post_template/","content":"\n好的，这是一个可以创建 Markdown 文档模板的 Shell 脚本：\n\n```bash\n#!/bin/bash\n\n# 检查是否提供了文件名参数\nif [ -z \"$1\" ]; then\n  echo \"用法: $0 <文件名>\"\n  exit 1\nfi\n\nname=\"$1\"\ncurrent_date=$(date +%Y-%m-%d)\n\ncat << EOF > \"$name\".md\n---\ntitle: $name\ndate: $current_date\ncategories:\n  - {optional}\ntags: [{optional}]\ntoc: true\nexcerpt: \"{optional}\"\n---\nEOF\n\necho \"已创建文件: $name.md\"\n```\n\n**脚本解释:**\n\n1.  `#!/bin/bash`:  指定脚本使用 Bash 解释器执行。\n2.  `if [ -z \"$1\" ]; then ... fi`: 检查是否提供了文件名参数。 `$1` 代表脚本的第一个参数，`-z` 用于检查字符串是否为空。 如果没有参数，则输出用法信息并退出。\n3.  `name=\"$1\"`: 将第一个参数赋值给变量 `name`。\n4.  `current_date=$(date +%Y-%m-%d)`: 使用 `date` 命令获取当前日期，并格式化为 `YYYY-MM-dd`，然后赋值给变量 `current_date`。\n5.  `cat << EOF > \"$name\".md ... EOF`:  使用 `cat` 命令和 Here 文档 (`<< EOF ... EOF`) 将多行文本输出到以 `name` 变量命名的 `.md` 文件中。\n6.  `echo \"已创建文件: $name.md\"`: 输出一个确认信息，告知用户文件已创建。\n\n**如何使用:**\n\n1.  **保存脚本:** 将上面的代码保存为一个文件，例如 `create_md.sh`。\n2.  **添加执行权限:** 使用命令 `chmod +x create_md.sh` 为脚本添加执行权限。\n3.  **运行脚本:** 在终端中，使用以下命令运行脚本，并将要创建的 Markdown 文件名作为参数：\n\n    ```bash\n    ./create_md.sh my-new-document\n    ```\n\n    这将会创建一个名为 `my-new-document.md` 的文件，其中包含预定义的元数据。\n\n**示例输出:**\n\n如果执行命令 `./create_md.sh my-blog-post`，将会生成一个名为 `my-blog-post.md` 的文件，内容如下：\n\n```markdown\n---\ntitle: my-blog-post\ndate: 2023-10-27\ncategories:\n  - {optional}\ntags: [{optional}]\ntoc: true\nexcerpt: \"{optional}\"\n---\n```\n\n**说明:**\n\n*   `{optional}` 部分是占位符，你需要根据实际情况修改它们。\n*   这个脚本创建的文件名会自动添加 `.md` 后缀。\n","tags":["Shell","markdown"],"categories":["Shell"]},{"title":"Sql for Median","url":"/2024/12/13/sql_for_median/","content":"\n在 SQL 中求中位数稍微比求平均数复杂一些，因为中位数需要找到排序后中间的那个值（或者中间两个值的平均值）。不同的数据库系统提供了不同的方法来实现，这里我将介绍几种常用的方法，并针对不同的数据库给出示例。\n\n**核心思想：**\n\n1. **排序:** 首先需要对数据进行排序。\n2. **确定中位数位置:**\n   - 如果数据量是奇数，中位数就是中间的那个值。\n   - 如果数据量是偶数，中位数是中间两个值的平均值。\n\n**方法和示例：**\n\n**1. 使用窗口函数 (适用于 SQL Server, PostgreSQL, Oracle, MySQL 8+):**\n\n   窗口函数可以方便地计算排序后的行号，然后根据行号来确定中位数。\n\n   ```sql\n   -- 假设表名为 `your_table`，需要求中位数的列名为 `your_column`\n   WITH RankedData AS (\n       SELECT \n           your_column,\n           ROW_NUMBER() OVER (ORDER BY your_column) AS rn,\n           COUNT(*) OVER () AS total_count\n       FROM your_table\n   )\n   SELECT\n       CASE\n           WHEN total_count % 2 = 0 THEN  -- 偶数个数据\n               (SELECT AVG(your_column) FROM RankedData WHERE rn IN (total_count/2, total_count/2 + 1))\n           ELSE  -- 奇数个数据\n               (SELECT your_column FROM RankedData WHERE rn = (total_count + 1)/2)\n       END AS median\n   FROM RankedData\n   LIMIT 1;\n   ```\n\n   **解释:**\n\n   - `ROW_NUMBER() OVER (ORDER BY your_column)`: 为每一行按照 `your_column` 的值排序并生成行号。\n   - `COUNT(*) OVER ()`: 计算总共有多少行数据。\n   - `CASE WHEN ... THEN ... ELSE ... END`: 根据数据总量是奇数还是偶数来选择不同的计算方式。\n   - 偶数时，取中间两个行号的数值并计算平均值。\n   - 奇数时，取中间行号的数值。\n   - `LIMIT 1`: 因为所有行计算出的中位数是一样的，所以只需要返回一行。\n\n**2. 使用子查询和变量 (适用于 MySQL 5.x - 7.x, MariaDB):**\n\n   早期的 MySQL 版本没有窗口函数，需要使用变量来实现排序和行号计算。\n\n   ```sql\n   SET @row_number = 0;\n   SET @total_count = (SELECT COUNT(*) FROM your_table);\n\n   SELECT \n       CASE\n           WHEN @total_count % 2 = 0 THEN  -- 偶数个数据\n               (SELECT AVG(your_column) \n                FROM (SELECT @row_number := @row_number + 1 AS rn, your_column\n                      FROM your_table\n                      ORDER BY your_column) AS t\n                WHERE t.rn IN (@total_count/2, @total_count/2 + 1))\n           ELSE -- 奇数个数据\n               (SELECT your_column\n                FROM (SELECT @row_number := @row_number + 1 AS rn, your_column\n                      FROM your_table\n                      ORDER BY your_column) AS t\n                WHERE t.rn = (@total_count + 1)/2)\n       END AS median;\n   ```\n\n   **解释:**\n\n   - `@row_number` 和 `@total_count`: 定义变量来存储行号和总行数。\n   - `SELECT @row_number := @row_number + 1 AS rn`: 在子查询中计算行号。\n   - 其余部分逻辑与使用窗口函数的版本类似。\n\n**3. 使用 `PERCENTILE_CONT` 或 `PERCENTILE_DISC` (适用于 PostgreSQL, Oracle):**\n\n   这些函数可以直接计算指定百分位数的值，中位数就是 50% 百分位数。\n\n   ```sql\n   -- 使用 PERCENTILE_CONT (连续百分位数，结果可能是小数)\n   SELECT PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY your_column) AS median\n   FROM your_table;\n\n   -- 使用 PERCENTILE_DISC (离散百分位数，结果是实际数据中的值)\n   SELECT PERCENTILE_DISC(0.5) WITHIN GROUP (ORDER BY your_column) AS median\n   FROM your_table;\n   ```\n\n   **解释:**\n\n   - `PERCENTILE_CONT(0.5)`: 返回一个连续的 50% 百分位数，如果结果落在两个值之间，它会进行插值计算。\n   - `PERCENTILE_DISC(0.5)`: 返回一个离散的 50% 百分位数，它会返回排序后实际存在的值。\n\n**选择哪种方法？**\n\n- **推荐使用窗口函数:** 简单易懂，性能通常也比较好，适用于支持窗口函数的数据库。\n- **MySQL 5.x - 7.x 和 MariaDB 用户:** 只能使用变量和子查询的方式。\n- **如果数据库支持 `PERCENTILE_CONT` 或 `PERCENTILE_DISC`:**  使用这些函数是最简洁的方法。\n\n**注意事项:**\n\n- **性能:**  对于大型数据集，排序操作可能会比较耗时，需要根据实际情况进行优化。\n- **空值:**  需要考虑空值 (`NULL`) 的处理，通常在排序时会将其放在最前或最后，可以根据需求调整。\n- **数据类型:** 确保需要求中位数的列是数值类型。\n\n**总结：**\n\nSQL 中求中位数有多种方法，选择哪种取决于你使用的数据库系统以及你的偏好。理解核心思想（排序和确定中位数位置）是关键。希望这些示例和解释能帮助你开始在 SQL 中求中位数。记得根据你使用的具体数据库系统选择相应的方法。\n","tags":["medain"],"categories":["SQL"]},{"title":"typora图片上传解决办法","url":"/2024/12/08/typora图片上传解决办法/","content":"\n## 背景\n\n因为之前利用Oracle的免费服务器构建了一个图床服务，但由于每次写博客时复制粘贴完图片后都需要单独重新上传一遍图片再做图片链接替换整个过程相对较为繁琐且大大降低了博客编写的兴致。而且前几天在`linuxdo` 论坛上有刷到如何配置typora图片自动上传的一篇文章，今天想着研究一下如何实现这一功能。\n\n## 步骤\n\n1. 首先是查看typora提供的帮助文档实现custom文件上传脚本的学习\n\n   > Custom 定制\n   >\n   > You could config a custom command to upload images, using tools that is not listed in above options, or even write your own tools / scripts. Typora will append all images that needs to be uploaded after the custom command you filled.\n   > 你可以配置一个自定义命令来上传图片，使用上述选项中没有列出的工具，甚至可以编写自己的工具/脚本。Typora 会在你填写的自定义命令后附加所有需要上传的图片。\n   >\n   > Then, Typora will fetch image urls from the last N lines of the standard output of your custom command. (N is the number of images to upload).\n   > 然后，Typora 将从您自定义命令的标准输出的最后 N 行中获取图片 URL。（N 为要上传的图片数量）。\n   >\n   > For example, if you write a tool upload-image.sh, then you can input [some path]/upload-image.sh in the command filed. Typora will call [some path]/upload-image.sh \"image-path-1\" \"image-path-2\" to upload two images located in image-path-1 and image-path-2. Then the command may return something like:\n   > 例如，如果你编写一个工具 upload-image.sh ，那么你可以在命令字段中输入 [some path]/upload-image.sh 。Typora 将调用 [some path]/upload-image.sh \"image-path-1\" \"image-path-2\" 上传位于 image-path-1 和 image-path-2 的两个图片。然后命令可能会返回类似以下的内容：\n   >\n   > Upload Success:\n   > http://remote-image-1.png\n   > http://remote-image-2.png\n   > Then Typora will get the two remote image url from the output, and replace the original local images used in the Markdown document.\n   > 然后 Typora 将从输出中获取两个远程图片 URL，并替换 Markdown 文档中使用的原始本地图片。\n   >\n   > You could click the “Test Uploader” button to verify your custom commands.\n   > 你可以点击“测试上传器”按钮来验证你的自定义命令。\n   >\n   > Use current filename / filepath in custom commands\n   > 使用当前文件名/文件路径在自定义命令中\n   >\n   > You can use ${filename} and ${filepath} in your custom commands, they will be replace as the current markdown file name and current file path. For “untitled” files that have not been saved on your disk, they will be empty strings.\n   > 您可以在自定义命令中使用 ${filename} 和 ${filepath} ，它们将被替换为当前 Markdown 文件名和当前文件路径。对于尚未在您的磁盘上保存的“未命名”文件，它们将是空字符串。\n\n2. 图传上传命令实现主要是通过`post`\n\n   ```shell\n   curl -X POST \\\n   -H \"Authorization: Bearer {token}\" \\\n   -F \"/Users/eddieho/Downloads/image.png\" \\\n   https://hexo.kygoho.win/upload/\n   ```\n\n3. 根据上述内容实现汇总进行脚本编写\n\n   ```shell \n   #!/bin/bash\n   \n   # 替换成你的实际 token\n   TOKEN=\"your_actual_token\"\n   # 替换成你的实际上传 API 地址\n   API_URL=\"https://hexo.kygoho.win/upload/\"\n   \n   # 获取要上传的图片数量\n   IMAGE_COUNT=$#\n   \n   # 存储上传后的图片 URL\n   UPLOADED_URLS=()\n   \n   # 循环处理每个图片路径\n   for ((i=1; i<=IMAGE_COUNT; i++)); do\n     IMAGE_PATH=$1\n     shift\n   \n     # 使用 curl 上传图片\n     RESPONSE=$(curl -s -X POST \\\n       -H \"Authorization: Bearer ${TOKEN}\" \\\n       -F \"image=@${IMAGE_PATH}\" \\\n       \"${API_URL}\")\n   \n     # 使用 jq 提取 JSON 中的 url 字段\n     UPLOADED_URL=$(echo \"$RESPONSE\" | jq -r '.url')\n   \n     # 检查 URL 是否为空，如果为空，则输出错误信息并退出\n     if [ -z \"$UPLOADED_URL\" ]; then\n       echo \"Error: Upload failed for ${IMAGE_PATH}\"\n       echo \"$RESPONSE\"\n       exit 1\n     fi\n   \n     # 将上传后的 URL 添加到数组中\n     UPLOADED_URLS+=(\"$UPLOADED_URL\")\n   \n   done\n   \n   # 输出 Upload Success:\n   echo \"Upload Success:\"\n   \n   # 循环输出上传后的 URL\n   for URL in \"${UPLOADED_URLS[@]}\"; do\n     echo \"$URL\"\n   done\n   \n   exit 0\n   \n   ```\n\n## 总结\n\n通过这个脚本实现typora图片上传的自动配置\n\n![image-20241208161818867](http://hexo.kygoho.win/upload/uploads/030fef6e-231d-4961-8dde-e7fd8756c08c.png)\n","tags":["api","shell","typora"],"categories":["Shell"]},{"title":"修改hexo代码块主题样式","url":"/2024/11/20/修改hexo代码样式/","content":"\n## 背景\n最近在重新部署了Hexo博客，利用`bolt.new`重写了一个主题，整体效果就是现在这个博客的样子。在处理代码块这部分内容时出现了部分问题导致一直不断在重试报错，今天借机做一个小的总结。\n\n## Prism\nPrism 是用于语法高亮的 JavaScript 库。它主要用于在网页上以一种美观和易读的方式显示代码段。起初在引入`prism`是通过hexo插件`hexo-prism-plugin`来处理的，代码实现是：\n- 安装\n```shell\nnpm install hexo-prism-plugin --save\n```\n- 使用\n配置`_config.yml`文件\n```yaml\nprism_plugin:\n  mode: 'preprocess'    # realtime/preprocess\n  theme: 'tomorrow'\n  line_number: true    # default false\n```\n但这样配置完会出现花括号渲染成转义符号。\n\n现在的解决方案是：\n- 删除`hexo-prism-plugin`从`package.json`\n- 修改`_config.yml`文件\n    ```yaml\n     # prism_plugin:\n     #   mode: 'preprocess'    # realtime/preprocess\n     #   # theme: 'tomorrow'\n     #   line_number: true    # default false\n     highlight:\n     enable: false\n\n     prismjs:\n     enable: true\n     line_number: true\n    ```\n- 增加`prism.js`文件和`prism.css`文件\n    可从网站上进行下载\n    [js](https://prismjs.com/download.html#themes=prism-coy&languages=markup+css+clike+javascript)\n    [css](https://raw.githubusercontent.com/PrismJS/prism-themes/refs/heads/master/themes/prism-duotone-forest.css)\n\n- 修改`layout.ejs`文件\n    ```ejs\n    <!DOCTYPE html>\n    <html>\n    <head>\n    <meta charset=\"utf-8\">\n    <title><%= config.title %></title>\n    <link rel=\"stylesheet\" href=\"/css/style.css\">\n    <link rel=\"stylesheet\" href=\"/css/prism-code.css\">\n    <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css\">\n    <link rel=\"icon\" href=\"https://od.009100.xyz/api/raw/?path=/picture/Icon/shark.png\" type=\"image/x-icon\">\n    </head>\n    <body>\n    <header>\n    </header>\n\n    <main>\n    </main>\n\n    <footer>\n        <p>&copy; 2024 <%= config.title %></p>\n    </footer>\n\n    <script src=\"/js/search.js\"></script>\n    <script src=\"/js/main.js\"></script>\n    <script src=\"/js/copy-code.js\"></script>\n    <script src=\"/js/prism.js\"></script>\n\n    </body>\n    </html>\n    ```\n\n","tags":["hexo","js","css"],"categories":["Hexo"]},{"title":"Python 中环境变量的使用","url":"/2024/11/19/the_usage_of_denote_env/","content":"\n## 背景\n环境变量的使用很大程度上是基于隐藏敏感信息，避免敏感信息明文在代码中。实现环境变量的的读取有多种方法，而且不同时候会出现不同的使用方式。\n\n## 使用\n### 安装\n```python\npip install python-dotenv\n```\n### 创建`.env`文件\n\n```markdown\n   DATABASE_URL=postgres://user:password@localhost:5432/mydatabase\n   SECRET_KEY=mysecretkey\n```\n\n### 加载环境变量\n```python\n   from dotenv import load_dotenv\n   import os\n\n   load_dotenv()\n\n   database_url = os.getenv('DATABASE_URL')\n   secret_key = os.getenv('SECRET_KEY')\n\n   print(f\"Database URL: {database_url}\")\n   print(f\"Secret Key: {secret_key}\")\n```\n\n**注意**\n确保 .env 文件在正确的位置。默认情况下，load_dotenv() 会在当前工作目录查找 .env 文件。如果 .env 文件在其他位置，你可以指定路径\n```python\nload_dotenv(\"/完整/路径/到/.env\")\n```\n\n另外如果有引入Openai包的话，他会自动搜寻环境变量，不用取通过 `load_dotenv()`来加载。\n\n### export环境变量\n如果在终端设置了环境变量\n```shell\n   export DATABASE_URL=postgres://user:password@localhost:5432/mydatabase\n   export SECRET_KEY=mysecretkey\n```\n则可以直接读取环境变量\n```python\n   import os\n\n   database_url = os.getenv('DATABASE_URL')\n   secret_key = os.getenv('SECRET_KEY')\n\n   print(f\"Database URL: {database_url}\")\n   print(f\"Secret Key: {secret_key}\")\n```","tags":["python","env"],"categories":["Python"]},{"title":"ssh 密钥登陆服务器","url":"/2024/01/03/ssh_knowledge/","content":"\n### 登陆服务器后\n\n```shell\n[root@host ~]$ ssh-keygen  <== 建立密钥对\nGenerating public/private rsa key pair.\nEnter file in which to save the key (/root/.ssh/id_rsa): <== 按 Enter\nCreated directory '/root/.ssh'.\nEnter passphrase (empty for no passphrase): <== 输入密钥锁码，或直接按 Enter 留空\nEnter same passphrase again: <== 再输入一遍密钥锁码\nYour identification has been saved in /root/.ssh/id_rsa. <== 私钥\nYour public key has been saved in /root/.ssh/id_rsa.pub. <== 公钥\nThe key fingerprint is:\n0f:d3:e7:1a:1c:bd:5c:03:f1:19:f1:22:df:9b:cc:08 root@host\n```\n\n### 建立共钥\n\n```shell\n[root@host ~]$ cd .ssh\n[root@host .ssh]$ cat id_rsa.pub >> authorized_keys\n-- 更改权限\n[root@host .ssh]$ chmod 600 authorized_keys\n[root@host .ssh]$ chmod 700 ~/.ssh\n```\n\n### 关闭密码登陆权限\n\n```shell\nvi /etc/ssh/sshd_config\n```\n\n```shell\nPasswordAuthentication no\n```\n\n### 下载私钥\n\n```shell\ncat root/.ssh/id_rsa\n-- 直接复制，粘贴到文本编辑器中保存为无格式 shift + cmd +T\n-- 本地更改私钥权限\nchmod 600 id_rsa\n```\n\n### 登陆服务器\n\n```shell\nssh root@ip -p aaa -i /Users/eddieho/documents/hk666\n```\n","tags":["Linux","SSH"],"categories":["Linux"]},{"title":"linux 知识","url":"/2024/01/01/linux_knowledge/","content":"\n## 查看服务器情况\n\n```shell\n#查看硬盘使用情况\ndf -h\n#查看内存使用情况\nfree -h\n#查看块设备情况\nlsblk\n```\n\n## 服务器登陆\n\n```shell\n#ssh 用户名@ip地址 -p 端口号 -i 密钥地址\nssh root@206.190.232.205 -p 26838 -i /Users/eddieho/hx_for_hob/ssa/bwg_jp\n# 使用管理员权限\nsudo -i\n```\n\n## 安装相关依赖\n\n```shell\nyum -y install wget\nyum update -y && yum install curl -y\n```\n\n## 常用 vm 测试\n\n```shell\nwget -qO- bench.sh | bash\n```\n\n## 查看 ip\n\n```shell\n#查看公网ip\ncurl -s http://tnx.nl/ip\n#\nifconfig -a\n```\n\n## 文件夹操作\n\n```shell\n# 移动或者改名\nmv mynginx/ myblog/\n# 创建\nmkdir myblog\n# 删除\nrm myblog\nrm -rf blog\n```\n\n## 处理防火墙\n\n```shell\n#停止firewall\nsystemctl stop firewalld.service\n#禁止firewall开机启动\nsystemctl disable firewalld.service\n#关闭iptables\nservice iptables stop\n#去掉iptables开机启动\nchkconfig iptables off\n```\n\n## 环境变量\n\n```shell\n# 直接用export命令\nexport PATH=$PATH:/opt/au1200_rm/build_tools/bin\n# 修改profile文件\nvi /etc/profile\nexport PATH=\"$PATH:/opt/au1200_rm/build_tools/bin\"\nsource /etc/profile\n# 修改.bashrc文件\n vi /root/.bashrc\nexport PATH=\"$PATH:/opt/au1200_rm/build_tools/bin\"\n# 修改.bashrc\nvi ~/.bashrc\nalias v2ray=/usr/local/sbin/v2ray\n```\n\n## 日期&&时间\n\n```sh\n# 当前时间\ndate\n- Tue 14 Nov 2023 04:13:21 PM UTC\n# 当前时区\ntimedatectl\n- Local time: Tue 2023-11-14 16:14:00 UTC\n           Universal time: Tue 2023-11-14 16:14:00 UTC\n                 RTC time: Tue 2023-11-14 16:14:01\n                Time zone: Etc/UTC (UTC, +0000)\n- System clock synchronized: yes\n              NTP service: active\n          RTC in local TZ: no\n\n# 修改时区为上海\ntimedatectl set-timezone Asia/Shanghai\n```\n","tags":["linux"],"categories":["Linux"]},{"title":"python学习记录","url":"/2023/11/30/Python学习记录/","content":"## 1\n\n1. 数据创建和读取\n2. 数据查看和选择\n3. 数据清洗和处理\n4. 数据统计和聚合\n5. 数据合并和重塑\n\n```python \nimport pandas as pd\nimport numpy as np\n\n# 1. 创建数据\ndf = pd.DataFrame({\n    '姓名': ['张三', '李四', '王五', '赵六'],\n    '年龄': [25, 30, 35, 40],\n    '城市': ['北京', '上海', '广州', '深圳'],\n    '工资': [10000, 20000, 15000, 25000]\n})\n\n# 2. 基础查看方法\ndf.head()        # 查看前几行\ndf.shape        # 查看维度\ndf.info()       # 查看基本信息\ndf.describe()   # 统计描述\n\n# 3. 数据选择\ndf['姓名']                  # 选择单列\ndf[['姓名', '年龄']]        # 选择多列\ndf.loc[0]                  # 按标签选择行\ndf.iloc[0:2]              # 按位置选择行\ndf.loc[df['年龄'] > 30]     # 条件筛选\n\n# 4. 数据处理\ndf['工资'].fillna(0)        # 填充空值\ndf.drop_duplicates()      # 删除重复行\ndf['工资等级'] = df['工资'].apply(lambda x: '高' if x > 20000 else '低')  # 添加新列\n\n# 5. 统计聚合\ndf.groupby('城市')['工资'].mean()    # 分组计算平均值\ndf.sort_values('工资', ascending=False)  # 排序\ndf['工资'].value_counts()           # 计数统计\n\n# 6. 数据合并\ndf2 = pd.DataFrame({\n    '城市': ['北京', '上海', '广州', '深圳'],\n    '人口': [2100, 2400, 1600, 1700]\n})\npd.merge(df, df2, on='城市')        # 合并数据框\n\n# 7. 数据重塑\ndf.pivot_table(                    # 透视表\n    values='工资',\n    index='城市',\n    aggfunc='mean'\n)\n```\n\n## 2\n\n1. 创建含缺失值的示例数据\n2. 展示isnull/isna检测方法\n3. 展示dropna删除方法\n4. 展示drop删除行列方法\n\n```python\nimport pandas as pd\nimport numpy as np\n\n# 创建带有缺失值的数据框\ndf = pd.DataFrame({\n    '姓名': ['张三', '李四', np.nan, '赵六'],\n    '年龄': [25, np.nan, 35, 40],\n    '城市': ['北京', '上海', None, '深圳'],\n    '工资': [10000, 20000, np.nan, 25000]\n})\n\n# 1. isnull()/isna() - 检测缺失值\nprint(\"检查缺失值:\")\nprint(df.isnull())  # 返回布尔矩阵\nprint(\"\\n缺失值统计:\")\nprint(df.isnull().sum())  # 每列缺失值计数\n\n# 2. dropna() - 删除缺失值\n# 删除任何含有缺失值的行\nprint(\"\\n删除所有含缺失值的行:\")\nprint(df.dropna())\n\n# 删除全部为缺失值的行\nprint(\"\\n删除全部为缺失值的行:\")\nprint(df.dropna(how='all'))\n\n# 仅当特定列有缺失值时删除\nprint(\"\\n仅当年龄列缺失时删除:\")\nprint(df.dropna(subset=['年龄']))\n\n# 3. drop() - 删除指定行或列\n# 删除指定列\nprint(\"\\n删除'工资'列:\")\nprint(df.drop(columns=['工资']))\n\n# 删除指定行（基于索引）\nprint(\"\\n删除第0行:\")\nprint(df.drop(index=0))\n\n# 删除多个列\nprint(\"\\n删除多个列:\")\nprint(df.drop(columns=['工资', '城市']))\n```\n\n**检测缺失值:**\n\n- `isnull()`/`isna()`: 检测每个元素是否为缺失值\n- `isnull().sum()`: 统计每列缺失值数量\n\n**dropna()参数:**\n\n- `axis=0/1`: 0删除行，1删除列\n- `how='any'/'all'`: any-任何缺失值都删除，all-全部缺失才删除\n- `subset=[列名]`: 仅检查指定列的缺失值\n- `thresh=n`: 至少有n个非空值才保留\n\n**drop()参数:**\n\n- `labels`: 要删除的行标签或列名\n- `axis=0/1`: 0删除行，1删除列\n- `index`: 要删除的行索引\n- `columns`: 要删除的列名\n- `inplace=True/False`: 是否修改原数据\n\n## 3\n\n1. 数据转换方法\n2. 索引操作\n3. 时间序列处理\n4. 字符串处理\n\n```python\nimport pandas as pd\nimport numpy as np\n\n# 创建示例数据\ndf = pd.DataFrame({\n    '日期': pd.date_range('2024-01-01', periods=4),\n    '产品': ['A产品', 'B产品', 'A产品', 'B产品'],\n    '数量': [100, 200, 150, 250],\n    '价格': [10.5, 20.5, 15.5, 25.5],\n    '描述': ['好评-优秀', '好评-良好', '差评-一般', '好评-优秀']\n})\n\n# 1. 数据转换\n# astype() - 类型转换\ndf['数量'] = df['数量'].astype('float64')\n\n# reset_index() - 重置索引\ndf_reset = df.reset_index(drop=True)\n\n# set_index() - 设置索引\ndf_indexed = df.set_index('日期')\n\n# 2. 字符串操作 (str)\n# 分割字符串\ndf['评价类型'] = df['描述'].str.split('-').str[0]\ndf['评价等级'] = df['描述'].str.split('-').str[1]\n\n# 包含判断\nmask = df['描述'].str.contains('好评')\nprint(\"好评数据：\\n\", df[mask])\n\n# 3. 时间序列操作 (dt)\n# 提取日期组件\ndf['年份'] = df['日期'].dt.year\ndf['月份'] = df['日期'].dt.month\ndf['星期'] = df['日期'].dt.day_name()\n\n# 4. 数据计算\n# 算术运算\ndf['总价'] = df['数量'] * df['价格']\n\n# 累计统计\ndf['累计数量'] = df['数量'].cumsum()\n\n# 5. 数据替换\n# replace() - 值替换\ndf['产品'] = df['产品'].replace({'A产品': 'A', 'B产品': 'B'})\n\n# 6. 数据排序\n# sort_values() - 多列排序\ndf_sorted = df.sort_values(['产品', '数量'], ascending=[True, False])\n\n# 7. 数据采样\n# sample() - 随机采样\ndf_sample = df.sample(n=2)  # 随机抽取2行\n\n# 8. 数据统计\nprint(\"\\n基本统计：\")\nprint(df.agg({\n    '数量': ['sum', 'mean', 'max', 'min'],\n    '价格': ['mean', 'std']\n}))\n```\n\n**数据转换：**\n\n- `astype()`: 类型转换\n- `reset_index()`: 重置索引\n- `set_index()`: 设置索引\n\n**字符串操作：**\n\n- `str.split()`: 分割字符串\n- `str.contains()`: 包含判断\n- `str.replace()`: 替换字符串\n\n**时间序列：**\n\n- `dt.year/month/day`: 提取日期组件\n- `dt.day_name()`: 获取星期名称\n\n**数据计算：**\n\n- 直接算术运算\n- `cumsum()/cumprod()`: 累计统计\n- `agg()`: 聚合统计\n\n**其他操作：**\n\n- `replace()`: 值替换\n- `sort_values()`: 排序\n- `sample()`: 随机采样\n\n## 4\n\n1. 高级分组操作\n2. 窗口函数\n3. 数据重塑操作\n4. 多索引处理\n\n```python\nimport pandas as pd\nimport numpy as np\n\n# 创建示例数据\ndf = pd.DataFrame({\n    '部门': ['技术', '技术', '销售', '销售', '技术', '销售'],\n    '日期': pd.date_range('2024-01-01', periods=6),\n    '姓名': ['张三', '李四', '王五', '赵六', '张三', '王五'],\n    '销售额': [1000, 2000, 3000, 4000, 1500, 3500],\n    '成本': [800, 1500, 2000, 3000, 1200, 2500]\n})\n\n# 1. 高级分组操作\n# transform - 广播计算结果\ndf['部门平均销售额'] = df.groupby('部门')['销售额'].transform('mean')\n\n# agg - 多重聚合\nresult = df.groupby('部门').agg({\n    '销售额': ['sum', 'mean', 'count'],\n    '成本': ['sum', 'mean']\n})\n\n# 2. 窗口函数\n# rolling - 滚动窗口\ndf['销售额_3日均值'] = df['销售额'].rolling(window=3).mean()\n\n# expanding - 扩展窗口\ndf['销售额_累计均值'] = df['销售额'].expanding().mean()\n\n# 3. 数据重塑\n# pivot - 数据透视\npivot_table = df.pivot_table(\n    values='销售额',\n    index='部门',\n    columns='姓名',\n    aggfunc='sum',\n    fill_value=0\n)\n\n# melt - 宽转长\nmelted_df = pivot_table.reset_index().melt(\n    id_vars=['部门'],\n    var_name='姓名',\n    value_name='销售额'\n)\n\n# 4. 多索引操作\n# 创建多索引DataFrame\nmulti_idx = pd.MultiIndex.from_product([\n    ['技术', '销售'],\n    ['一季度', '二季度']\n], names=['部门', '季度'])\n\ndf_multi = pd.DataFrame(\n    np.random.randn(4, 2),\n    index=multi_idx,\n    columns=['收入', '支出']\n)\n\n# 多索引选择\ntech_data = df_multi.loc['技术']\nq1_data = df_multi.xs('一季度', level='季度')\n\n# 5. 高级计算\n# pct_change - 计算环比变化\ndf['销售额环比'] = df['销售额'].pct_change()\n\n# diff - 计算差值\ndf['销售额增量'] = df['销售额'].diff()\n\n# shift - 数据移位\ndf['上期销售额'] = df['销售额'].shift(1)\n```\n\n**高级分组：**\n\n- `transform()`: 广播聚合结果\n- `agg()`: 多重聚合运算\n\n**窗口函数：**\n\n- `rolling()`: 滑动窗口计算\n- `expanding()`: 扩展窗口计算\n\n**数据重塑：**\n\n- `pivot_table()`: 数据透视表\n- `melt()`: 宽格式转长格式\n\n**多索引操作：**\n\n- `MultiIndex`: 创建多级索引\n- `xs()`: 跨层级选择数据\n\n**高级计算：**\n\n- `pct_change()`: 计算变化百分比\n- `diff()`: 计算差值\n- `shift()`: 数据移位","tags":["python"],"categories":["Python"]},{"title":"Configuring Xray with VLESS, Reality and gRPC","url":"/2023/11/01/Configuring Xray with VLESS, Reality, and gRPC/","content":"This guide outlines setting up a secure and efficient Xray connection using the VLESS protocol with Reality and gRPC. This method offers several advantages:\n\n* **Enhanced Security:**  Benefits from the inherent security of VLESS and Reality protocols.\n* **Simplified Setup:** Eliminates the need for SSL certificates.\n* **Exposure to New Technologies:** Provides an opportunity to learn and implement cutting-edge protocols like gRPC.\n\n### Prerequisites\n\n* A server running a compatible operating system (e.g., Linux).\n* Root access to the server.\n\n### Step 1: Installing Xray\n\nUse the following command to install the latest beta version of Xray on your server:\n\n```bash\nbash -c \"$(curl -L https://github.com/XTLS/Xray-install/raw/main/install-release.sh)\" @ install --beta -u root\n```\n\n### Step 2: Generating Credentials\n\nGenerate the necessary credentials for your Xray configuration:\n\n```bash\nxray uuid # Generate a UUID\nxray x22519 # Generate a X25519 key pair\n# Choose a short ID (between 1 and 16 characters long, using characters 1-F)\n# Example:\nb1 \n```\n\n### Step 3: Finding a Suitable Website for Reality\n\nIdentify a website that supports HTTP/2 and TLS 1.3, preferably with an X25519 certificate. You can use the following resources to find a suitable website:\n\n* **GitHub Issue:** [https://github.com/XTLS/Xray-core/issues/2005](https://github.com/XTLS/Xray-core/issues/2005)\n\n* **BGP.tools:** [https://bgp.tools/](https://bgp.tools/)\n\n   1. Open [https://bgp.tools/](https://bgp.tools/).\n   2. Input your VPS IP address and search.\n   3. Navigate to the \"DNS\" tab.\n   4. Choose the \"Show Forward DNS\" option.\n   5. Select a few domains from the list and use your browser's developer tools to verify if they use TLS 1.3 and an X25519 certificate.\n\n   Ideally, the chosen website should be in the same IP range as your server, relatively unknown (for better privacy), and have low latency.\n   \n* Check command\n\n   ```shell\n   curl -I --tlsv1.3 --http2 https://englishdog.xyz\n   ```\n   \n   ```shell\n   #return 200 means successful\n   HTTP/2 200\n   server: nginx\n   date: Mon, 03 Jun 2024 13:43:42 GMT\n   content-type: text/html; charset=utf-8\n   content-length: 12181\n   last-modified: Wed, 08 May 2024 12:16:39 GMT\n   vary: Accept-Encoding\n   etag: \"663b6d27-2f95\"\n   x-xss-protection: 1; mode=block\n   x-content-type-options: nosniff\n   referrer-policy: no-referrer-when-downgrade\n   content-security-policy: default-src 'self' http: https: ws: wss: data: blob: 'unsafe-inline'; frame-ancestors 'self';\n   permissions-policy: interest-cohort=()\n   strict-transport-security: max-age=31536000; includeSubDomains\n   accept-ranges: bytes\n   ```\n   \n   \n\n### Step 4: Configuring the Server\n\nConfigure your Xray server by modifying the `config.json` file located at `/usr/local/etc/xray/config.json`.  You can find example configurations in this GitHub repository: [https://github.com/chika0801/Xray-examples/tree/main/VLESS-gRPC-REALITY](https://github.com/chika0801/Xray-examples/tree/main/VLESS-gRPC-REALITY).  \n\n**Remember to replace the placeholders in the example configurations with your generated credentials and chosen website details.**\n\n### Step 5: Configuring the Client\n\nConfigure your Xray client with the corresponding settings from the server configuration.  Below is an example client configuration. Be sure to adjust it to your specific needs and match the settings with your server configuration.\n\n```json\n{\n  \"dns\": {\n    \"servers\": [\n      {\n        \"tag\": \"remote\",\n        \"address\": \"https://1.1.1.1/dns-query\",\n        \"detour\": \"vless-out\"\n      },\n      {\n        \"tag\": \"local\",\n        \"address\": \"https://223.5.5.5/dns-query\",\n        \"detour\": \"direct\"\n      },\n      {\n        \"address\": \"rcode://success\",\n        \"tag\": \"block\"\n      }\n    ],\n    \"rules\": [\n      {\n        \"outbound\": [ \"any\" ],\n        \"server\": \"local\"\n      },\n      {\n        \"disable_cache\": true,\n        \"geosite\": [ \"category-ads-all\" ],\n        \"server\": \"block\"\n      },\n      {\n        \"clash_mode\": \"global\",\n        \"server\": \"remote\"\n      },\n      {\n        \"clash_mode\": \"direct\",\n        \"server\": \"local\"\n      },\n      {\n        \"geosite\": \"cn\",\n        \"server\": \"local\"\n      }\n    ],\n    \"strategy\": \"prefer_ipv4\"\n  },\n  \"inbounds\": [\n    {\n      \"type\": \"tun\",\n      \"inet4_address\": \"172.19.0.1/30\",\n      \"inet6_address\": \"2001:0470:f9da:fdfa::1/64\",\n      \"sniff\": true,\n      \"sniff_override_destination\": true,\n      \"domain_strategy\": \"prefer_ipv4\",\n      \"stack\": \"mixed\",\n      \"strict_route\": true,\n      \"mtu\": 9000,\n      \"endpoint_independent_nat\": true,\n      \"auto_route\": true\n    },\n    {\n      \"type\": \"socks\",\n      \"tag\": \"socks-in\",\n      \"listen\": \"127.0.0.1\",\n      \"sniff\": true,\n      \"sniff_override_destination\": true,\n      \"domain_strategy\": \"prefer_ipv4\",\n      \"listen_port\": 1087,\n      \"users\": []\n    },\n    {\n      \"type\": \"mixed\",\n      \"tag\": \"mixed-in\",\n      \"sniff\": true,\n      \"sniff_override_destination\": true,\n      \"domain_strategy\": \"prefer_ipv4\",\n      \"listen\": \"127.0.0.1\",\n      \"listen_port\": 1088,\n      \"users\": []\n    }\n  ],\n  \"log\": {\n    \"disabled\": false,\n    \"level\": \"info\",\n    \"timestamp\": true\n  },\n  \"outbounds\": [\n    {\n      \"type\": \"vless\",\n      \"tag\": \"vless-out\",\n      \"server\": \"{your_server_ip}\",\n      \"server_port\": 443,\n      \"uuid\": \"{your_uuid}\",\n      \"tls\": {\n        \"enabled\": true,\n        \"server_name\": \"{your_domain}\",\n        \"insecure\": false,\n        \"reality\": {\n          \"enabled\": true,\n          \"public_key\": \"{your_public_key}\",\n          \"short_id\": \"{your_short_id}\"\n        },\n        \"utls\": {\n          \"enabled\": true,\n          \"fingerprint\": \"chrome\"\n        }\n      },\n      \"transport\": {\n        \"type\": \"grpc\",\n        \"service_name\": \"{your_service_name}\",\n        \"idle_timeout\": \"60s\",\n        \"ping_timeout\": \"20s\"\n      }\n    },\n    {\n      \"tag\": \"direct\",\n      \"type\": \"direct\"\n    },\n    {\n      \"tag\": \"block\",\n      \"type\": \"block\"\n    },\n    {\n      \"tag\": \"dns-out\",\n      \"type\": \"dns\"\n    }\n  ],\n  \"route\": {\n    \"auto_detect_interface\": true,\n    \"rules\": [\n      {\n        \"geosite\": \"category-ads-all\",\n        \"outbound\": \"block\"\n      },\n      {\n        \"outbound\": \"dns-out\",\n        \"protocol\": \"dns\"\n      },\n      {\n        \"clash_mode\": \"direct\",\n        \"outbound\": \"direct\"\n      },\n      {\n        \"clash_mode\": \"global\",\n        \"outbound\": \"vless-out\"\n      },\n      {\n        \"geoip\": [ \"cn\", \"private\" ],\n        \"outbound\": \"direct\"\n      },\n      {\n        \"geosite\": \"geolocation-!cn\",\n        \"outbound\": \"vless-out\"\n      },\n      {\n        \"geosite\": \"cn\",\n        \"outbound\": \"direct\"\n      }\n    ]\n  }\n}\n```\n\n**Replace the following placeholders with your specific information:**\n\n* `{your_server_ip}`: Your server's IP address.\n* `{your_uuid}`: The UUID generated in Step 2.\n* `{your_domain}`: The domain of the website you chose in Step 3.\n* `{your_public_key}`: The public key generated in Step 2.\n* `{your_short_id}`: The short ID you chose in Step 2.\n* `{your_service_name}`:  The gRPC service name configured on your server.\n\n### Conclusion\n\nBy following these steps, you will have successfully configured Xray with VLESS, Reality, and gRPC. This setup provides a secure and efficient way to browse the internet while benefiting from the latest advancements in network protocols. Remember to keep your configuration files secure and choose a strong password for your Xray client.","tags":["xray","vless","grpc","singbox"],"categories":["Linux"]},{"title":"SingBox DNS Configuration","url":"/2023/11/01/DNS_Configuration/","content":"\n## DNS Configuration\n\nThere are generally three types of DNS servers: remote, local, and blocking.\n\n**Remote DNS servers** are public DNS services offered by companies like Google and Cloudflare. Examples include:\n\n```json\n{\n  \"tag\": \"google\",\n  \"address\": \"tls://8.8.8.8\",\n  \"detour\": \"Proxy\"\n},\n{\n  \"tag\": \"cloudflare\",\n  \"address\": \"https://1.1.1.1/dns-query\",\n  \"detour\": \"Proxy\"\n}\n```\n\n**Local DNS servers** are typically used to resolve domain names within a local network:\n\n```json\n{ \n  \"tag\": \"local-dns\",\n  \"address\": \"tls://223.5.5.5\", \n  \"detour\": \"direct\"\n}\n```\n\n**Blocking DNS servers** are used to block access to specific domains:\n\n```json\n{\n  \"tag\": \"block-dns\",\n  \"address\": \"rcode://success\"\n}\n```\n\n**Fake IP DNS** can be used to potentially speed up connections by reducing DNS requests:\n\n```json\n{\n  \"tag\": \"fakeip-dns\",\n  \"address\": \"fakeip\"\n}\n```\n\n## Other Configuration Options\n\nBesides server information, you can configure:\n\n* **DNS Rules:** Define the order of DNS server queries and rules for specific domains.\n* **Strategy:** Specify the preferred DNS record type (A or AAAA).\n* **Fake IP:** Define IP address ranges for the `fakeip-dns` server.\n\n### Fake IP Configuration\n\n```json\n{\n\"enabled\": true,\n\"inet4_range\": \"198.18.0.0/15\",\n\"inet6_range\": \"fc00::/18\"\n}\n```\n\n### DNS Rule Example\n\n```json\n{\n    \"rule_set\": [ //advertisement match\n      \"geosite-adguard\"\n    ],\n    \"server\": \"block-dns\"\n  },\n  {\n    \"rule_set\": [ // geosit match\n      \"geosite-netflix\",\n      \"geosite-youtube\",\n      \"geosite-openai\",\n      \"geosite-speedtest\",\n      \"geosite-github\",\n      \"geosite-cloudflare\",\n      \"Gemini\",\n      \"geosite-google\",\n      \"geosite-tiktok\",\n      \"geosite-jable\"\n    ],\n    \"rewrite_ttl\": 1,\n    \"server\": \"fakeip-dns\"\n  },\n  {\n    \"domain_suffix\": [ //domain match\n      \"edu.cn\",\n      \"gov.cn\",\n      \"mil.cn\",\n      \"ac.cn\",\n      \"com.cn\",\n      \"net.cn\",\n      \"org.cn\",\n      \"中国\",\n      \"中國\"\n    ],\n    \"server\": \"local-dns\"\n  },\n  {\n    \"rule_set\": [ //domestic site match\n      \"geosite-cn\",\n      \"geosite-icloud@cn\",\n      \"geosite-apple@cn\"\n    ],\n    \"server\": \"local-dns\"\n  },\n\n  {\n    \"query_type\": [\n      \"A\",\n      \"AAAA\"\n    ],\n    \"rewrite_ttl\": 1,\n    \"server\": \"fakeip-dns\"\n  },\n  {\n    \"outbound\": \"any\",\n    \"server\": \"local-dns\"\n  }\n\n```\n\nThis rule `outboud: any, server:local` dictates that for any `outbound service`, the `local-dns` server will be used to resolve domain names. For example, if you have a VLESS outbound service, this rule ensures that the `local-dns` server is queried to find the IP address of the service's domain before establishing a connection. If the specified server is `fakeip-dns`, a fake IP address within the configured address range will be returned instead. ","tags":["singbox","dns"],"categories":["Linux"]},{"title":"Lowendtalk_rss监听storage_vps_offer","url":"/2023/11/01/Lowendtalk_rss监听storage_vps_offer/","content":"\n### 背景\n\n最近开始玩pt，想要看看有没有合适又便宜的storage vps，所以通过chat-gpt写了个python脚本实现对于Lowendtalk offer板块的监听。逻辑大致是通过`feedparser` 获取rss内容找到大于最晚时间的文章标题查询是否包含storage，将rss中的最晚文章的时间进行保存，如果有则通过telegram api 实现消息推送。\n\n\n\n### 脚本\n\n```python\nimport feedparser\nimport requests\nfrom datetime import datetime, timezone\n\ndef notice(text):\n    api_url = \"api_url\"\n    data = {\n        \"chat_id\": \"-chat_id\",\n        \"text\": text\n    }\n    response = requests.post(api_url, json=data)\n    return response\n\ndef fetch_and_notify():\n    try:\n        # URL of the RSS feed\n        rss_url = \"https://lowendtalk.com/categories/offers/feed.rss\"\n\n        # Parse the RSS feed\n        feed = feedparser.parse(rss_url)\n\n        # Read the last published date from file\n        with open(\"/root/script/let_monitor/last_date.txt\", \"r\") as file:\n            last_date_str = file.read().strip()\n            last_date = datetime.strptime(last_date_str, '%a, %d %b %Y %H:%M:%S %z') if last_date_str else datetime.min.replace(tzinfo=timezone.utc)\n\n        # Initialize the latest date variable\n        latest_date = last_date\n\n        # Sort entries by publication date\n        entries = sorted(feed.entries, key=lambda e: datetime.strptime(e.published, '%a, %d %b %Y %H:%M:%S %z'), reverse=True)\n\n        # Iterate over sorted entries and send notifications for new entries\n        for entry in entries:\n            entry_date = datetime.strptime(entry.published, '%a, %d %b %Y %H:%M:%S %z')\n            if entry_date > last_date and 'storage' in entry.title.lower():\n                title = entry.title\n                link = entry.link\n                pub_date = entry.published\n\n                # Format the message to send\n                message = f\"#VPSOFFER👋👋\\nNew Post: {title}\\nLink: {link}\\nPublished on: {pub_date}\"\n\n                # Send the notification\n                response = notice(message)\n                print(f\"Notification sent for {title}, status code: {response.status_code}\")\n\n                # Update the latest date\n                if entry_date > latest_date:\n                    latest_date = entry_date\n\n        # Save the latest date to file\n        with open(\"/root/script/let_monitor/last_date.txt\", \"w\") as file:\n            file.write(latest_date.strftime('%a, %d %b %Y %H:%M:%S %z'))\n\n    except Exception as e:\n        error_message = f\"Error in fetch_and_notify: {str(e)}\"\n        notice(error_message)\n        print(error_message)\n\n# Call the function to start the process\nfetch_and_notify()\n\n```\n\n### feedparser 的用法\n\n```python\nimport feedparser\n\n# 1. 从 URL 获取 RSS/Atom 订阅内容\nfeed_url = \"https://www.example.com/feed.xml\"  # 替换成实际的 RSS/Atom 订阅链接\nfeed = feedparser.parse(feed_url)\n\n# 2. 访问订阅内容信息\nprint(\"订阅名称:\", feed.feed.title)\nprint(\"订阅链接:\", feed.feed.link)\nprint(\"订阅描述:\", feed.feed.description)\n\n# 3. 访问订阅内容中的条目\nfor entry in feed.entries:\n    print(\"条目标题:\", entry.title)\n    print(\"条目链接:\", entry.link)\n    print(\"条目发布时间:\", entry.published)\n    print(\"条目摘要:\", entry.summary)\n    # 访问其他条目信息，例如作者，标签，内容，等\n    # ...\n\n# 4. 访问条目内容\nfor entry in feed.entries:\n    # 访问完整的内容，如果可用\n    print(\"条目完整内容:\", entry.content[0].value)\n\n# 5. 处理错误\nif feed.status != 200:\n    print(\"错误:\", feed.bozo_exception)\n```\n\n**示例解释:**\n\n1. **获取订阅内容:** 使用 `feedparser.parse(feed_url)` 函数解析 RSS/Atom 订阅内容。\n2. **访问订阅信息:**  可以使用 `feed.feed.title`、`feed.feed.link` 和 `feed.feed.description` 访问订阅的标题、链接和描述信息。\n3. **访问条目:** `feed.entries` 包含订阅中的所有条目，你可以遍历它们并访问每个条目的信息。\n4. **访问条目内容:** 每个条目都包含 `title`、`link`、`published` 和 `summary` 属性，你可以根据需要访问其他属性。\n5. **处理错误:**  `feed.status` 表示 HTTP 状态码，`feed.bozo_exception` 表示解析错误信息。\n\n**其他常用用法:**\n\n* **指定解析器:**  可以使用 `feedparser.parse(feed_url, handlers={})`, 其中 `handlers` 是一个字典，用于指定自定义解析器。\n* **使用缓存:** 使用 `feedparser.parse(feed_url, cache=True)` 可以将订阅内容缓存起来，避免重复解析。\n* **处理图片:**  某些 RSS/Atom 订阅包含图片信息，可以使用 `entry.media_content` 访问它们。\n* **处理自定义字段:** 不同订阅可能有不同的自定义字段，可以使用 `entry.keys()` 查看可用字段，并根据需要访问它们。\n\n**示例代码中的注释部分包含更多示例用法，你可以根据需要进行修改。**\n","tags":["python","rss","vps"],"categories":["Python"]},{"title":"Secure Your Socks5 Proxy with Gost Authentication","url":"/2023/11/01/Secure Your Socks5 Proxy with Gost Authentication/","content":"\nToday, I received an email about IP abuse, likely due to my open Socks5 service on the server.  Without any authentication, anyone with the IP address and port could connect. \n\nTo enhance security, I've added authentication to my Gost service. \n\n### Configuration\n\nHere's how I configured Gost for secure Socks5 proxying:\n\n**1. config.json:**\n\n```json\n{\n    \"Debug\": true,\n    \"ServeNodes\": [\n        \"socks5://username:password@0.0.0.0:port\"\n    ]\n}\n```\n**Replace the following:**\n\n*  `username`: Your desired username\n*  `password`: Your desired password\n*  `port`: Your desired Socks5 port\n\n**2. docker-compose.yml:**\n\n```yaml\nversion: \"3\"\n\nservices:\n  gost:\n    image: ginuerzh/gost\n    restart: always\n    network_mode: \"host\"\n    volumes:\n      - ./config.json:/gost/config.json \n    command:\n      - \"-C=/gost/config.json\" \n    logging:\n      driver: \"json-file\"\n      options:\n        max-size: \"10m\"\n        max-file: \"3\"\n```\n\nThis configuration launches a Gost container, mounts the `config.json`, and sets it to restart automatically.\n\n### Application Example: IPv6 Forwarding with Hysteia2\n\nOne use case for a Socks5 proxy is forwarding IPv6 traffic for servers with only IPv4 connectivity. Here's an example using Hysteia2:\n\n```yaml\n#hysteia configuration\n  -acl: \n  \tinline:\n\t\t\t\t- so(::/0)  \n\t\t\t\t\n  - name: so\n    type: socks5\n    socks5:\n      addr: your-server-ip:your-gost-port\n      username: your-gost-username\n      password: your-gost-password\n```\n\n**Replace the placeholders with your actual Gost server information.**\n\n### Conclusion\n\nAdding authentication significantly improves the security of your Socks5 service.  After implementing it, I noticed several failed connection attempts from suspicious IPs in the Gost logs. \n","tags":["socks5","gost"],"categories":["Linux"]},{"title":"Atom 常用命令","url":"/2023/11/01/atom_cmd/","content":"\n### 开启设置面板\n```cmd + shift + p```\n\n### 查找文件/替换文件\n``` cmd + f / shift + cmd + f```\n\n","tags":["atom","commands"],"categories":["Others"]},{"title":"docker 知识","url":"/2023/11/01/docker教程/","content":"#### 安装\n\n```shell\n# 安装依赖包\nyum install -y yum-utils\n# 加 yum 软件源\nyum-config-manager \\\n     --add-repo \\\n     https://download.docker.com/linux/centos/docker-ce.repo\n# 更新 yum 软件源缓存，并安装 docker-ce\nyum install docker-ce docker-ce-cli containerd.io\n```\n\n``` shell\n# FirewallBackend=nftables 关闭防火墙\nFirewallBackend=iptables\n# 启动\nsystemctl enable docker\nsystemctl start docker\n```\n\n\n\n#### 获取镜像\n\n```shell\n# docker pull --help\n# docker pull 应用名：【标签】\ndocker pull [选项] [Docker Registry 地址[:端口号]/]仓库名[:标签]\n```\n\n\n\n#### 运行\n\n```shell\n docker run -it --rm ubuntu:18.04 bash\n docker exec -it nginx /bin/bash\n# -it:这是两个参数，一个是 -i：交互式操作，一个是 -t 终端。我们这里打算进入 bash 执行一些命令并查看返回结果，因此我们需要交互式终端。\n# --rm：这个参数是说容器退出后随之将其删除。默认情况下，为了排障需求，退出的容器并不会立即删除，除非手动 docker rm。我们这里只是随便执行个命令，看看结果，不需要排障和保留结果，因此使用 --rm 可以避免浪费空间。\n# ubuntu:18.04：这是指用 ubuntu:18.04 镜像为基础来启动容器\n# bash：放在镜像名后的是 命令，这里我们希望有个交互式 Shell，因此用的是 bash\n```\n\n#### 列出镜像\n\n``` shell\n$ docker image ls\nREPOSITORY           TAG                 IMAGE ID            CREATED             SIZE\nredis                latest              5f515359c7f8        5 days ago          183 MB\nnginx                latest              05a60462f8ba        5 days ago          181 MB\nmongo                3.2                 fe9198c04d62        5 days ago          342 MB\n<none>               <none>              00285df0df87        5 days ago          342 MB\nubuntu               18.04               329ed837d508        3 days ago          63.3MB\nubuntu               bionic              329ed837d508        3 days ago          63.3MB\n```\n\n> 列表包含了 `仓库名`、`标签`、`镜像 ID`、`创建时间` 以及 `所占用的空间`。\n>\n> 其中仓库名、标签在之前的基础概念章节已经介绍过了。**镜像 ID** 则是镜像的唯一标识，一个镜像可以对应多个 **标签**。因此，在上面的例子中，我们可以看到 `ubuntu:18.04` 和 `ubuntu:bionic` 拥有相同的 ID，因为它们对应的是同一个镜像。\n\n#### 镜像体积\n\n```shell\n#镜像、容器、数据卷所占用的空间\n$ docker system df\n\nTYPE                TOTAL               ACTIVE              SIZE                RECLAIMABLE\nImages              24                  0                   1.992GB             1.992GB (100%)\nContainers          1                   0                   62.82MB             62.82MB (100%)\nLocal Volumes       9                   0                   652.2MB             652.2MB (100%)\nBuild Cache                                                 0B                  0B\n```\n\n#### 删除本地镜像\n\n``` shell\n#先将运行容器暂停删除才能删除镜像\ndocker stop $(docker ps -aq)\ndocker rm $(docker ps -aq)\ndocker image rm $(docker images)\n$ docker image rm [选项] <镜像1> [<镜像2> ...]\n# 我们需要删除所有仓库名为 redis 的镜像\n$ docker image rm $(docker image ls -q redis)\n# 删除所有在 mongo:3.2 之前的镜像\n$ docker image rm $(docker image ls -q -f before=mongo:3.2)\n```\n\n#### dockerfile定制镜像\n\n``` shell\n$ mkdir mynginx\n$ cd mynginx\n$ touch Dockerfile\nvi Dockerfile\n#dockerfile 文件内容\nFROM nginx\nRUN echo '<h1>Hello, Docker!</h1>' > /usr/share/nginx/html/index.html\n#构建镜像\nbuild -t nginx:v3 .\n```\n\ndocker还能通过git、tar压缩包进行构建相应的镜像\n\n#### run执行命令\n\n- *shell* 格式：`RUN <命令>`，就像直接在命令行中输入的命令一样。刚才写的 Dockerfile 中的 `RUN` 指令就是这种格式。\n- *exec* 格式：`RUN [\"可执行文件\", \"参数1\", \"参数2\"]`，这更像是函数调用中的格式。\n\n#### copy复制文件\n\n- ``COPY [--chown=<user>:<group>] <源路径>... <目标路径>``\n- ``COPY [--chown=<user>:<group>] [\"<源路径1>\",... \"<目标路径>\"]``\n\n``` shell\nCOPY hom* /mydir/\nCOPY hom?.txt /mydir/\n```\n\n`<目标路径>` 可以是容器内的绝对路径，也可以是相对于工作目录的相对路径（工作目录可以用 `WORKDIR` 指令来指定）。目标路径不需要事先创建，如果目录不存在会在复制文件前先行创建缺失目录。\n\n此外，还需要注意一点，使用 `COPY` 指令，源文件的各种元数据都会保留。比如读、写、执行权限、文件变更时间等。这个特性对于镜像定制很有用。特别是构建相关文件都在使用 Git 进行管理的时候。\n\n#### add更高级的复制文件\n\n\n\n#### docker compose\n\n``` shell\n#二进制安装\ncurl -L https://github.com/docker/compose/releases/download/1.27.4/docker-compose-`uname -s`-`uname -m` > /usr/local/bin/docker-compose\n\n# 国内用户可以使用以下方式加快下载\n curl -L https://download.fastgit.org/docker/compose/releases/download/1.27.4/docker-compose-`uname -s`-`uname -m` > /usr/local/bin/docker-compose\n\n chmod +x /usr/local/bin/docker-compose\n```\n\n\n\n## docker 常用命令\n\n``` shel\ndocker stop $(docker ps -q)\ndocker rm $(docker ps -aq)\n```\n\n#### docker 实战wordpress\n\n```shell\nversion: \"3\"\nservices:\n\n   db:\n     image: mysql:8.0\n     command:\n      - --default_authentication_plugin=mysql_native_password\n      - --character-set-server=utf8mb4\n      - --collation-server=utf8mb4_unicode_ci\n     volumes:\n       - db_data:/var/lib/mysql\n     restart: always\n     environment:\n       MYSQL_ROOT_PASSWORD: 123456\n       MYSQL_DATABASE: wordpress\n       MYSQL_USER: wordpress\n       MYSQL_PASSWORD: wordpress\n\n   wordpress:\n     depends_on:\n       - db\n     image: wordpress:latest\n     ports:\n       - \"8000:80\"\n     restart: always\n     volumes:\n       - wp_data:/var/www/html\n     environment:\n       WORDPRESS_DB_HOST: db:3306\n       WORDPRESS_DB_USER: wordpress\n       WORDPRESS_DB_PASSWORD: wordpress\n   nginx:\n     depends_on:\n       - wordpress\n     image: nginx:latest\n     ports:\n       - \"80:80\"\n       - \"443:443\"\n     restart: always\n     volumes:\n       - ng_data:/etc/nginx\nvolumes:\n    db_data:\n    ng_data:\n    wp_data:\n```\n\n```sh\ndocker-compose up -d\n#后台执行\n```\n","tags":["docker","tutorial"],"categories":["Docker"]},{"title":"git 知识","url":"/2023/11/01/git学习/","content":"## git环境配置\n```shell\nssh -T git@github.com\n--首先配置config\ngit config --global user.name \"your name\"\ngit config --global user.email \"your_email@youremail.com\"\n\ngit config --global credential.helper store\ngit pull /git push \n```\n> **caution** git 密码目前不能使用只能使用口令\n\n## 推送文件\n\n```shell\n# 推文件\ngit init\ngit add . / 任意文件/文件夹（但必须非空）\ngit commit -m \"    \"\ngit remote add origin git@github.com:yourName/yourRepo.git\ngit push origin master\n```\n## 删除文件\n```shell\n-- 删文件\nrm file --先将文件删除\ngit status --查看状态\n# 如果文件是误删 可以通过版本进行恢复\ngit checkout\n# 如果需要将远程删除\ngit rm file.name\ngit commit -m \"\"\ngit push origin master\n```\n\n\n\n## 拉文件\n```shell\ngit clone repo.addr\ngit pull repo.addr\n```\n\n","tags":["tutorial","git"],"categories":["Git"]},{"title":"hexo博客pure主题安装记录","url":"/2023/11/01/hexo博客pure主题安装记录/","content":"\n## hexo插件安装\n\n其实在dockerfile定制安装了一部分插件，但由于当时使用的教程和目前使用的主题不一致故导致部分插件未能安装。\n\n- hexo-wordcount 文章数字计算\n- hexo-generator-json-content 内部搜索\n\n安装代码：\n\n```shell\nnpm install hexo-wordcount --save\nnpm install hexo-generator-json-content --save\n```\n\n> 需要注意的是安装路径需在博客文件根目录下即～/blog/hexo（这个坑一开始我没注意）\n\n## 修改icon和avatar 图像地址\n\n主题中图像都从本地路径加载。\n\n因上传麻烦且占用服务器资源故直接选择图传连接\n\n`avatar: https://od.xxxx.cf/api/raw/?path=/picture/Icon/fish.png`\n\n## 将Links（友链）改为service（服务）\n\n- 处理主题文件夹下的_config.yml文件\n\n```yaml\nmenu:\n  Home: .\n  #Archives: archives  # 归档\n  Categories: categories  # 分类\n  #Tags: tags  # 标签\n  #Repository: repository  # github repositories\n  #Books: books  # 豆瓣书单\n  Services: links  # Links 改成Service\n  About: about  # 关于\n\n# Enable/Disable menu icons\nmenu_icons:\n  enable: true  # 是否启用导航菜单图标\n  home: icon-home-fill\n  archives: icon-archives-fill\n  categories: icon-folder\n  tags: icon-tags\n  repository: icon-project\n  #books: icon-book-fill\n  services: icon-project\n  about: icon-book-fill\n```\n\n- 修改links index.md 路径：～/blog/hexo/source/links/index.md\n\n```markdown\n---\ntitle: 服务\nlayout: links\ncomments: true\nsidebar: none\n---\n```\n\n- 修改语言文件\n\n```yaml\nServices: 服务\nlinks-desc: 服务描述\n```\n\n## 删除文章版权信息\n\n修改layout/_partial/post/copyright.ejs 文件\n\n- 添加 `style=\"display: none;\"`\n- 修改条件判断`if(theme.profile && theme.profile.articleSelfBlock)`\n\n## 关闭评论板块\n\n修改_config.yml文件 将type 改成 false\n\n```yaml\ncomment:\n  type: false  # 启用哪种评论系统\n```\n\n## 文章目录开启\n\n根据sidebar.ejs的判断语句:\n\n```ejs\nif (!index && theme.config.toc && post.toc)\n```\n\n我们可以看出需要满足三个条件:\n\n- 不是index文件\n\n- 主题config toc 需要开启为true\n\n- post toc 需为true 即需要在每篇文章开头\n\n  ``` markdown\n  ---\n  title: hexo博客pure主题安装记录\n  categories: code\n  tags: hexo\n  toc: true #这个\n  ---\n  ```\n\n## 添加服务\n\n在source/_data/links.yml中按照格式添加即可\n\n```yaml\nJupyter:\n  link: https://hexo.kygoho.win/py\n  avatar: /images/favatar/chuangzaoshi-logo.png\n  desc: jupyter-notebook\n\nMonitor:\n  link: https://cloud.kygoho.win/\n  avatar: https://od.wadaho.cf/api/raw/?path=/picture/Icon/star.png\n  desc: service monitor panel\n\nRSS:\n  link: https://rss.009100.xyz\n  avatar: https://od.wadaho.cf/api/raw/?path=/picture/Icon/read.png\n  desc: RSS online reader\n```\n\n\n\n","tags":["hexo","theme"],"categories":["Hexo"]},{"title":"hexo 容器化部署","url":"/2023/11/01/hexo容器化部署/","content":"#### Dockerfile 构建hexo镜像\n```FROM node:latest\n# 维护者信息\nMAINTAINER eddie <kygoho@live.com>\n# 工作目录\nWORKDIR root/blog\n# 安装Hexo\nRUN npm init -y\\\n   && npm install hexo-cli -g \\\n   && npm install hexo-server \\\n   && hexo init hexo && cd hexo \\\n   && npm install \\\n   && npm install hexo-deployer-git \\\n# 设置git\n   && git config --global user.name \"eddiehex\" \\\n   && git config --global user.email \"kygoho@live.com\"\\\n   && npm i -S hexo-prism-plugin \\\n   && npm install hexo-generator-search --save \\\n   && npm i hexo-permalink-pinyin --save \\\n   && npm install hexo-generator-feed --save \\\n   && npm install hexo-deployer-git --saven\n# 映射端口\nEXPOSE 4000\nWORKDIR hexo\n# 运行命令\nCMD [\"/usr/bin/env\", \"hexo\", \"server\"]\n```\n#### 运行容器\n```\ndocker run -d -p 4000:4000 -v $PWD/hexo:/root/blog/hexo --name hexo hexo\n```\n笔记：\n\n- 最新的npm需要先确定workdir 才能安装\n- 需要先进行npm init初始化才能生成package.json 才能npm install\n- hexo init 需要在一个新的空文件\n","tags":["docker","hexo"],"categories":["Docker"]},{"title":"iphone手机库存监听","url":"/2023/11/01/iPhone手机库存监听/","content":"\n\n\n```python\nfrom enum import Flag\nfrom random import choice\nimport requests\nimport re\nfrom time import sleep\n\nimport requests\nimport time\n\nimport smtplib\nfrom email.mime.text import MIMEText\n\n# Telegram Bot API token and chat ID\ntelegram_api_token = 'bot_token'\ntelegram_chat_id = '-chat_id'\n\ndef sendTelegramMessage(message):\n    try:\n        url = f\"https://api.telegram.org/bot{telegram_api_token}/sendMessage\"\n        params = {\"chat_id\": telegram_chat_id, \"text\": message}\n        response = requests.post(url, params=params)\n        response.raise_for_status()\n        print('Telegram message sent successfully')\n    except Exception as e:\n        print('Error sending Telegram message:', e)\n\n\ndef AppleMonitor(flag0, count):\n    try:\n        #根据需要修改地址及产品id，location、product（parts.0=后面的）\n        location='辽宁 大连 中山区'\n        product='MTQA3CH/A'\n        #product='MPUW3CH/A'\n        url='https://www.apple.com.cn/shop/fulfillment-messages?pl=true&mts.0=regular&parts.0='+product+'&location='+location\n        print(url)\n        user_agent_list=['Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/536.5 (KHTML, like Gecko)','Mozilla/5.0 (Windows NT 6.1; WOW64; rv:11.0) Gecko/20100101 Firefox/11.0','Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; .NET4.0E)','Opera/9.80 (Windows NT 6.1; WOW64; U; zh-cn) Presto/2.10.229 Version/11.62']\n        kv = {'user-agent': choice(user_agent_list)}\n        r = requests.get(url, headers=kv)\n        r.raise_for_status()\n        pattern = re.compile('\"pickupDisplay\":\"(.*?)\"', re.S)\n        r.encoding = r.apparent_encoding\n        res=re.search(pattern,r.text)\n        if res.group(1) != 'unavailable':\n            if not flag0:\n                # Send a Telegram message when the product is available\n                sendTelegramMessage('AppleMonitor: ip15pro 大连有货！')\n                flag0 = True\n                count = 120\n\n            print('=======================================')\n            print('=======================================')\n            print('=======================================')\n            print('                有货！')\n            print('=======================================')\n            print('=======================================')\n            print('=======================================')\n            print('')\n\n        if flag0:\n            count -= 1\n            if count == 0:\n                flag0 = False\n\n        return flag0, count\n    except Exception as e:\n        print(\"失败:\", e)\n        return flag0, count\n\nif __name__ == '__main__':\n    flag0 = False\n    count = 120\n    while 1:\n        flag0, count = AppleMonitor(flag0, count)\n        # Add a sleep to avoid excessive requests\n        time.sleep(30)\n```\n\n","tags":["python","iphone","stock monitoring"],"categories":["Python"]},{"title":"如何编写简易登陆脚本&&安装iterm2和oh-my-zsh&&配置oh-my-zsh","url":"/2023/11/01/install_zsh_login_shell/","content":"\n最近看了几篇推荐mac软件的文章，其中多少有提到了iterm2再加上mac自带的终端相对有些局限就开始捣鼓iterm2和oh-my-zsh；\n\n#### 安装iterm2和oh-my-zsh\n\n- 安装iterm2\n\n搜索并直接下载\n\n- 安装oh-my-zsh\n\n``` shell\n#查看当前环境shell\necho $SHELL\n#查看系统自带shell\ncat /etc/shells\n#安装zsh\nyum install zsh # CentOS\nbrew install zsh # mac安装\n#将zsh设置默认\nchsh -s /bin/zsh # CentOS\n# Mac如下\n# 在 /etc/shells 文件中加入如下一行\n/usr/local/bin/zsh\n# 接着运行\nchsh -s /usr/local/bin/zsh\n```\n\n``` shell\n#安装oh-my-zsh\nsh -c \"$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\" #curl\nsh -c \"$(wget https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O -)\" #wget\n```\n\n```shell\n#在安装oh-my-zsh时会受到网络因素影响，故应先设置iterm2代理\nexport http_proxy=http://127.0.0.1:1087\nexport https_proxy=$http_proxy\n#关闭\nunset http_proxy https_proxy\n#在安装zsh后也可以写入.zshrc中\nalias goproxy='export http_proxy=http://127.0.0.1:1087 https_proxy=http://127.0.0.1:1087'\nalias disproxy='unset http_proxy https_proxy'\n#测试方式\ncurl cip.cc\n```\n\n\n\n- 配置zsh\n\nzsh的配置文件为.zshrc 路径为 ``` ~/.zshrc ``` 文件中可进行主题、环境变量、alias修改\n\n不过zsh的配置尽量在  ```  ~/.oh-my-zsh/custom/example.zsh  ``` 中进行修改\n\n``` shell\n#快速编辑配置文件\nalias zshconfig=\"vim ~/.oh-my-zsh/custom/example.zsh\"\n#配置主文件夹bin的环境变量\nexport PATH=~/bin:\"$PATH\"\n#启动配置文件\nsource ~/.oh-my-zsh/custom/example.zsh\n```\n\n#### 编写简易登陆脚本\n\n上一次学会了如何设置ssh私钥登陆，但是每一次都要复制登陆命令，整个行为极度繁琐，索性就想学习制作一个简易脚本可以直接登陆；\n\n- 编写shell文件\n\n``` shell\n#在主文件夹中创建一个文件夹bin 并创建一个文件 lg.sh ,并写入脚本\nmkdir bin\ntouch lg.sh\nvim lg.sh\n```\n\n```shell\n#登陆脚本\n#!/bin/bash\necho \"1) hk server\"\necho \"2) jp_gigs server\"\necho \"3) jp_bwg server\"\necho \"please input your choice:\"\nread num\nif test $num -eq 1\n        then ssh root@121.xxx.xxx.96 -i /Users/eddieho/hx_for_hob/ssa/hk666\nelif test $num -eq 2\n\tthen ssh root@103.xxx.xxx.96 -i /Users/eddieho/hx_for_hob/ssa/gigs_jp\nelse ssh root@206.xxx.xxx.205 -p 26838 -i /Users/eddieho/hx_for_hob/ssa/bwg_jp\nfi\n```\n\n- 为shell文件授予执行权限，并添加环境变量\n\n``` shell\nchmod +x ./lg.sh\n# 正常文件路径下可以直接执行，但想要全局的话就得增加环境变量\n# vshconfgi -- 最后添加下面这条命令\nexport PATH=~/bin:\"$PATH\"\n```\n\n这样就可直接用命令进行登陆了！！\n","tags":["iterm2","oh-my-zsh","zsh"],"categories":["Linux"]},{"title":"nginx ssl 配置","url":"/2023/11/01/nginx ssl/","content":"### 80端口访问\n\n```shell\nserver {\n  listen 80;\n  listen [::]:80;\n\n  # 域名\n  server_name domain.com  www.domain.com;\n\n  location / {\n    ## 定位到wordpress\n    proxy_pass http://wordpress;\n\n        proxy_http_version    1.1;\n        proxy_cache_bypass    $http_upgrade;\n\n        proxy_set_header Upgrade            $http_upgrade;\n        proxy_set_header Connection         \"upgrade\";\n        proxy_set_header Host                $host;\n        proxy_set_header X-Real-IP            $remote_addr;\n        proxy_set_header X-Forwarded-For    $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto    $scheme;\n        proxy_set_header X-Forwarded-Host    $host;\n        proxy_set_header X-Forwarded-Port    $server_port;\n  }\n}\n```\n### 443端口访问\n需要将证书文件放置nginx容器中\n\n```sh\nserver {\n    # 服务器端口使用443，开启ssl, 这里ssl就是上面安装的ssl模块\n    listen       443 ssl;\n    # 域名，多个以空格分开\n    server_name  domain.com www.domain.com;\n\n    # ssl证书地址\n    ssl_certificate     /etc/nginx/cert/6827606_www.domain.com.pem;  # pem文件的路径\n    ssl_certificate_key  /etc/nginx/cert/6827606_www.domain.com.key; # key文件的路径\n\n    # ssl验证相关配置\n    ssl_session_timeout  5m;    #缓存有效期\n    ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4;    #加密算法\n    ssl_protocols TLSv1 TLSv1.1 TLSv1.2;    #安全链接可选的加密协议\n    ssl_prefer_server_ciphers on;   #使用服务器端的首选算法\n\n    location / {\n    ## 定位到wordpress\n    proxy_pass http://wordpress;\n\n        proxy_http_version    1.1;\n        proxy_cache_bypass    $http_upgrade;\n\n        proxy_set_header Upgrade            $http_upgrade;\n        proxy_set_header Connection         \"upgrade\";\n        proxy_set_header Host                $host;\n        proxy_set_header X-Real-IP            $remote_addr;\n        proxy_set_header X-Forwarded-For    $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto    $scheme;\n        proxy_set_header X-Forwarded-Host    $host;\n        proxy_set_header X-Forwarded-Port    $server_port;\n    }\n}\nserver {\n    listen       80;\n    server_name  domain.com www.domain.com;\n    return 301 https://$server_name$request_uri;\n}\n\n```\n","tags":["nginx","ssl"],"categories":["Linux"]},{"title":"nginx 知识","url":"/2023/11/01/nginx/","content":"#### 配置文件详解\n\n​\t\tNginx的主配置文件是nginx.conf，这个配置文件一共由三部分组成，分别为**全局块、events块和http块**。在http块中，又包含http全局块、多个server块。每个server块中，可以包含server全局块和多个location块。在同一配置块中嵌套的配置块，各个之间不存在次序关系。\n\n​\t\t配置文件支持大量可配置的指令，绝大多数指令不是特定属于某一个块的。同一个指令放在不同层级的块中，其作用域也不同，一般情况下，高一级块中的指令可以作用于自身所在的块和此块包含的所有低层级块。如果某个指令在两个不同层级的块中同时出现，则采用“就近原则”，即以较低层级块中的配置为准。比如，<font color=red size=3>某指令同时出现在http全局块中和server块中，并且配置不同，则应该以server块中的配置为准。</font>\n\n整个配置文件的结构大致如下：\n\n``` shell\n#全局块\n#user  nobody;\nworker_processes  1;\n\n#event块\nevents {\n    worker_connections  1024;\n}\n\n#http块\nhttp {\n    #http全局块\n    include       mime.types;\n    default_type  application/octet-stream;\n    sendfile        on;\n    keepalive_timeout  65;\n    #server块\n    server {\n        #server全局块\n        listen       8000;\n        server_name  localhost;\n        #location块\n        location / {\n            root   html;\n            index  index.html index.htm;\n        }\n        error_page   500 502 503 504  /50x.html;\n        location = /50x.html {\n            root   html;\n        }\n    }\n    #这边可以有多个server块\n    server {\n      ...\n    }\n}\n```\n\n#### 全局块\n\n全局块是默认配置文件从开始到events块之间的一部分内容，主要设置一些影响Nginx服务器整体运行的配置指令，因此，这些指令的作用域是Nginx服务器全局。\n\n通常包括配置运行Nginx服务器的用户（组）、允许生成的worker process数、Nginx进程PID存放路径、日志的存放路径和类型以及配置文件引入等。\n\n``` shell\n# 指定可以运行nginx服务的用户和用户组，只能在全局块配置\n# user [user] [group]\n# 将user指令注释掉，或者配置成nobody的话所有用户都可以运行\n# user nobody nobody;\n# user指令在Windows上不生效，如果你制定具体用户和用户组会报小面警告\n# nginx: [warn] \"user\" is not supported, ignored in D:\\software\\nginx-1.18.0/conf/nginx.conf:2\n\n# 指定工作线程数，可以制定具体的进程数，也可使用自动模式，这个指令只能在全局块配置\n# worker_processes number | auto；\n# 列子：指定4个工作线程，这种情况下会生成一个master进程和4个worker进程\n# worker_processes 4;\n\n# 指定pid文件存放的路径，这个指令只能在全局块配置\n# pid logs/nginx.pid;\n\n# 指定错误日志的路径和日志级别，此指令可以在全局块、http块、server块以及location块中配置。(在不同的块配置有啥区别？？)\n# 其中debug级别的日志需要编译时使用--with-debug开启debug开关\n# error_log [path] [debug | info | notice | warn | error | crit | alert | emerg]\n# error_log  logs/error.log  notice;\n# error_log  logs/error.log  info;\n```\n\n#### events块\n\nevents块涉及的指令主要影响Nginx服务器与用户的网络连接。常用到的设置包括是否开启对多worker process下的网络连接进行序列化，是否允许同时接收多个网络连接，选取哪种事件驱动模型处理连接请求，每个worker process可以同时支持的最大连接数等。\n\n这一部分的指令对Nginx服务器的性能影响较大，在实际配置中应该根据实际情况灵活调整。\n\n``` shell\n# 当某一时刻只有一个网络连接到来时，多个睡眠进程会被同时叫醒，但只有一个进程可获得连接。如果每次唤醒的进程数目太多，会影响一部分系统性能。在Nginx服务器的多进程下，就有可能出现这样的问题。\n# 开启的时候，将会对多个Nginx进程接收连接进行序列化，防止多个进程对连接的争抢\n# 默认是开启状态，只能在events块中进行配置\n# accept_mutex on | off;\n\n# 如果multi_accept被禁止了，nginx一个工作进程只能同时接受一个新的连接。否则，一个工作进程可以同时接受所有的新连接。\n# 如果nginx使用kqueue连接方法，那么这条指令会被忽略，因为这个方法会报告在等待被接受的新连接的数量。\n# 默认是off状态，只能在event块配置\n# multi_accept on | off;\n\n# 指定使用哪种网络IO模型，method可选择的内容有：select、poll、kqueue、epoll、rtsig、/dev/poll以及eventport，一般操作系统不是支持上面所有模型的。\n# 只能在events块中进行配置\n# use method\n# use epoll\n\n# 设置允许每一个worker process同时开启的最大连接数，当每个工作进程接受的连接数超过这个值时将不再接收连接\n# 当所有的工作进程都接收满时，连接进入logback，logback满后连接被拒绝\n# 只能在events块中进行配置\n# 注意：这个值不能超过超过系统支持打开的最大文件数，也不能超过单个进程支持打开的最大文件数，具体可以参考这篇文章：https://cloud.tencent.com/developer/article/1114773\n# worker_connections  1024;\n```\n\n#### http块\n\nhttp块是Nginx服务器配置中的重要部分，代理、缓存和日志定义等绝大多数的功能和第三方模块的配置都可以放在这个模块中。\n\n前面已经提到，http块中可以包含自己的全局块，也可以包含server块，server块中又可以进一步包含location块，在本书中我们使用“http全局块”来表示http中自己的全局块，即http块中不包含在server块中的部分。\n\n可以在http全局块中配置的指令包括文件引入、MIME-Type定义、日志自定义、是否使用sendfile传输文件、连接超时时间、单连接请求数上限等。\n\n```shell\n# 常用的浏览器中，可以显示的内容有HTML、XML、GIF及Flash等种类繁多的文本、媒体等资源，浏览器为区分这些资源，需要使用MIME Type。换言之，MIME Type是网络资源的媒体类型。Nginx服务器作为Web服务器，必须能够识别前端请求的资源类型。\n\n# include指令，用于包含其他的配置文件，可以放在配置文件的任何地方，但是要注意你包含进来的配置文件一定符合配置规范，比如说你include进来的配置是worker_processes指令的配置，而你将这个指令包含到了http块中，着肯定是不行的，上面已经介绍过worker_processes指令只能在全局块中。\n# 下面的指令将mime.types包含进来，mime.types和ngin.cfg同级目录，不同级的话需要指定具体路径\n# include  mime.types;\n\n# 配置默认类型，如果不加此指令，默认值为text/plain。\n# 此指令还可以在http块、server块或者location块中进行配置。\n# default_type  application/octet-stream;\n\n# access_log配置，此指令可以在http块、server块或者location块中进行设置\n# 在全局块中，我们介绍过errer_log指令，其用于配置Nginx进程运行时的日志存放和级别，此处所指的日志与常规的不同，它是指记录Nginx服务器提供服务过程应答前端请求的日志\n# access_log path [format [buffer=size]]\n# 如果你要关闭access_log,你可以使用下面的命令\n# access_log off;\n\n# log_format指令，用于定义日志格式，此指令只能在http块中进行配置\n# log_format  main '$remote_addr - $remote_user [$time_local] \"$request\" '\n#                  '$status $body_bytes_sent \"$http_referer\" '\n#                  '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n# 定义了上面的日志格式后，可以以下面的形式使用日志\n# access_log  logs/access.log  main;\n\n# 开启关闭sendfile方式传输文件，可以在http块、server块或者location块中进行配置\n# sendfile  on | off;\n\n# 设置sendfile最大数据量,此指令可以在http块、server块或location块中配置\n# sendfile_max_chunk size;\n# 其中，size值如果大于0，Nginx进程的每个worker process每次调用sendfile()传输的数据量最大不能超过这个值(这里是128k，所以每次不能超过128k)；如果设置为0，则无限制。默认值为0。\n# sendfile_max_chunk 128k;\n\n# 配置连接超时时间,此指令可以在http块、server块或location块中配置。\n# 与用户建立会话连接后，Nginx服务器可以保持这些连接打开一段时间\n# timeout，服务器端对连接的保持时间。默认值为75s;header_timeout，可选项，在应答报文头部的Keep-Alive域设置超时时间：“Keep-Alive:timeout= header_timeout”。报文中的这个指令可以被Mozilla或者Konqueror识别。\n# keepalive_timeout timeout [header_timeout]\n# 下面配置的含义是，在服务器端保持连接的时间设置为120 s，发给用户端的应答报文头部中Keep-Alive域的超时时间设置为100 s。\n# keepalive_timeout 120s 100s\n\n# 配置单连接请求数上限，此指令可以在http块、server块或location块中配置。\n# Nginx服务器端和用户端建立会话连接后，用户端通过此连接发送请求。指令keepalive_requests用于限制用户通过某一连接向Nginx服务器发送请求的次数。默认是100\n# keepalive_requests number;\n```\n\n#### server块\n\nserver块和“虚拟主机”的概念有密切联系。\n\n虚拟主机，又称虚拟服务器、主机空间或是网页空间，它是一种技术。该技术是为了节省互联网服务器硬件成本而出现的。这里的“主机”或“空间”是由实体的服务器延伸而来，硬件系统可以基于服务器群，或者单个服务器等。虚拟主机技术主要应用于HTTP、FTP及EMAIL等多项服务，将一台服务器的某项或者全部服务内容逻辑划分为多个服务单位，对外表现为多个服务器，从而充分利用服务器硬件资源。从用户角度来看，一台虚拟主机和一台独立的硬件主机是完全一样的。\n\n在使用Nginx服务器提供Web服务时，利用虚拟主机的技术就可以避免为每一个要运行的网站提供单独的Nginx服务器，也无需为每个网站对应运行一组Nginx进程。虚拟主机技术使得Nginx服务器可以在同一台服务器上只运行一组Nginx进程，就可以运行多个网站。\n\n在前面提到过，每一个http块都可以包含多个server块，而每个server块就相当于一台虚拟主机，它内部可有多台主机联合提供服务，一起对外提供在逻辑上关系密切的一组服务（或网站）。\n\n和http块相同，server块也可以包含自己的全局块，同时可以包含多个location块。在server全局块中，最常见的两个配置项是本虚拟主机的监听配置和本虚拟主机的名称或IP配置。\n\n#### listen指令\n\nserver块中最重要的指令就是listen指令，这个指令有三种配置语法。这个指令默认的配置值是：listen *:80 | *:8000；只能在server块种配置这个指令。\n\n``` shell\n//第一种\nlisten address[:port] [default_server] [ssl] [http2 | spdy] [proxy_protocol] [setfib=number] [fastopen=number] [backlog=number] [rcvbuf=size] [sndbuf=size] [accept_filter=filter] [deferred] [bind] [ipv6only=on|off] [reuseport] [so_keepalive=on|off|[keepidle]:[keepintvl]:[keepcnt]];\n\n//第二种\nlisten port [default_server] [ssl] [http2 | spdy] [proxy_protocol] [setfib=number] [fastopen=number] [backlog=number] [rcvbuf=size] [sndbuf=size] [accept_filter=filter] [deferred] [bind] [ipv6only=on|off] [reuseport] [so_keepalive=on|off|[keepidle]:[keepintvl]:[keepcnt]];\n\n//第三种（可以不用重点关注）\nlisten unix:path [default_server] [ssl] [http2 | spdy] [proxy_protocol] [backlog=number] [rcvbuf=size] [sndbuf=size] [accept_filter=filter] [deferred] [bind] [so_keepalive=on|off|[keepidle]:[keepintvl]:[keepcnt]];\n\n```\n\nlisten指令的配置非常灵活，可以单独制定ip，单独指定端口或者同时指定ip和端口。\n\n``` shell\nlisten 127.0.0.1:8000;  #只监听来自127.0.0.1这个IP，请求8000端口的请求\nlisten 127.0.0.1; #只监听来自127.0.0.1这个IP，请求80端口的请求（不指定端口，默认80）\nlisten 8000; #监听来自所有IP，请求8000端口的请求\nlisten *:8000; #和上面效果一样\nlisten localhost:8000; #和第一种效果一致\n```\n\n关于上面的一些重要参数做如下说明：\n\n- address：监听的IP地址（请求来源的IP地址），如果是IPv6的地址，需要使用中括号“[]”括起来，比如[fe80::1]等。\n- port：端口号，如果只定义了IP地址没有定义端口号，就使用80端口。**这边需要做个说明：要是你压根没配置listen指令，那么那么如果nginx以超级用户权限运行，则使用`\\*`:80，否则使用`\\*`:8000**。多个虚拟主机可以同时监听同一个端口,但是server_name需要设置成不一样；\n- default_server：假如通过Host没匹配到对应的虚拟主机，则通过这台虚拟主机处理。具体的可以参考这篇[文章](https://segmentfault.com/a/1190000015681272)，写的不错。\n- backlog=number：设置监听函数listen()最多允许多少网络连接同时处于挂起状态，在FreeBSD中默认为-1，其他平台默认为511。\n- accept_filter=filter，设置监听端口对请求的过滤，被过滤的内容不能被接收和处理。本指令只在FreeBSD和NetBSD 5.0+平台下有效。filter可以设置为dataready或httpready，感兴趣的读者可以参阅Nginx的官方文档。\n- bind：标识符，使用独立的bind()处理此address:port；一般情况下，对于端口相同而IP地址不同的多个连接，Nginx服务器将只使用一个监听命令，并使用bind()处理端口相同的所有连接。\n- ssl：标识符，设置会话连接使用SSL模式进行，此标识符和Nginx服务器提供的HTTPS服务有关。\n\nlisten指令的使用看起来比较复杂，但其实在一般的使用过程中，相对来说比较简单，并不会进行太复杂的配置。\n\n#### server_name指令\n\n用于配置虚拟主机的名称。语法是：\n\n``` shell\nSyntax:\tserver_name name ...;\nDefault:\nserver_name \"\";\nContext:\tserver\n```\n\n对于name 来说，可以只有一个名称，也可以由多个名称并列，之间用空格隔开。每个名字就是一个域名，由两段或者三段组成，之间由点号“.”隔开。比如\n\n``` shell\nserver_name myserver.com www.myserver.com\n```\n\n在该例中，此虚拟主机的名称设置为myserver.com或www. myserver.com。Nginx服务器规定，第一个名称作为此虚拟主机的主要名称。\n\n在name 中可以使用通配符“*”，但通配符只能用在由三段字符串组成的名称的首段或尾段，或者由两段字符串组成的名称的尾段，如：\n\n```shell\nserver_name myserver.* *.myserver.com\n```\n\n另外name还支持正则表达式的形式。这边就不详细展开了。\n\n由于server_name指令支持使用通配符和正则表达式两种配置名称的方式，因此在包含有多个虚拟主机的配置文件中，可能会出现一个名称被多个虚拟主机的server_name匹配成功。那么，来自这个名称的请求到底要交给哪个虚拟主机处理呢？Nginx服务器做出如下规定：\n\na. 对于匹配方式不同的，按照以下的优先级选择虚拟主机，排在前面的优先处理请求。\n\n- ① 准确匹配server_name\n- ② 通配符在开始时匹配server_name成功\n- ③ 通配符在结尾时匹配server_name成功\n- ④ 正则表达式匹配server_name成功\n\nb. 在以上四种匹配方式中，如果server_name被处于同一优先级的匹配方式多次匹配成功，则首次匹配成功的虚拟主机处理请求。\n\n有时候我们希望使用基于IP地址的虚拟主机配置，比如访问192.168.1.31有虚拟主机1处理，访问192.168.1.32由虚拟主机2处理。\n\n这时我们要先网卡绑定别名，比如说网卡之前绑定的IP是192.168.1.30，现在将192.168.1.31和192.168.1.32这两个IP都绑定到这个网卡上，那么请求这个两个IP的请求才会到达这台机器。\n\n绑定别名后进行以下配置即可：\n\n```shell\nhttp\n{\n\t{\n\t   listen:  80;\n\t   server_name:  192.168.1.31;\n     ...\n\t}\n\t{\n\t   listen:  80;\n\t   server_name:  192.168.1.32;\n     ...\n\t}\n}\n```\n\n#### location块\n\n每个server块中可以包含多个location块。在整个Nginx配置文档中起着重要的作用，而且Nginx服务器在许多功能上的灵活性往往在location指令的配置中体现出来。\n\nlocation块的主要作用是，基于Nginx服务器接收到的请求字符串（例如， server_name/uri-string），对除虚拟主机名称（也可以是IP别名，后文有详细阐述）之外的字符串（前例中“/uri-string”部分）进行匹配，对特定的请求进行处理。地址定向、数据缓存和应答控制等功能都是在这部分实现。许多第三方模块的配置也是在location块中提供功能。\n\n在Nginx的官方文档中定义的location的语法结构为：\n\n``` shell\nlocation [ = | ~ | ~* | ^~ ] uri { ... }\n\n```\n\n其中，uri变量是待匹配的请求字符串，可以是不含正则表达的字符串，如/myserver.php等；也可以是包含有正则表达的字符串，如 .php$（表示以.php结尾的URL）等。为了下文叙述方便，我们约定，不含正则表达的uri称为“标准uri”，使用正则表达式的uri称为“正则uri”。\n\n其中方括号里的部分，是可选项，用来改变请求字符串与 uri 的匹配方式。在介绍四种标识的含义之前，我们需要先了解不添加此选项时，Nginx服务器是如何在server块中搜索并使用location块的uri和请求字符串匹配的。\n\n在不添加此选项时，Nginx服务器首先在server块的多个location块中搜索是否有标准uri和请求字符串匹配，如果有多个可以匹配，就记录匹配度最高的一个。然后，服务器再用location块中的正则uri和请求字符串匹配，当第一个正则uri匹配成功，结束搜索，并使用这个location块处理此请求；如果正则匹配全部失败，就使用刚才记录的匹配度最高的location块处理此请求。\n\n了解了上面的内容，就可以解释可选项中各个标识的含义了：\n\n- “=”，用于标准uri前，要求请求字符串与uri严格匹配。如果已经匹配成功，就停止继续向下搜索并立即处理此请求。\n- “^～”，用于标准uri前，要求Nginx服务器找到标识uri和请求字符串匹配度最高的location后，立即使用此location处理请求，而不再使用location块中的正则uri和请求字符串做匹配。\n- “～”，用于表示uri包含正则表达式，并且区分大小写。\n- “～`*`”，用于表示uri包含正则表达式，并且不区分大小写。注意如果uri包含正则表达式，就必须要使用“～”或者“～*”标识。\n","tags":["tutorial","nginx"],"categories":["Linux"]},{"title":"oracle_centos引导卷扩增","url":"/2023/11/01/oracle_centos引导卷扩增/","content":"\n### 1 调整引导卷大小\n\n路径：存储-引导卷-选择编辑引导卷-调整大小\n\n\n\n### 2 运行重新扫描命令\n\n```shell\nsudo dd iflag=direct if=/dev/sda of=/dev/null count=1\necho \"1\" | sudo tee /sys/class/block/sda/device/rescan\necho \"y\" | sudo /usr/libexec/oci-growfs\n```\n\n\n\n因为centos中没有 `` /usr/libexec/oci-growfs``\n\n所以通过以下命令进行分区扩展\n\n```shell\nsudo yum -y install cloud-utils-growpart\ngrowpart /dev/sda 3 (需要扩展的 partition)\nxfs_growfs /\nlsblk (查看情况)\n```\n\n\n\n### 3 调整逻辑分区`centosvolum-root`\n\n```\nsda                     8:0    0   100G  0 disk\n├─sda1                  8:1    0   100M  0 part /boot/efi\n├─sda2                  8:2    0     1G  0 part /boot\n└─sda3                  8:3    0  98.9G  0 part\n  └─centosvolume-root 253:0    0  39.1G  0 lvm  /\n```\n\n- **将 `sda3` 的未使用空间添加到逻辑卷组**\n\n``` shell\nsudo pvresize /dev/sda3\n# Physical volume \"/dev/sda3\" changed\n#  1 physical volume(s) resized or updated / 0 physical volume(s) not resized\n```\n\n- **将未使用的空间添加到逻辑卷**\n\n```shell\nsudo lvextend -l +100%FREE /dev/mapper/centosvolume-root\n  # Size of logical volume centosvolume/root changed from <39.06 GiB (9999 extents) to <98.90 GiB (25318 extents).\n  # Logical volume centosvolume/root successfully resized.\n```\n\n- **调整文件系统大小**\n\n```shell\nsudo xfs_growfs /dev/mapper/centosvolume-root\n# data blocks changed from 10238976 to 25925632\n```\n\n","tags":["centos","oracle"],"categories":["Linux"]},{"title":"python 虚拟环境安装","url":"/2023/11/01/python 虚拟环境安装/","content":"\n- 自带工具包 vevn\n\n```sh\npython3 -m venv myenv\n# 近支持Python 3.3 以后的版本\n```\n\n- 第三发工具包\n\n```sh\npython3 -m virtualenv myenv\n# 需要先安装\npip install virtualenv\n```\n\n\n\n## 使用命令\n\n- 激活\n\n```sh\nsource myenv/bin/activate\n```\n\n- 退出\n\n```sh\ndeactivate\n```\n\n- 删除\n\n```sh\nrm -rf myenv\n```\n\n## 其他\n\n- pyvenv.cfg 文件\n\n```text\nhome = /opt/homebrew/opt/python@3.10/bin\nimplementation = CPython\nversion_info = 3.10.13.final.0\nvirtualenv = 20.24.6\ninclude-system-site-packages = false\nbase-prefix = /opt/homebrew/opt/python@3.10/Frameworks/Python.framework/Versions/3.10\nbase-exec-prefix = /opt/homebrew/opt/python@3.10/Frameworks/Python.framework/Versions/3.10\nbase-executable = /opt/homebrew/opt/python@3.10/bin/python3.10\n```\n\n文件中包含了环境使用的解释器相关信息，文件位于myenv目录下\n\n- requirements.txt\n\n通常项目会有一些自己需要的特定版本的依赖，虚拟环境很好的解决了不同项目对同一模块不同版本需求的管理\n\n安装：\n\n```sh\npip install -r requirements.txt\n```\n\n\n\n","tags":["python","virtual environment"],"categories":["Python"]},{"title":"python 学习笔记","url":"/2023/11/01/python_note/","content":"\n## python环境安装\n\n### python 语言介绍\n\n* python是一种面对对戏那个的解释型计算机程序设计语言\n* python是纯粹的自由软件\n* 源代码和解释器遵循GPL协议\n* python语法简洁清晰，强制用空白符作为语句锁进 （标准四个空格）\n\n### python语言发展\n\n20世界90年代初诞生，可以做后台开发，应用领域非常广泛（专业数据采集与处理即数据爬虫、自动化测试运维领域、人工智能与机器学习领域、数据计算与分析领域等）\n\n2017年python超越C#语言升至第四名；\n\n### python语言特点\n\n+ 简单易学\n+ 开发效率高\n+ 典型的工具语言\n+ 强大丰富的模块库——*最重要*\n+ 跨平台\n\n### python版本介绍\n\n- 常见版本python2.x 和python3.x\n- 对比：2.x很多依赖模块基本不更新；3.x没有考虑到向下兼容\n\n### 验证python开发环境\n\n```python\npython -V #反显的是python2\n```\n\n### python 开发环境&开发工具\n\n- python编译环境anaconda安装和使用\n- 文本编辑器 sublime text3\n- pycharm python IDE 集成开发环境\n\n## 第一个python程序\n\n### 步骤\n\n- 常见工程以及源文件\n- 编写*.py文件\n- 运行程序\n\n``` python\nprint (\"hello,world\") #第一个程序\n```\n\n## python变量与循环定义\n\n### 变量\n\n- 什么是变量 -- 计算机语言能存储计算结果或能表示值的一个抽象概念\n- 变量的特点\n  - 变量可以通过变量名访问\n  - 在指令式语言中，变量通常是可变的\n\n- 变量命名的规范\n\n  变量名就是一个非常典型的标志符 必须是*** 字母、下划线开头 后续必须包括字母数字下划线  ***\n\n- python中的变量赋值不需要类型声明\n\n- 每个变量在内存中创建，包括变量的标志，名称和数据这些信息\n\n- 每个变量在使用前都必须赋值，变量赋值以后改变量才会被创建\n\n### 变量赋值\n\n- 等号（=）用来给变量赋值\n\n- 等号左边是一个变量名，右边是储存在变量中的值\n\n- 赋值语法 变量名 = 值\n\n- ```python\n  #示例\n  counter = 100 #整型变量\n  miles = 1000.0  #浮点型\n  name = \"john\" #字符串\n  ```\n\n### 循环控制\n\n- 控制流语句用来实现对程序流程的选择、循环、转向和返回等进行控制\n- 控制语句可以用来控制程序的流程，以实现程序的各种结构方式\n- 常见的*条件控制语句* 和*循环控制语句*\n\n#### 循环控制语句\n\n- 在某条件下，循环执行某段程序，处理重复的相同任务\n- 循环控制语句回使用到 while 或 for 关键字\n\n##### 循环基本规则\n\n- 需要循环变量（可以为数字类型，字符或字符串类型）\n- 循环条件（可以为表达式或布尔值）\n- 循环语句块中修改循环变量（否则无限循环）\n\n### 循环分类和关键字\n\n- while循环 在给定的判断条件为ture时执行循环体，否则退出循环\n- for循环 重复执行语句\n- 嵌套循环 在while循环中嵌套for循环  *python中没有 do while 循环*\n- break 在语句执行过程中终止循环，并且跳出整个循环\n- conitnue 在语句执行过程中终止当前循环，跳出该次循环，执行下一次循环 *python支持循环控制语句关键字更改语句执行的顺序*\n\n### while循环\n\n```python\nwhile 判断条件:\n  执行语句块\n  pass\n```\n\n#### while循环例子\n\neg1 从一个列表中分别筛选出奇数和偶数，并分别放到不同的列表中\n\n```python\nnumbers = [12, 37, 5, 42, 8, 3]\neven = []\nodd = []\nwhile len(numbers)> 0: #计算列表的个数\n    number = numbers.pop() #从列表中取出一个数赋值给number\n    if number % 2 == 0:\n        odd.append(number) #为数组添加一个数据\n    else:\n        even.append(number)\nprint (even,odd)\n```\n\neg2 输出0-9\n\n```python\nnum = []\na = 0\nwhile a <= 9 :\n    num.append(a)\n    a=a+1\nprint(num);\n\nnum = 0\nwhile num <= 9 :\n    num= num+1\n    print (num);\n```\n\neg 3 输出数字1-10 的偶数\n\n```python\na = 1\neven = []\nwhile a < 11:\n    if a % 2 == 0:\n        even.append(a)\n        a=a+1\n    else:\n        print(a)\n        a=a+1\nprint (even)\n```\n\n```python\nnum = 1\nwhile True:\n    if num > 10:\n        break\n    if num % 2 != 0:\n        num += 1\n        continue\n        pass\n    else:\n        print('num:> %d' %num)\n        num += 1\n        pass\n    pass\n```\n\n### for循环\n\n- range使用\n\n  range也是一种类型，他是一个数字的序列（sequence of numbers），而且是不可变的，通常用在for循环中\n\n  range（）默认返回一个列表的对象\n\n  创建语法： range（stop）：list 创建一个列表，**默认从0开始到stop指定范围的前一个结束，每次递增步长为1**\n\n  ​\t\t\t\t  range（start，stop，step）：list 创建一个列表，**从start到stop前一个结束每次按照step增长**\n\n\n\n- ```python\n  for in range()\n  ```\n\n## python数据结构\n\n### 序列\n\n- 序列对象（sequence）：“序列是程序设计中经常用到的数据存储方式，在其他程序设计语言中，”序列”通常被称为数组，用于存储相关数据项的数据结构。几乎每一种设计语言都提供了序列数据接哦股，\n\n  **python中本身没有数组的概念，但在numpy中提供了数组对象，也祢补了python的不足**\n\n- 序列和数组的区别：\n\n  数组提供了能存放**同一数据类型且连续的**内存空间；\n\n  序列虽然是连续的存储空间，但可以存放不同类型数据，更加高级的数组\n\n- python中常用的序列对象\n\n  - 列表list 可变数据类型\n\n  - 元祖 tuple（不可变数据类型）\n\n  - 集合 sets（可变）\n\n  - 字典 dictinary（可变）\n\n  - 字符串string（不可变）\n\n  - range（）\n\n### 列表基础\n\n- list（列表）是python中使用最频繁的数据类型\n\n  - 列表是一种有序的集合，可以随时添加和删除其中的元素\n  - 列表可以完成大多数集合类的数据结构实现\n  - 它支持字符，数字，字符串甚至可以包含列表（即多维列表）\n  - 立标用[]标志，是python最通用的复合数据类型\n\n- 如何创建列表\n\n  - 默认方法 语法：列表对象名称=[元素1， 元素2， 元素3，……， 元素N]\n\n    ```python\n    list1 = [0,1,2,3]\n    list2= ['a','b','c']\n    list3= ['a',2,True,'hello']\n    print(list1)\n    print(list2)\n    print(list3)\n    ```\n\n  - 使用range（）内置函数\n\n    ```python\n    list1 = range(10)\n    ```\n\n- 如何访问列表\n\n  - 列表中的值的切割也可以用到[头下标：尾下标：步长]，就可以截取相对应的列表\n\n  - 从左到右下标索引默认0开始，从右到左下标索引默认-1开始，下标可以为空表示截取到头或尾\n\n  - ```python\n    list1= list(range(10))\n    print (list1)\n    print (list1[0])\n    print(list1[1:7])\n    print(list1[1:10:2])\n    print(list1[-5:-1])\n    ```\n\n- 列表更新\n\n  - 所谓列表更新是指对列表元素重新赋值、删除、添加等相关操作\n\n  - ```python\n    # 定义列表\n    list1=['html','css','xml','databas']\n  \n    # 更新列表某元素\n    list1[1] = 'css3'\n    print (list1)\n    # 删除累表的database\n    del list1[-1]\n    print (list1)\n    # 使用remove来移除指定元素\n    list1.remove('xml')\n    list1.remove(list1[0])\n    print (list1)\n    # 向列表增加一个元素\n    list1.append('python')\n    print (list1)\n    # 向列表增加一子列表\n    list1.append(list(range(3)))\n    print(list1)\n    ```\n\n  - ```python\n    # 常用函数\n    len(list):获取列表元素的个数\n    max(list)：获取列表最大值\n    min(list)\n    list(seq):将元祖对象转化成列表对象\n    ```\n\n- 元祖\n\n  - 元祖是不可变的\n  - 元祖作为序列的一种，支持分片\n  - 元祖可以在映射中作为键（key）使用，而列表不行\n  - 元祖作为很多内建函数和方法的返回值存在\n\n- 元祖创建 相当只读列表\n\n- 字典\n\n  - 字典由多个键及对应的值组成，每个键及其对应的值为一项\n  - dict函数能将（key->value）形式的序列转换为字典\n  - 字典也是序列的一种，很多基本操作和序列类似\n  - 字典是除列表以外python之中最灵活的内置数据结构类型\n  - 字典中元素是通过键来截取的，而不是通过偏移存取\n  - 字典用{}标志，典型的k-v值数据结构\n\n- 如何创建字典\n\n  ```python\n  定义创建\n  字典对象名称 = {}\n  字典对象名称 = {key1:value1,key2:value2……}\n  字典[key] =\n  字典.keys 字典.values\n  ```\n\n- 集合set\n  - 集合是一个无序不重复元素的集，可删除重复值，检测成员\n  - 可以用大括号{}创建，注意**要创建一个空集合必须使用set()而不是set{}**\n\n## python条件\n\n### 控制语句\n\n- 控制流语句用来实现对程序流程的选择、循环、转向和返回等进行控制\n- 控制语句可以用于控制程序的流程，以使心啊程序的各种结构方式\n  - 一般情况下程序按照编写顺序依次执行，形成一个标准的面向过程的结构化形式。\n  - 当需要非顺序执行的时候，我们就需要控制流语句\n  - 在python中最常见的两种控制流语句是：条件控制语句 和 循环控制语句\n\n### 条件控制语句\n\n- 条件控制语句根据是否满足自定义的条件选择性执行条件下的语句块\n\n- ``` python\n  #基本语法\n  1 if 条件表达式：\n  \t\t条件语句块\n    \tpass #pass是python中的关键字，代表一个空行（也可以不写），代表条件语句块结束\n  \n  2 if 条件表达式：\n  \t\t条件语句块\n    \tpass\n  else：\n  \t\t条件语句库\n    \tpass\n  \n  3 if 条件表达式1:\n    \t\t条件语句块\n      \tpass\n    elif 条件表达式2:\n      \t\t条件语句块\n        \tpass\n    else：\n    \t\t条件语句块\n      \tpass\n  ```\n\n- if elif 可以与if 嵌套互相替代\n\n## python函数\n\n### 函数定义\n\n- 函数是组织好的，可重复使用的，用来实现单一，或相关联功能的代码段\n\n- 函数能提高应用的模块性，和代码的重复利用率\n\n- python提供许多内建函数 如print\n\n- python提供用户自定义函数接口\n\n- 自定义函数\n\n  - def 关键词开头\n\n  - 函数标志符名称和圆括号\n\n  - 任何传入参数和自变量必须放在圆括号中间\n\n  - 函数的第一行可以选择性地使用文档字符串用于存放函数说明\n\n  - 函数内容以冒号起始，并且缩紧\n\n  - return[表达式]结束函数，选择性地返回一个值给调用方\n\n  - **不带表带是的热突然相当返回none**\n\n    ```python\n    def sum(arg1,arg2):\n      total = arg1+arg2\n      return tatal\n    total = sum(10+20)\n    print 'tatal',total\n    ```\n\n### 参数传递\n\n- python函数的参数传递\n  - 不可变类型：在程序编程中的值传递，如整数、字符串、元祖。如fun（a），传递的只是a的值，没有影响a对象本身，比如在fun（a）内部修改a的值，只是修改另一个复制的对象，不会影响a的本身，我经常称为值传递\n  - 可变类型：类似编程中的引用传递（址传递），如累表，字典。如fun（a），则是将a真正的传过去，修改后fun外部的a也会受影响\n  - python中一切都是对戏那个，严格意义我们不能说值传递还是引用传递，我们应该说传不可变对戏那个和传可变对象\n\n## python模块化\n\n### 模块定义\n\n- python模块\n  - 模块是一种组织形式，将许多有关联的代码组织放到单独的独立文件\n  - 模块能定义函数，类和变量\n  - 模块可以理解为一个包含了许多强大功能的包\n  - 一个完整饿大型的python程序是由模块和包的形式组织起来的\n  - python标准库中就包含许多模块，有很多模块内的方法都会被经常使用到\n- python模块的作用\n  - 模块内有许多函数方法\n  - 模块可以在文件中永久保存代码\n  - 模块可以跨系统平台使用\n\n### 模块导入\n\n- python模块包含包含标准库和第三方库\n\n- 倒入python模块，可以使用下列语句\n\n  ```python\n  #下载 终端命令 （自动安装） pip install 模块名 /pip3 install pandas\n  #导入\n  import pandas\n  import pandas as pd\n  from import\n  from import*\n  ```\n\n## python类概念\n\n### 类基础\n\n- 面对对象编程\n  - 时间万物皆是对象\n  - 任何对象都存在自己独有的状态和行为\n  - 状态：描述事务的名词形式\n  - 行为：描述事务的动词形式\n- 对象的状态称为属性，对象的行为称为方法\n\n- 类就是类别的意思，是对现实事物的抽象\n  - 同一类别的事物都是会有共同的属性或方法\n- 对戏那个就是类的一个具体表现事物\n  - 任何对象都拥有相同的属性和方法\n  - 每个对象的属性或方法会有独特的异性\n- 类的创建\n  - class是关键字，表示类\n  - 类的创建语法 class person：\n- 对象的创建\n  - 对象使用类名进行创建\n- 属性的定义有两种方式\n  - 装饰器：在方法上应用装饰器\n  - 静态字段：在类中定义值为property对象的静态字段\n- 方法包括：普通方法、静态方法和类方法\n  - 普通方法：由对象调用；至少一个self参数；执行普通方法是，自动将该方法的对象赋值给self\n  - 类方法：由类调用；至少一个cls参数；执行类方法时，自动将调用该方法的类复制给cls\n  - 静态方法：由类调用；无默认参数\n- 构造方法\n  - 构造方法时类成员方法中特殊的一种方法\n  - 该方法在类实例化对象的过程中自动调用\n- 类成员类型\n  - 公有成员，在任何地方都能访问\n  - 私有成员，只有在类的内部才能方法\n  - 受保护的成员\n  - 私有成员的命名，前两个字符是下划线（特殊成员除外）\n\n### 类的继承\n\n- 继承：子类可以继承父类的内容\n- 继承是面对对象开发的重要特性，可以提高代码的重用性\n- 继承关系下，子类拥有父类全部方法\n- 继承关系下，super（）用于调用父类的方法\n- 多继承\n  - python的类可以继承多个类\n  - 多继承，对象调用方法的方式有两种：深度优先和广度优先\n  - 当类经典类，多继承会按照深度优先\n  - 当类是新式，多继承会按照广度优先\n\n## python的IO流\n\n### 文件操作\n\n- 文件介绍\n\n  - 可分为文本文件和二进制文件两类\n    - 文本文件，在不同操作系统下，可以用文本编辑器仅需读写的操作\n    - 二进制文件，二进制的文件的处理效率更高\n  - 文件的路径\n    - 绝对路径\n    - 相对路径 ./** 点 杠\n\n  - 文件读写操作\n\n  - ``` python\n    input = open ('文件路径'，'读写模式'，[encoding=编码格式])\n    #open 在获取文件若无则创建\n    ```\n\n  - 文件读取的模式主要有5种\n\n    - r读取模式，r+读写文件模式\n\n    - w写入模式\n\n    - a追加模式\n\n    - rb二进制数据读取模式\n\n    - wb二进制数据写入模式\n\n      - +的作用在原有基础上完善所有操作\n\n    - ```python\n      input.write\n      input.read\n      ```\n\n  ## python异常处理\n\n  ### 异常\n\n  - 异常是一个事件，在程序执行过程中发生，影响程序的正常执行\n  - 异常时python对象，表示一个错误\n\n  - 错误分为两种，一种是编译报错；另一种是运行报错\n  - 无论哪种都称为异常情况\n    - 语法错误 error -syntaxerror\n    - 异常- except indexerror\n\n  ### 异常处理\n\n  - 处理的关键字及语法结构\n\n  - ```python\n    try:\n      # 可能出现异常的语句块#\n    except 捕获异常的名词:\n      #处理异常的语句块\n    finally:\n      #异常结束后续执行语句块\n    ```\n\n  - try中的龃龉一旦发生异常，则后续语句将会中止执行，程序控制流直接跳转至except块中。except若未能捕获异常，则系统报错。finally中的语句无论是否出现异常均会执行。try必须伴随except出现，不能独立出现。\n","tags":["python"],"categories":["Python"]},{"title":"python异步函数初步认识","url":"/2023/11/01/python异步函数初步认识/","content":"\n## 背景\n\n上次在Chat-gpt的指导下完成了通过flaresolverr绕过cloudflare访问nodeseek的RSS，一段时间后发现因为访问频繁，常常会发生服务堵塞问题导致服务不可用，主要原因在于flaresolverr处理请求maxTimeout为60秒，任务时间较长，基于该情况下有了此次改造。\n\n## 实现\n\n- 将代码拆分为两部分，一部分通过flaresolverr获取RSS数据；另一部分为创建Http服务监听8123端口，响应访问请求返回RSS数据\n- 引入异步函数，提升并发处理能力，避免多次请求造成服务堵塞\n\n### 代码梳理\n\n好的，让我们来详细解释一下整个访问响应的过程：\n\n1. **启动服务：** 通过运行 `app.py` 中的代码，启动了一个 HTTP 服务。这个服务监听在 8123 端口。\n\n    ```python\n    if __name__ == '__main__':\n        loop = asyncio.get_event_loop()\n        app = loop.run_until_complete(init_app())\n        web.run_app(app, port=8123)\n    ```\n\n2. **路由设置：** 在 `app.py` 中的 `init_app` 函数中，设置了路由。对于根路径 `'/'` 的 GET 请求，会调用 `handle` 函数。\n\n    ```python\n    async def init_app():\n        app = web.Application()\n        app.router.add_get('/', handle)\n        return app\n    ```\n\n3. **异步处理请求：** 在 `handle` 函数中，通过调用 `get_rss_data()` 获取 RSS 数据，`get_rss_data` 函数定义在 `get_rss_data.py` 中。\n\n    ```python\n    async def handle(request):\n        rss_data = await get_rss_data()\n        return web.Response(text=rss_data, content_type='application/xml')\n    ```\n\n4. **获取最新 RSS 数据：** 在 `get_rss_data` 函数中，通过异步调用 `reqfla` 函数，该函数发送 POST 请求获取 RSS 数据。注意，在 `get_rss_data` 函数中使用了全局变量 `session_id` 和 `sessions` 来保持会话信息。\n\n    ```python\n    async def reqfla():\n      \t\t\t# ... 省略\n    async def get_rss_data():\n        try:\n            global session_id, sessions\n            # ... 省略部分代码\n            response = await reqfla(request_cmd)\n            result = response.get('solution', {}).get('response', '')\n            # ... 省略部分代码\n            return formatted_xml\n        except Exception as e:\n            print(f\"Error: {e}\")\n    ```\n\n5. **返回响应：** 最终，`handle` 函数返回了包含最新 RSS 数据的 HTTP 响应。\n\n    ```python\n    return web.Response(text=rss_data, content_type='application/xml')\n    ```\n\n6. **运行事件循环：** 在 `__main__` 部分，通过 `loop.run_until_complete(init_app())` 创建并运行事件循环。事件循环负责协调和调度异步任务的执行。\n\n    ```python\n    if __name__ == '__main__':\n        loop = asyncio.get_event_loop()\n        app = loop.run_until_complete(init_app())\n        web.run_app(app, port=8123)\n    ```\n\n这样，当有请求访问 8123 端口的根路径时，就会触发 `handle` 函数，进而调用 `get_rss_data` 函数获取最新的 RSS 数据，并将数据返回给客户端。整个过程中，利用了异步编程的特性，确保在等待 I/O 操作的过程中，不会阻塞其他任务的执行，提高了程序的效率。\n\n### get_rss_data.py\n\n```python\nimport json\nimport requests\nimport xml.dom.minidom\nimport xml.parsers.expat\nfrom datetime import datetime\n\n# 获取当前日期和时间\ncurrent_datetime = datetime.now()\n\n# 格式化日期和时间为字符串\nformatted_datetime = current_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n\nurl = 'http://localhost:8191/v1'\ndest = 'http://www.nodeseek.com/rss.xml'\nsession_id = \"\"\nsessions = []\noutput_file = \"rss.xml\"\nheaders = {\n    \"Content-Type\": \"application/json\"\n}\n\ncreate_cmd = {\n    \"cmd\": \"sessions.create\",\n    \"url\": dest\n}\n\nlist_cmd = {\n    \"cmd\": \"sessions.list\"\n}\n\ndestroy_cmd = {\n    \"cmd\": \"sessions.destroy\",\n    \"session\": session_id\n}\n\nrequest_cmd = {\n    \"cmd\": \"request.get\",\n    \"url\": dest,\n    \"maxTimeout\": 60000,\n    \"session\": session_id\n}\n\nasync def reqfla(cmd):\n    response = requests.post(url, headers=headers, json=cmd)\n    result = response.json()\n    if result.get('status') == 'ok':\n        return result\n    else:\n        raise Exception(result.get('message', 'Unknown error'))\n\nasync def get_rss_data():\n    global session_id, sessions  # Declare as global variables\n    try:\n        # Modify the parameters to match create_cmd\n        if session_id == \"\":\n            list_session = await reqfla(list_cmd)\n            sessions = list_session.get(\"sessions\")\n            print(\"留存的 sessions:\", sessions)\n\n            if len(sessions) > 1:\n                for session in sessions:\n                    destroy_cmd[\"session\"] = session\n                    destroy_session = await reqfla(destroy_cmd)\n                    print(destroy_session.get(\"message\"))\n\n            if not sessions:\n                create_session = await reqfla(create_cmd)\n                session_id = create_session.get(\"session\")\n                sessions.append(session_id)\n                print(\"最新创建的 sessions：\", sessions)\n\n        session_id = sessions[0]\n        print(\"-\" * 10, formatted_datetime, \"-\" * 10)\n        print(\"本次使用 session_id：\", session_id)\n        response = await reqfla(request_cmd)\n        result = response.get('solution', {}).get('response', '')\n        print(\"请求完成，开始处理 xml\")\n\n        dom = xml.dom.minidom.parseString(result)\n        formatted_xml = dom.toprettyxml()\n\n        with open(output_file, 'w', encoding='utf-8') as file:\n            file.write(formatted_xml)\n        print(\"xml文件处理完成等待访问\")\n\n        return formatted_xml\n    except Exception as e:\n        print(f\"Error: {e}\")\n\nif __name__ == '__main__':\n    import asyncio\n\n    async def main():\n        await get_rss_data()\n\n    asyncio.run(main())\n\n```\n\n### app.py\n\n```python\nimport asyncio\nfrom aiohttp import web\nfrom get_rss_data import get_rss_data\n\nasync def handle(request):\n    # 异步处理 HTTP 请求的函数\n    rss_data = await get_rss_data()\n    return web.Response(text=rss_data, content_type='application/xml')\n\nasync def init_app():\n    # 初始化异步 web 应用\n    app = web.Application()\n    app.router.add_get('/', handle)\n    return app\n\nif __name__ == '__main__':\n    loop = asyncio.get_event_loop()\n    app = loop.run_until_complete(init_app())\n    web.run_app(app, port=8123)\n```\n\n## 异步函数\n\n当你使用异步编程时，主要涉及到协程（coroutine）和事件循环（event loop）的概念。以下是异步编程的基本流程：\n\n1. **定义异步函数（协程）：** 使用 `async def` 定义异步函数。异步函数中可以包含 `await` 表达式，用于等待其他异步任务的执行。\n\n    ```python\n    async def some_async_function():\n        result = await some_other_async_function()\n        print(result)\n    ```\n\n2. **创建事件循环：** 使用 `asyncio.get_event_loop()` 创建一个事件循环对象。\n\n    ```python\n    loop = asyncio.get_event_loop()\n    ```\n\n3. **运行异步任务：** 使用事件循环的 `run_until_complete` 方法运行异步任务。\n\n    ```python\n    loop.run_until_complete(some_async_function())\n    ```\n\n4. **启动事件循环：** 使用 `loop.run_forever()` 启动事件循环。事件循环会负责调度异步任务的执行。\n\n    ```python\n    loop.run_forever() \n    #在上述的代码中，虽然没有显式地调用 `loop.run_forever()`，但 `web.run_app(app, port=8123)` 实际上会在内部启动事件循环并一直运行，直到应用程序停止。\n    \n    #`web.run_app()` 是 aiohttp 框架提供的一个方便的函数，它会在内部启动事件循环，监听 HTTP 请求，并持续运行直到应用程序结束。因此，在这个特定的情境下，你不需要显式地调用 `loop.run_forever()`。\n    ```\n\n或者，在终止事件循环前，通过 `loop.run_until_complete` 来运行异步任务：\n\n```python\nloop.run_until_complete(some_async_function())\n```\n\n总体来说，异步编程的关键在于充分利用等待 I/O 操作的时间，让程序在等待的时候不阻塞，而是去执行其他任务。事件循环负责协调和调度这些异步任务的执行，使得整个程序能够以更高效的方式运行。","tags":["python","async"],"categories":["Python"]},{"title":"python构建通知api服务","url":"/2023/11/01/python构建通知api服务/","content":"\n## 模块准备\n\n有必要的话先安装虚拟环境，进行隔离\n\n```shell\npip install virtualenv\n# 或者\napt-get install python3-venv\n# 当前目录创建虚拟环境\npython -m venv venv\n# 激活虚拟环境\nsource venv/bin/activate\n# 关闭虚拟环境\ndeactive\n```\n\n安装必要模块\n\n```shell\npip install flask\npip requests\n```\n\n## 代码\n\n利用flask创建api服务\n\n```python\nfrom flask import Flask, request, jsonify\nimport requests\n\napp = Flask(__name__)\n\n# Telegram 机器人的 API 地址\ntelegram_bot_api = \"your token api url\"\n\n@app.route('/send_notification', methods=['POST'])\ndef send_notification():\n    try:\n        # 获取请求中的数据\n        data = request.json\n        chat_id = data.get('chat_id')\n        text = data.get('text')\n\n        # 检查是否缺少必要的参数\n        if not chat_id or not text:\n            return jsonify({\"error\": \"Missing required parameters\"}), 400\n\n        # 构建请求参数\n        params = {\n            \"chat_id\": chat_id,\n            \"text\": text,\n        }\n\n        # 发送 HTTP 请求\n        response = requests.get(telegram_bot_api, params=params)\n\n        # 返回响应结果\n        return jsonify({\"status\": \"success\", \"response\": response.text}), 200\n\n    except Exception as e:\n        return jsonify({\"error\": str(e)}), 500\n\nif __name__ == '__main__':\n    app.run(port=5000)\n```\n\n## ngixn 反代该服务\n\n通过nginx实现反向代理：\n\n- 第一种写法nginx会将`/api` 路径带入服务导致路由路径变为`/api/send_notification` 从而报错，需要改flask路由\n- 第二种写法则去掉了路径中的`/api` \n\n```shell\nlocation /api {\n    proxy_pass http://127.0.0.1:5000;\n    proxy_set_header Host $host;\n    proxy_set_header X-Real-IP $remote_addr;\n    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n    proxy_set_header X-Forwarded-Proto $scheme;\n}\n```\n\n```shell\nlocation /api {\n    rewrite ^/api(/.*)$ $1 break;  # 去掉路径中的 /api 部分\n    proxy_pass http://127.0.0.1:5000;\n    proxy_set_header Host $host;\n    proxy_set_header X-Real-IP $remote_addr;\n    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n    proxy_set_header X-Forwarded-Proto $scheme;\n}\n```\n\n## 服务请求\n\n通过python request 请求服务实现通知发送\n\n```python\nimport requests\n\ndef notice(text):\n    api_url = \"https://address.com/api/send_notification\"\n    data = {\n        \"chat_id\": \"-1001970059003\",\n        \"text\": text\n    }\n    response = requests.post(api_url, json=data)\n\n    return print(response.json())\n\nif __name__ == \"__main__\":\n    text = \"你好\"\n    notice(text)\n```\n\n## 后记\n\n感觉像是在无限套娃，虽然少写了一个bot token，也还行吧！","tags":["python","nginx","api","flask"],"categories":["Python"]},{"title":"python自动签到pt站点","url":"/2023/11/01/python自动签到pt站点/","content":"## python调用requests 实现签到动作\n\n- requsets 访问pt站点签到\n- telegram 机器人通知签到结果\n\n```python\nimport requests\n\nurl = 'https://www.ptdomain.org/attendance.php'\nnotify_success_url = 'http://api.telegram.org/bot{bot-token}/sendMessage?chat_id={chat-id}&text=PT签到成功!'\nnotify_failure_url = 'http://api.telegram.org/bot{bot-token}/sendMessage?chat_id={chat-id}&text=PT签到失败!'\n\nheaders = {\n    'authority': 'www.ptdomain.org',\n    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n    'accept-language': 'zh,zh-CN;q=0.9',\n    'cache-control': 'max-age=0',\n    'cookie': 'your cookies', #修改为你的cookies\n    'referer': 'https://www.pttime.org/index.php',\n    'sec-ch-ua': '\"Google Chrome\";v=\"119\", \"Chromium\";v=\"119\", \"Not?A_Brand\";v=\"24\"',\n    'sec-ch-ua-mobile': '?0',\n    'sec-ch-ua-platform': '\"macOS\"',\n    'sec-fetch-dest': 'document',\n    'sec-fetch-mode': 'navigate',\n    'sec-fetch-site': 'same-origin',\n    'sec-fetch-user': '?1',\n    'upgrade-insecure-requests': '1',\n    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',\n}\n\nresponse = requests.get(url, headers=headers)\n\n# Check if the response contains a string indicating successful sign-in\nif '签到成功' in response.text:\n    print('签到成功！')\n    # 发送成功通知\n    notify_response = requests.get(notify_success_url)\n    if notify_response.status_code == 200:\n        print('成功通知已发送成功！')\n    else:\n        print(f'成功通知发送失败，状态码: {notify_response.status_code}')\nelse:\n    print('签到未成功。')\n    # 发送失败通知\n    notify_response = requests.get(notify_failure_url)\n    if notify_response.status_code == 200:\n        print('失败通知已发送成功！')\n    else:\n        print(f'失败通知发送失败，状态码: {notify_response.status_code}')\n```\n\n## headers 获取\n\n利用chrome开发工具直接复制请求代码\n\n![chrome开发工具](https://od.wadaho.cf/api/raw/?path=/picture/blog/picshoot1.png)\n\n## 使用crontab定时执行任务\n\n添加下列任务\n\n```shell\ncrontab -e\n0 9 * * * /usr/bin/python3 /root/script/sign/abc.py  # 每天上午9点执行\n```\n\n","tags":["python","pt"],"categories":["Python"]},{"title":"qBittorrent下载完成后自动上传","url":"/2023/11/01/qBittorrent下载完成后自动上传/","content":"\n### 背景\n\n为了合理化对于PT爱好的投入，在storage-vps的支出偏低，选择了[Host-bb](https://my.hostbrr.com/order/main/packages/storage/?group_id=27)的IPV4 NAT 1T的版本，再加上手上还有一台[V6node](https://cloud.v6node.com/plans)16G的服务器，所以打算将两者组合起来一个下载一个保存。在A服务器下载完成后需要尽快将其转移到B服务器，手动处理起来过于频繁。\n\n\n\n### 方案\n\n通过webdav+rclone+shell+qBittorrent实现自动化转存。\n\n- qBittorrent标签处理，通过标签实现命令触达\n\n```\nunfinished_tag=\"【待上传云端】\" # 这个是手动设置某些tag，因为有用才上传\nuploading_tag=\"【正在上传】\"\nfinished_tag=\"【结束】\"\nnoupload_tag=\"无效-不上传\"\n```\n\n- 代码来自\n\n```\nhttps://www.dboke.com/article/b1b4d8fc-c3b3-42a9-b306-4ec141357c01\nhttps://hostloc.com/thread-639215-1-2.html\n```\n\n\n\n```shell\n#!/bin/bash\n\nqb_version=\"4.6.4\" # 改：改为你的实际qb的版本号\nqb_username=\"admin\" # 改：该为你的qb登录用户名\nqb_password=\"Psd\" # 改：改为你qb登录的密码\nqb_web_url=\"http://127.0.0.1:8080\" # 查：改为qb的登录地址，一般可以不改\nlog_dir=\"/root/script/autoUpload\" # 改：改为你日志运行的路径\nrclone_dest=\"pt:\" # 运行rclone config查看name字段即可；格式就是\"XX:\"\nfrom_dc_tag=\"/baoZ\" # 改：上传后的相对根目录，可为空\nrclone_parallel=\"32\" # rclone上传线程 默认4\n\n# 下面的也可以自定义，但是推荐不改动\nunfinished_tag=\"【待上传云端】\" # 这个是手动设置某些tag，因为有用才上传\nuploading_tag=\"【正在上传】\"\nfinished_tag=\"【结束】\"\nnoupload_tag=\"无效-不上传\"\n\n\nif [ ! -d ${log_dir} ]\nthen\n\tmkdir -p ${log_dir}\nfi\n\nversion=$(echo ${qb_version} | grep -P -o \"([0-9]\\.){2}[0-9]\" | sed s/\\\\.//g)\nstartPat=`date +'%Y-%m-%d %H:%M:%S'`  # 时间计算方案\nstart_seconds=$(date --date=\"$startPat\" +%s);\n\nfunction qb_login(){\n\tif [ ${version} -gt 404 ]\n\tthen\n\t\tqb_v=\"1\"\n\t\tcookie=$(curl -i --header \"Referer: ${qb_web_url}\" --data \"username=${qb_username}&password=${qb_password}\" \"${qb_web_url}/api/v2/auth/login\" | grep -P -o 'SID=\\S{32}')\n\t\tif [ -n ${cookie} ]\n\t\tthen\n\t\t\techo \"[$(date '+%Y-%m-%d %H:%M:%S')] 登录成功！cookie:${cookie}\"\n\n\t\telse\n\t\t\techo \"[$(date '+%Y-%m-%d %H:%M:%S')] 登录失败！\"\n\t\tfi\n\telif [[ ${version} -le 404 && ${version} -ge 320 ]]\n\tthen\n\t\tqb_v=\"2\"\n\t\tcookie=$(curl -i --header \"Referer: ${qb_web_url}\" --data \"username=${qb_username}&password=${qb_password}\" \"${qb_web_url}/login\" | grep -P -o 'SID=\\S{32}')\n\t\tif [ -n ${cookie} ]\n\t\tthen\n\t\t\techo \"[$(date '+%Y-%m-%d %H:%M:%S')] 登录成功！cookie:${cookie}\"\n\t\telse\n\t\t\techo \"[$(date '+%Y-%m-%d %H:%M:%S')] 登录失败\"\n\t\tfi\n\telif [[ ${version} -ge 310 && ${version} -lt 320 ]]\n\tthen\n\t\tqb_v=\"3\"\n\t\techo \"陈年老版本，请及时升级\"\n\t\texit\n\telse\n\t\tqb_v=\"0\"\n\t\texit\n\tfi\n}\n\n# 先移除指定tag，后增加自己的tag\nfunction qb_change_hash_tag(){\n    file_hash=$1\n    fromTag=$2\n    toTag=$3\n    if [ ${qb_v} == \"1\" ]\n    then # 这里是添加某些tag的方法\n\t\tcurl -s -X POST -d \"hashes=${file_hash}&tags=${fromTag}\" \"${qb_web_url}/api/v2/torrents/removeTags\" --cookie \"${cookie}\"\n        curl -s -X POST -d \"hashes=${file_hash}&tags=${toTag}\" \"${qb_web_url}/api/v2/torrents/addTags\" --cookie \"${cookie}\"\n    elif [ ${qb_v} == \"2\" ]\n    then\n        curl -s -X POST -d \"hashes=${file_hash}&category=${fromTag}\" \"${qb_web_url}/command/removeCategories\" --cookie ${cookie}\n        curl -s -X POST -d \"hashes=${file_hash}&category=${toTag}\" \"${qb_web_url}/command/setCategory\" --cookie ${cookie}\n    else\n        echo \"qb_v=${qb_v}\"\n    fi\n}\n\nfunction rclone_copy(){\n    torrent_name=$1\n    torrent_hash=$2\n    torrent_path=$3\n\n    echo \"${torrent_name}\"  >> ${log_dir}/qb.log\n    echo \"${torrent_hash}\"  >> ${log_dir}/qb.log\n    echo \"${torrent_path}\"  >> ${log_dir}/qb.log\n\n    # tag = 待上传\n    # 这里执行上传程序\n    if [ -f \"${torrent_path}\" ]\n    then\n       echo \"[$(date '+%Y-%m-%d %H:%M:%S')] 类型：文件\"\n       echo \"[$(date '+%Y-%m-%d %H:%M:%S')] 类型：文件\" >> ${log_dir}/qb.log\n       type=\"file\"\n    elif [ -d \"${torrent_path}\" ]\n    then\n       echo \"[$(date '+%Y-%m-%d %H:%M:%S')] 类型：目录\"\n       echo \"[$(date '+%Y-%m-%d %H:%M:%S')] 类型：目录\" >> ${log_dir}/qb.log\n       type=\"dir\"\n    else\n       echo \"[$(date '+%Y-%m-%d %H:%M:%S')] 未知类型，取消上传\"\n       echo \"[$(date '+%Y-%m-%d %H:%M:%S')] 未知类型，取消上传\" >> ${log_dir}/qb.log\n       # tag = 不上传\n       qb_change_hash_tag ${torrent_hash} ${unfinished_tag} ${noupload_tag}\n       return\n    fi\n    # tag = 上传中\n    qb_change_hash_tag ${torrent_hash} ${unfinished_tag} ${uploading_tag}\n    # 执行上传\n    if [ ${type} == \"file\" ]\n    then # 这里是rclone上传的方法\n        rclone_copy_cmd=$(rclone -v copy --transfers ${rclone_parallel} --log-file  ${log_dir}/qbauto_copy.log \"${torrent_path}\" ${rclone_dest}/${from_dc_tag})\n    elif [ ${type} == \"dir\" ]\n    then\n\t\trclone_copy_cmd=$(rclone -v copy --transfers ${rclone_parallel} --log-file ${log_dir}/qbauto_copy.log \"${torrent_path}\"/ ${rclone_dest}/${from_dc_tag}/\"${torrent_name}\")\n    fi\n\n    # tag = 已上传\n    qb_change_hash_tag ${torrent_hash} ${uploading_tag} ${finished_tag}\n\n    endPat=`date +'%Y-%m-%d %H:%M:%S'`\n    end_seconds=$(date --date=\"$endPat\" +%s);\n    use_seconds=$((end_seconds-start_seconds));\n    use_min=$((use_seconds/60));\n    use_sec=$((use_seconds%60));\n    echo \"上传完成-耗时:${use_min}分${use_sec}秒\"\n}\n\nfunction file_lock(){\n    $(touch qbup.lock)\n}\nfunction can_go_lock(){\n    lockStatus=$(ls | grep qbup.lock)\n    if [ -z \"${lockStatus}\" ]\n    then\n        noLock=\"1\"\n        return\n    fi\n    noLock=\"0\"\n}\nfunction file_unlock(){\n    $(rm -rf qbup.lock)\n}\n\nfunction doUpload(){\n    torrentInfo=$1\n    i=$2\n    echo $2\n    echo ${i}\n\n    # IFS保存，因为名字中可能出现多个空格\n\tOLD_IFS=$IFS\n\tIFS=\"\\n\"\n\n    torrent_name=$(echo \"${torrentInfo}\" | jq \".[$i] | .name\" | sed s/\\\"//g)\n    torrent_hash=$(echo \"${torrentInfo}\" | jq \".[$i] | .hash\" | sed s/\\\"//g)\n    save_path=$(echo \"${torrentInfo}\" | jq \".[$i] | .save_path\" | sed s/\\\"//g)\n\n    IFS=$OLD_IFS\n    \n    echo \"${torrent_name}\";\n\n    if [[ $save_path != /* ]]\n    then\n\t\tsave_path=\"/mnt${save_path}\"\n    fi\n\n    torrent_path=\"/mnt${save_path}/${torrent_name}\" # 这里就是他的本地实际路径，尝试将这里上传上去\n\n    can_go_lock\n    if [[ ${noLock} == \"1\" ]] # 厕所门能开\n    then\n        file_lock # 锁上厕所门\n        echo '执行上传没事的~~~';\n        echo ${torrent_name}\n        echo ${torrent_hash}\n        echo ${torrent_path}\n        rclone_copy \"${torrent_name}\" \"${torrent_hash}\" \"${torrent_path}\"\n    else\n        echo '已有程序在上传，退出'\n        return # 打不开门，换个时间来\n    fi\n    file_unlock # 打开厕所门，出去\n}\n\n# 每次只查询一条数据，！！上传一条数据！！\nfunction qb_get_status(){\n\tqb_login\n\tif [ ${qb_v} == \"1\" ]\n\tthen\n\t\tcompleted_torrents_num=$(curl -s \"${qb_web_url}/api/v2/torrents/info?filter=completed\" --cookie \"${cookie}\" | jq '.[] | length' | wc -l)\n\t\techo \"任务数：\".${completed_torrents_num}\n\t\tfor((i=0;i<${completed_torrents_num};i++));\n\t\tdo\n\t\t\tcurtag=$(curl -s \"${qb_web_url}/api/v2/torrents/info?filter=completed\" --cookie \"${cookie}\" | jq \".[$i] | .tags\" | sed s/\\\"//g | grep -P -o \"${unfinished_tag}\")\n\t\t\tif [ -z \"${curtag}\" ]\n\t\t\tthen\n\t\t\t\tcurtag=\"null\"\n\t\t\tfi\n\t\t\tif [ ${curtag} == \"${unfinished_tag}\" ]\n\t\t\tthen\n\t\t\t    torrentInfo=$(curl -s \"${qb_web_url}/api/v2/torrents/info?filter=completed\" --cookie \"${cookie}\")\n\n\t\t\t\tdoUpload \"${torrentInfo}\" ${i}\n                # 每次只上传一个数据，否则的话，可能会导致多线程的争用问题\n                break\n\t\t\tfi\n\t\tdone\n\telif [ ${qb_v} == \"2\" ]\n\tthen\n\t\tcompleted_torrents_num=$(curl -s \"${qb_web_url}/query/torrents?filter=completed\" --cookie \"${cookie}\" | jq '.[] | length' | wc -l)\n\t\tfor((i=0;i<${completed_torrents_num};i++));\n\t\tdo\n\t\t\tcurtag=$(curl -s \"${qb_web_url}/query/torrents?filter=completed\" --cookie \"${cookie}\" | jq \".[$i] | .category\" | sed s/\\\"//g)\n\t\t\tif [ -z \"${curtag}\" ]\n\t\t\tthen\n\t\t\t\tcurtag=\"null\"\n\t\t\tfi\n\t\t\tif [ ${curtag} == \"${unfinished_tag}\" ]\n\t\t\tthen\n\t\t\t\ttorrentInfo=$(curl -s \"${qb_web_url}/query/torrents?filter=completed\" --cookie \"${cookie}\")\n\n                doUpload \"${torrentInfo}\" ${i}\n                # 每次只上传一个数据，否则的话，可能会导致多线程的争用问题\n                break\n\t\t\tfi\n\t\tdone\n\t\techo \"啥事都不干\";\n\telse\n\t\techo \"获取错误\"\n\t\techo \"qb_v=${qb_v}\"\n\tfi\n}\n\nqb_get_status\n\n```\n\n","tags":["qBittorrent","shell"],"categories":["Python"]},{"title":"wordpress容器化部署","url":"/2023/11/01/wordpress容器化部署/","content":"\n## 前记\n\n早在大学期间就有接触过wordpress这个软件，当时纯粹是因为手头的vps仅仅是挂个代理有点没能做到物尽其用，便在网上各种搜寻，想着利用vps整点有意思的东西。不过当时通过wordpress搭建个人还是建非常繁琐的事情，啥啥不干就得下四个软件，中间各种配置连接测试令人头大；这两天闲来无事，便又开始捣鼓起了手上的vps，再加上最近docker成风，就寻思能否利用docker来进行搭建，一来docker独立于服务器中，不受环境影响，也不影响环境；二来docker的镜像安装卸载及其方便几条命令就能解决；三来也能学习学习。\n\n## 正文\n\n1. Docker环境的搭建\n\n```shell\ncurl -fsSL https://get.docker.com | bash -s docker  #安装\nsystemctl start docker #启动docker\ndocker run hello-world #测试是否成功安装\n```\n\ndocker 架构主要包括三个部分 镜像、仓库、容器；其中镜像可以理解成一个个root文件系统；容器就是运行镜像的一个载体，可以无限删除重建极为方便；仓库可以简单理解为镜像商店，可以从中pull各种镜像。\n\n2. mysql安装\n\n```shell\ndocker pull mysql #下载最新版mysql镜像\ndocker run -d --name mysql -e MYSQL_ROOT_PASSWORD=123456 -p 3306:3306 mysql #运行mysql镜像\n```\n\n`-d 容器后台运行，否则运行完将直接退出`\n\n`--name 容器定义别名 `\n\n`-e 配置环境，新版mysql不设定root密码无法安装`\n\n3. wordpress 安装\n\n``` shell\ndocker pull wordpress\ndocker run --name wordpress --link mysql:mysql -d -p 8080:80 wordpress\n```\n\n`--link 因为容器和容器之间都是独立，link为连个容器创造连接`\n\n到这通过 xxxx.xxx.xx.x:8080 便可登陆wordpress后台，但个人博客怎么能没有域名了，能通过域名进行访问的博客才是合格的\n\n4.ngnix 安装反向代理\n\n```shell\n# ngnix 安装\ndocker pull nginx\ndocker run --name nginx --link wordpress:wordpress -p 80:80 -d nginx\n```\n\n```shell\n# ngnix 配置\n# 新建一个 xxx.conf\ntouch vhost.conf\nvi vhost.conf\nserver {\n  listen 80;\n  listen [::]:80;\n\n  # 域名\n  server_name www.kygoho.win;\n\n  location / {\n    ## 定位到wordpress\n    proxy_pass http://wordpress;\n\n        proxy_http_version    1.1;\n        proxy_cache_bypass    $http_upgrade;\n\n        proxy_set_header Upgrade            $http_upgrade;\n        proxy_set_header Connection         \"upgrade\";\n        proxy_set_header Host                $host;\n        proxy_set_header X-Real-IP            $remote_addr;\n        proxy_set_header X-Forwarded-For    $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto    $scheme;\n        proxy_set_header X-Forwarded-Host    $host;\n        proxy_set_header X-Forwarded-Port    $server_port;\n  }\n}\n\n```\n\n```shell\n# 将vhost.conf 拷贝至 nginx 容器 /etc/ngnix/conf.d/ 目录下\ndocker cp vhost.conf blog_nginx_1:/etc/nginx/conf.d\n# 重启ngnix\ndocker restart blog_nginx_1\n```\n\n\n\n完成这一步稍等几分钟正常通过域名即可访问wordpress了。\n\n## 后记&思考\n\n```shell\ndocker exec -i -t ngnix bash #这条命令可以直接进入ngnix容器中，查看ngnix的各项内容\ndocker ps #查看容器状态\ndocker ps -a\ndocker images #查看镜像\ndockeer run -v #挂在容器到本地文件夹 我还没用过\n```\n\n1. 在ngnix代理这一块一开始想着因为wordpress有映射到本地端口8080，所以在映射时直接转proxy_pass http://本地ip：8080，但发现不行。即域名访问80端口——ngnix——8080端口实现域名访问wordpress\n2. 通过查找资料发现可以连接ngnix和wordpress两个容器，再通过配置实现域名直接访问。\n\ndocker 的link相关问题还需继续学习；\n\nngnix还要再学习！\n\n> [从0使用docker搭建wordpress](https://mhy12345.xyz/tutorials/wordpress-tutorials/)\n>\n> [docker教程](https://www.runoob.com/docker/docker-architecture.html)\n>\n> [小白学习docker](https://juejin.cn/post/6844904117165359111)\n>\n> [使用docker搭建WordPress博客系统并配置HTTPS](https://leozl.site/linux/%E4%BD%BF%E7%94%A8docker%E6%90%AD%E5%BB%BAwordpress%E5%8D%9A%E5%AE%A2%E7%B3%BB%E7%BB%9F%E5%B9%B6%E9%85%8D%E7%BD%AEhttps/)\n","tags":["docker","wordpress"],"categories":["Docker"]},{"title":"使用 Cloudflare Workers 和 OneDrive API 实现文件上传功能","url":"/2023/11/01/使用-Cloudflare-Workers-和-OneDrive-API-实现文件上传功能/","content":"\n\n\n在现代网络应用中，文件上传是一个常见的需求。在本文中，我们将介绍如何使用 Cloudflare Workers 和 OneDrive API 实现一个简单而强大的文件上传功能。我们将创建一个基于 HTML 和 JavaScript 的用户界面，允许用户选择文件并将其上传到 OneDrive 存储中。\n\n> 本文作者：Chat-gpt\n\n## 代码概览\n\n首先，让我们来看一下实现文件上传功能的关键代码。这段代码使用 Cloudflare Workers 提供的 `addEventListener` 函数来监听请求事件，然后调用 `handleRequest` 函数处理请求。\n\n```javascript\n// 以下为前文代码，省略部分与 OneDrive API 无关的代码\n\n// 处理 POST 请求\nelse if (request.method === 'POST') {\n  try {\n    // 解析上传的文件\n    const formData = await request.formData();\n    const file = formData.get('file');\n\n    // 获取 OneDrive 访问令牌\n    const accessToken = await getAccessToken();\n\n    // 构建 OneDrive API 请求\n    const apiUrl = `https://graph.microsoft.com/v1.0/users/eddie@412zml.onmicrosoft.com/drive/root:/Public/picture/blog/${file.name}:/content`;\n    const headers = {\n      'Authorization': `Bearer ${accessToken}`,\n      'Content-Type': 'application/octet-stream',\n    };\n    const options = {\n      method: 'PUT',\n      headers: headers,\n      body: file,\n    };\n\n    // 发送文件到 OneDrive\n    const response = await fetch(apiUrl, options);\n\n    if (response.ok) {\n      // 返回成功响应\n      const jsonResponse = await response.json();\n      return new Response(JSON.stringify({ success: true, response: jsonResponse }), {\n        status: 200,\n        headers: { 'Content-Type': 'application/json' },\n      });\n    } else {\n      // 处理上传失败情况\n      console.error('File upload failed:', response.statusText);\n      console.error('OneDrive API Error:', await response.json());\n      return new Response(JSON.stringify({ success: false, error: response.statusText }), {\n        status: response.status,\n        headers: { 'Content-Type': 'application/json' },\n      });\n    }\n  } catch (error) {\n    // 处理上传过程中的错误\n    console.error('Error uploading file:', error);\n    return new Response(JSON.stringify({ success: false, error: error.message }), {\n      status: 500,\n      headers: { 'Content-Type': 'application/json' },\n    });\n  }\n}\n\n// 以下为前文代码，省略部分与 OneDrive API 无关的代码\n```\n\n上述代码展示了如何使用 OneDrive API 将文件上传到 OneDrive 存储。首先，我们获取用户上传的文件，然后通过 OneDrive API 的 `PUT` 请求将文件上传到指定位置。上传成功后，返回相应的 JSON 数据。\n\n## OneDrive API 访问令牌获取\n\n在上述代码中，我们通过 `getAccessToken` 函数获取 OneDrive API 的访问令牌。以下是获取访问令牌的实现逻辑。\n\n```javascript\nasync function getAccessToken() {\n  // 在这里实现获取 Azure AD 访问令牌的逻辑，使用你的应用程序凭据等信息\n  // 返回获取到的访问令牌\n  const clientId = 'YOUR_CLIENT_ID'; // 替换为你的应用程序的客户端 ID\n  const clientSecret = 'YOUR_CLIENT_SECRET'; // 替换为你的应用程序的客户端密钥\n  const tenantId = 'YOUR_TENANT_ID'; // 替换为你的 Azure AD 租户 ID\n\n  const tokenUrl = `https://login.microsoftonline.com/${tenantId}/oauth2/v2.0/token`;\n\n  const requestBody = new URLSearchParams({\n    grant_type: 'client_credentials',\n    client_id: clientId,\n    client_secret: clientSecret,\n    scope: 'https://graph.microsoft.com/.default', // 这个 scope 取决于你的应用程序和所需的权限\n  });\n\n  const options = {\n    method: 'POST',\n    headers: {\n      'Content-Type': 'application/x-www-form-urlencoded',\n    },\n    body: requestBody,\n  };\n\n  try {\n    const response = await fetch(tokenUrl, options);\n    const tokenData = await response.json();\n\n    if (response.ok) {\n      const accessToken = tokenData.access_token;\n      return accessToken;\n    } else {\n      throw new Error(`Failed to get access token: ${tokenData.error_description}`);\n    }\n  } catch (error) {\n    throw new Error(`Error getting access token: ${error.message}`);\n  }\n}\n```\n\n在该函数中，我们使用应用程序的客户端 ID、客户端密钥和租户 ID 构建访问令牌请求，然后通过 `fetch` 函数发送请求获取访问令牌。获取到的访问令牌将用于构建 OneDrive API 请求的授权头部。\n\n## 前端界面\n\n用户界面是与用户交互的关键部分。在上面的代码中，我们通过 HTML 和 JavaScript 创建了一个简单的上传界面。用户可以选择文件，然后点击上传按钮。\n\n```html\n<!DOCTYPE html>\n<html>\n<head>\n  <!-- 省略部分头部信息和样式 -->\n</head>\n<body>\n  <h1>鲨鱼上传</h1>\n  <label for=\"fileInput\" class=\"custom-file-upload\">FILE HERE</label>\n  <input type=\"file\" id=\"fileInput\" style=\"display:none;\" />\n  <button onclick=\"uploadFile()\">Upload</button>\n  <!-- 省略部分界面元素 -->\n  <script>\n    // 省略部分 JavaScript 代码\n  </script>\n</body>\n</html>\n```\n\n在界面中，我们使用了一个简单的表单，包含文件选择框、上传按钮以及一些用于显示上传结果的元素。\n\n## 文件上传逻辑\n\n文件上传逻辑主要由 JavaScript 代码实现。当用户点击上传按钮时，`uploadFile` 函数被调用，该函数负责获取用户选择的文件，构建 OneDrive API 请求，并发送文件到 OneDrive 存储。\n\n```javascript\nasync function uploadFile() {\n  // 省略部分代码\n  const formData = new FormData();\n  formData.append('file', fileInput.files[0]);\n\n  try {\n    const response = await fetch('/', {\n      method: 'POST',\n      body: formData\n    });\n\n    if (response.ok) {\n      //\n\n 处理成功上传的情况\n      const data = await response.json();\n      const resultUrl = document.getElementById('webUrl');\n      const resultName = document.getElementById('fileNamePlaceholder');\n      resultUrl.href = data.response.webUrl;\n      resultUrl.textContent = data.response.webUrl;\n      resultName.textContent = data.response.name;\n      // 移除弹跳动画类\n      h1Element.classList.remove('bounce-animation');\n    } else {\n      // 处理上传失败的情况\n      console.error('File upload failed:', response.statusText);\n    }\n  } catch (error) {\n    // 处理上传过程中的错误\n    console.error('Error uploading file:', error);\n  }\n}\n```\n\n在这个函数中，我们首先创建一个 `FormData` 对象，将用户选择的文件附加到该对象中。然后，使用 `fetch` 函数发送 POST 请求，将文件上传到 Cloudflare Workers。在 Cloudflare Workers 中，我们再次使用 `fetch` 函数将文件传递给 OneDrive API。最终，将上传结果显示在界面上。\n\n## 结语\n\n通过结合 Cloudflare Workers 和 OneDrive API，我们实现了一个简单而有效的文件上传功能。这种方案具有低延迟、高可用性和强大的性能，为用户提供了良好的体验。在实际应用中，你可以根据需要进行扩展，添加更多功能和安全性措施，以满足特定的业务需求。希望本文能够帮助你理解如何使用 Cloudflare Workers 构建强大的文件上传服务。","tags":["api","js","cloudflare","onedrive"],"categories":["Javascript"]},{"title":"使用docker创建Jupyter并配置Nginx反向代理","url":"/2023/11/01/使用docker创建Jupyter并配置Nginx反向代理/","content":"\n#### docker创建Jupyter服务\n\n```shell\n#本地端创建相关文件夹\nmkdir -p /opt/jupyter/jovyan\nmkdir -p /opt/jupyter/jovyan/.jupyter\nchmod 777 -R /opt/jupyter/jovyan\n#获取jupyter镜像\ndocker pull jupyter/base-notebook:notebook-5.7.8\n#创建jupyter容器\ndocker run --name vk-jupyter -d \\\n-p 8888:8888 \\\n-v /opt/jupyter/jovyan:/home/jovyan \\\njupyter/base-notebook:notebook-5.7.8\n#获取jupyter的token\ndocker exec -it vk-jupyter jupyter notebook list\n#设置密码\ndocker exec -it vk-jupyter jupyter notebook password\ndocker restart vk-jupyter\n#安装插件\ndocker exec -it vk-jupyter pip install ipywidgets\n#安装常用命令\ndocker exec --user root -it vk-jupyter /bin/bash #使用管理员权限进入\napt update\napt install curl\napt install unzip\n```\n\n#### 配置nginx反向代理\n\n因为服务器中已存在多个应用，故使用服务器子地址路径实现域名访问notebook\n\n首先修改nginx_location配置文件\n\n```shell\n    location /py {\n        proxy_set_header   Host             $host;\n        proxy_set_header   X-Real-IP        $remote_addr;\n        proxy_set_header  X-Forwarded-For  $proxy_add_x_forwarded_for;\n\n        proxy_pass http://172.0.0.1:8888;\n\n        # WebSocket support\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection \"upgrade\";\n    }\n```\n\n重启nginx -t && nginx -s reload\n\n修改/opt/jupyter/jovyan/.jupyter/jupyter_notebook_config.py 配置文件\n\n```shell\n{\n  \"NotebookApp\": {\n    \"password\": \"your passwored\",\n    \"base_url\": \"/py\"\n  }\n}\n```\n\n重启docker restart vk-jupyter\n","tags":["docker","nginx","jupyter"],"categories":["Docker"]},{"title":"利用 API 自动获取汇率并实时通知","url":"/2023/11/01/利用-API-自动获取汇率并实时通知/","content":"\n## 背景\n\n> by chat-gpt3.5\n\n在国际贸易和金融交流中，汇率是一个至关重要的因素，直接影响着货币之间的兑换关系。为了方便地获取汇率信息并实时通知，我们可以利用 Python 编写一个脚本，通过 API 自动获取汇率数据，并将结果保存到 CSV 文件中。本文将介绍如何使用 ExchangeRate-API 提供的接口，获取土耳其里拉（TRY）、美元（USD）对人民币（CNY）的汇率，并额外获取美元对新加坡元（SGD）的汇率。\n\n## 实现\n\n首先，我们使用 `requests` 库发送 HTTP 请求，获取 ExchangeRate-API 提供的最新汇率数据。以下是获取汇率的函数：\n\n```python\nimport requests\nimport json\nimport schedule\nimport time\nimport csv\n\ndef get_exchange_rate_by_api(currency, currency_to):\n    url = \"https://api.exchangerate-api.com/v4/latest/\" + currency\n    response = requests.get(url)\n    data = response.json()\n    exchange_rate = data[\"rates\"][currency_to]\n    return exchange_rate\n```\n\n接下来，我们通过调度get_exchange_rate_by_api，每天执行获取汇率的操作，并将结果保存到 CSV 文件中：\n\n```python\ndef job():\n    rate_try = get_exchange_rate_by_api(\"TRY\", \"CNY\")\n    rate_usd = get_exchange_rate_by_api(\"USD\", \"CNY\")\n    rate_sgd = get_exchange_rate_by_api(\"USD\", \"SGD\")\n\n    today = time.strftime(\"%Y-%m-%d\", time.localtime())\n\n    with open('exchange_rates.csv', 'a', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerow([today, \"TRY\", \"CNY\", rate_try])\n        writer.writerow([today, \"USD\", \"CNY\", rate_usd])\n        writer.writerow([today, \"USD\", \"SGD\", rate_sgd])\n\n    notice_text = f\"今日{today}，土耳其里拉对人民币的汇率为：{rate_try}，美元对人民币的汇率为：{rate_usd}，美元对新加坡元的汇率为：{rate_sgd}\"\n    notice(notice_text)\n\n    print(f\"执行完成，{today}\")\n```\n\n最后，我们定义通知函数 `notice`，通过 Telegram Bot API 将获取到的汇率信息发送到指定的聊天群：\n\n```python\ndef notice(text):\n    api_url = \"https://domain.com/api/send_notification\"\n    data = {\n        \"chat_id\": \"{chat_id}\",\n        \"text\": text\n    }\n\n    response = requests.post(api_url, json=data)\n\n    if response.status_code == 200:\n        print(\"通知成功\")\n    else:\n        print(\"通知失败\")\n    return\n```\n\n在主程序中，我们通过 `if __name__ == '__main__':` 判断脚本是否直接运行，如果是，则执行 `job()` 函数，\n\n再通过cron任务实现每天早上 9 点自动获取并通知汇率信息。\n\n通过这样的自动化脚本，我们可以方便地获取最新的汇率数据，并及时了解货币兑换情况。","tags":["python","api"],"categories":["Python"]},{"title":"利用flaresolverr 实现cloudflare访问限制突破","url":"/2023/11/01/利用flaresolverr实现cloudflare访问限制突破/","content":"\n## 背景\n\n最近使用想通过RSS订阅常用论坛，做到信息统一浏览，但常用的nodeseek论坛因托管于cloudflare平台，直接订阅其RSS会出现因cloudflare保护措施无法访问获取。\n\n## 解决方案\n\n通过部署flaresolverr绕过cloudflare防范从而对网站进行访问。\n\n### flaresolverr 部署\n\n通过docker部署\n\n```shell\ndocker run -d   --name=flaresolverr   -p 8191:8191   -e LOG_LEVEL=info   --restart unless-stopped   ghcr.io/flaresolverr/flaresolverr:latest\n```\n\n### flaresolverr使用\n\n#### request.get\n\n```shell\ncurl -L -X POST 'http://localhost:8191/v1' -H 'Content-Type: application/json' --data-raw '{\n    \"cmd\": \"request.get\",\n\t\t\"url\":\"http://www.nodeseek.com/\",\n\t\t\"maxTimeout\": 60000\n }'\n```\n\n返回内容示例\n\n```json\n{\n    \"solution\": {\n        \"url\": \"https://www.google.com/?gws_rd=ssl\",\n        \"status\": 200,\n        \"headers\": {\n            \"status\": \"200\",\n            \"date\": \"Thu, 16 Jul 2020 04:15:49 GMT\",\n            \"expires\": \"-1\",\n            \"cache-control\": \"private, max-age=0\",\n            \"content-type\": \"text/html; charset=UTF-8\",\n            \"strict-transport-security\": \"max-age=31536000\",\n            \"p3p\": \"CP=\\\"This is not a P3P policy! See g.co/p3phelp for more info.\\\"\",\n            \"content-encoding\": \"br\",\n            \"server\": \"gws\",\n            \"content-length\": \"61587\",\n            \"x-xss-protection\": \"0\",\n            \"x-frame-options\": \"SAMEORIGIN\",\n            \"set-cookie\": \"1P_JAR=2020-07-16-04; expires=Sat...\"\n        },\n        \"response\":\"<!DOCTYPE html>...\",\n        \"cookies\": [\n            {\n                \"name\": \"NID\",\n                \"value\": \"204=QE3Ocq15XalczqjuDy52HeseG3zAZuJzID3R57...\",\n                \"domain\": \".google.com\",\n                \"path\": \"/\",\n                \"expires\": 1610684149.307722,\n                \"size\": 178,\n                \"httpOnly\": true,\n                \"secure\": true,\n                \"session\": false,\n                \"sameSite\": \"None\"\n            },\n            {\n                \"name\": \"1P_JAR\",\n                \"value\": \"2020-07-16-04\",\n                \"domain\": \".google.com\",\n                \"path\": \"/\",\n                \"expires\": 1597464949.307626,\n                \"size\": 19,\n                \"httpOnly\": false,\n                \"secure\": true,\n                \"session\": false,\n                \"sameSite\": \"None\"\n            }\n        ],\n        \"userAgent\": \"Windows NT 10.0; Win64; x64) AppleWebKit/5...\"\n    },\n    \"status\": \"ok\",\n    \"message\": \"\",\n    \"startTimestamp\": 1594872947467,\n    \"endTimestamp\": 1594872949617,\n    \"version\": \"1.0.0\"\n}\n```\n\n#### sessions.create\n\n```shell\n# 创建seesion\ncurl -L -X POST 'http://localhost:8191/v1' -H 'Content-Type: application/json' --data-raw '{\n    \"cmd\": \"sessions.create\",\n    \"url\":\"http://www.nodeseek.com/rss.xml\",\n    \"maxTimeout\": 60000\n  }'\n  \n# 应用session\ncurl -L -X POST 'http://localhost:8191/v1' -H 'Content-Type: application/json' --data-raw '{\n    \"cmd\": \"request.get\",\n    \"url\":\"http://www.nodeseek.com/rss.xml\",\n    \"maxTimeout\": 60000,\n    \"session\": \"48554e80-803c-11ee-804f-0242ac110002\"\n  }'\n```\n\n#### session.list\n\n```shell\ncurl -L -X POST 'http://localhost:8191/v1' -H 'Content-Type: application/json' --data-raw '{\n    \"cmd\": \"sessions.list\"}'\n```\n\n\n\n## 场景应用\n\n```python\nfrom http.server import BaseHTTPRequestHandler, HTTPServer\nfrom curl_cffi import requests\nimport xml.dom.minidom  # Library for XML formatting\nimport xml.parsers.expat\nimport subprocess\nimport json\n\n\n# Define the URL and the user agent\nurl = \"https://www.nodeseek.com/rss.xml\"\nuser_agent = \"chrome110\"\noutput_file = \"rss.xml\"  # New file to save the content\ncurl_command = (\n    \"curl -L -X POST 'http://localhost:8191/v1' \"\n    \"-H 'Content-Type: application/json' \"\n    \"--data-raw '{\"\n    \"  \\\"cmd\\\": \\\"request.get\\\",\"\n    \"  \\\"url\\\":\\\"http://www.nodeseek.com/rss.xml\\\",\"\n    \"  \\\"maxTimeout\\\": 60000,\"\n    \"  \\\"session\\\": \\\"48554e80-803c-11ee-804f-0242ac110002\\\"\"\n    \"}'\"\n)\n\nclass MyHandler(BaseHTTPRequestHandler):\n    def do_GET(self):\n        if self.path == '/':\n            try:\n                # Send an HTTP GET request\n                #response = requests.get(url, impersonate=user_agent)\n                #response_text = response.text\n                response = subprocess.run(curl_command, shell=True, check=True, capture_output=True, text=True)\n                json_response = json.loads(response.stdout)\n                response_text = json_response.get('solution', {}).get('response', '')\n\n                # Parse the RSS XML\n                dom = xml.dom.minidom.parseString(response_text)\n                formatted_xml = dom.toprettyxml()\n\n                # Save the formatted content to 'rss.xml'\n                with open(output_file, 'w', encoding='utf-8') as file:\n                    file.write(formatted_xml)\n\n                # Set the HTTP response headers\n                self.send_response(200)\n                self.send_header('Content-type', 'application/rss+xml; charset=utf-8')  # Set content type to XML\n                self.end_headers()\n\n                # Send the content of 'rss.xml' as the response\n                with open(output_file, 'rb') as file:\n                    self.wfile.write(file.read())\n            except xml.parsers.expat.ExpatError as e:\n                # Handle XML parsing errors\n                error_message = \"Error parsing XML: \" + str(e)\n                self.send_response(500)\n                self.send_header('Content-type', 'text/plain; charset=utf-8')\n                self.end_headers()\n                self.wfile.write(error_message.encode('utf-8'))\n        else:\n            self.send_response(404)\n            self.end_headers()\n            self.wfile.write(b'Not Found')\n\ndef run(server_class=HTTPServer, handler_class=MyHandler, port=8123):\n    server_address = ('', port)\n    httpd = server_class(server_address, handler_class)\n    print(f'Starting HTTP server on port {port}')\n    httpd.serve_forever()\n\nif __name__ == '__main__':\n    run()\n```\n\n","tags":["python","flaresolverr"],"categories":["Python"]},{"title":"容器化部署telegram自建api","url":"/2023/11/01/容器化部署telegram自建api/","content":"\n\n\n## 容器构建\n\n创建docker-compose.yml\n\n```yaml\nversion: '3.7'\n\nservices:\n  telegram-bot-api:\n    image: aiogram/telegram-bot-api:latest\n    restart: always\n    environment:\n      TELEGRAM_API_ID: \"your_api_id\"\n      TELEGRAM_API_HASH: \"your_api_hash\"\n      TELEGRAM_LOCAL: 1\n    volumes:\n      - $pwd/data:/var/lib/telegram-bot-api\n    ports:\n      # :左边为映射的端口\n      - 8888:8081\n```\n\n执行命令\n\n```shell\ndocker-compose up -d\n```\n\n\n\n## 使用\n\n- 调用api进行消息通知\n\n  ```\n  http://yourdomain.com:8888/bot{bot-token}/sendMessage?chat_id={chat-id}&text={content}\n  \n  https://api.telegram.org/botXXXXXX/sendMessage?chat_id=YYYYYY&text={content}\n  ```\n\n...\n","tags":["docker","api","telegram"],"categories":["Docker"]},{"title":"尝试调用E5-graph API 实现邮箱发送","url":"/2023/11/01/尝试调用E5-graph API 实现邮箱发送/","content":"\n## Azure 应用创建\n\n- 创建api 获取client_Id, client_secrete\n- 添加api权限\n\n![img](https://od.009100.xyz/api/raw/?path=/picture/blog/api_authority.png)\n\n## 基于Flask 创建API\n\n- 创建路由获取post请求的json数据\n- 通过client_id等信息请求发起token请求\n- 使用token调用api实现邮件发送\n\n```python\nfrom flask import Flask, request, jsonify\nimport requests\nimport json\n\napp = Flask(__name__)\n\n# 应用程序注册时获得的信息\nclient_id = \"{client_Id}\"\nclient_secret = \"{client_secrete}\"\nredirect_uri = \"http://localhost\"\n\n# 租户 ID，通常为 \"common\"\ntenant_id = \"{tenant_Id}\"\n\n# Microsoft Graph API 的端点\ntoken_url = f\"https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token\"\ngraph_api_url = \"https://graph.microsoft.com/v1.0/users/{mail_address}/sendMail\"\n\n@app.route('/send_email', methods=['POST'])\ndef send_email():\n    # 获取 POST 请求的 JSON 数据\n    data = request.get_json()\n\n    # 构建获取访问令牌的请求\n    token_payload = {\n        \"grant_type\": \"client_credentials\",\n        \"client_id\": client_id,\n        \"client_secret\": client_secret,\n        \"scope\": \"https://graph.microsoft.com/.default\"\n    }\n\n    token_response = requests.post(token_url, data=token_payload)\n    token_data = token_response.json()\n\n    # 提取访问令牌\n    access_token = token_data.get(\"access_token\")\n    print(access_token)\n\n    # 构建发送邮件的请求\n    email_payload = {\n        \"message\": {\n            \"subject\": data.get(\"subject\"),\n            \"body\": {\n                \"contentType\": \"Text\",\n                \"content\": data.get(\"body\"),\n            },\n            \"toRecipients\": [\n                {\n                    \"emailAddress\": {\n                        \"address\": data.get(\"recipient\"),\n                    }\n                }\n            ],\n        }\n    }\n\n    headers = {\n        \"Authorization\": \"Bearer \" + access_token,\n        \"Content-Type\": \"application/json\",\n    }\n\n    # 发送邮件\n    email_response = requests.post(graph_api_url, headers=headers, json=email_payload)\n\n    # 打印 API 响应\n    print(email_response.text)\n\n    # 检查是否成功发送邮件\n    if email_response.status_code == 202:\n        return jsonify({\"status\": \"success\", \"message\": \"邮件发送成功！\"})\n    else:\n        return jsonify({\"status\": \"error\", \"message\": f\"邮件发送失败，状态码：{email_response.status_code}, 内容：{email_response.text}\"})\n\n\nif __name__ == '__main__':\n    app.run(debug=True,port=8444)\n\n```\n\n## API调用\n\n- 创建待发邮件数据json数据\n- 发起post请求\n\n```python\nimport requests\nimport json\n\n# API 的完整 URL，包括协议、域名和端口\napi_url = \"https://domain.com/s/send_email\"\n\n# 要发送的邮件数据\nemail_data = {\n    \"subject\": \"Test Subject\",\n    \"body\": \"This is the email body.\",\n    \"recipient\": \"xxxxx@icloud.com\"\n}\n\ntry:\n    # 发送 POST 请求给 API\n    response = requests.post(api_url, json=email_data)\n\n    # 尝试解析 JSON\n    result = response.json()\n\n    # 检查响应状态码\n    if response.status_code == 200:\n        print(\"API 响应成功:\", result)\n    else:\n        print(\"API 响应错误:\", result)\n\nexcept requests.exceptions.RequestException as e:\n    print(\"请求异常:\", e)\nexcept json.JSONDecodeError as e:\n    print(\"JSON 解码错误:\", e)\n\n```\n\n## 失败\n\n因微软限制邮件滥发等原因，调用成功后邮件被退回\n\n![fail](https://od.009100.xyz/api/raw/?path=/picture/blog/fail.png)\n\n## requests 库post 和get 方法\n\n后续需完成这两个方法的总结，非常常用。\n","tags":["python","API"],"categories":["Python"]},{"title":"acme安装证书","url":"/2023/11/01/手动通过acme安装证书/","content":"#### 手动通过acme安装证书\n\n```shell\ncurl https://get.acme.sh | sh #安装acme\n```\n\n```shell\n~/.acme.sh/acme.sh --set-default-ca --server letsencrypt #替换服务\n```\n\n```shell\n~/.acme.sh/acme.sh --issue -d khw.423327.xyz --dns --yes-I-know-dns-manual-mode-enough-go-ahead-please  #生成添加TXT需要的NAME和Value\n```\n\n#### 去域名服务商处添加TXT\n\n```shell\n~/.acme.sh/acme.sh --renew -d khw.423327.xyz --yes-I-know-dns-manual-mode-enough-go-ahead-please  #生成证书\n```\n\n#### 生成结果如下\n\n```shell\n[Fri 06 Jan 2023 07:37:07 AM EST] Your cert is in: /root/.acme.sh/khw.423327.xyz/khw.423327.xyz.cer\n[Fri 06 Jan 2023 07:37:07 AM EST] Your cert key is in: /root/.acme.sh/khw.423327.xyz/khw.423327.xyz.key #使用这个\n[Fri 06 Jan 2023 07:37:07 AM EST] The intermediate CA cert is in: /root/.acme.sh/khw.423327.xyz/ca.cer\n[Fri 06 Jan 2023 07:37:07 AM EST] And the full chain certs is there: /root/.acme.sh/khw.423327.xyz/fullchain.cer #使用这个\n```\n","tags":["ssl","acme","certificate"],"categories":["Linux"]},{"title":"服务器开启BBR","url":"/2023/11/01/服务器开启BBR/","content":"\n### 什么是BBR\n\nBBR是Google提出的一种新型拥塞控制算法，可以提高服务器的吞吐和TCP连接的延迟；\n\n### 前期准备\n\n不同系统开启BBR的方式有些许的差异，大致可以区分是否需要安装最新内核\n\n1 查看服务器系统和服务器内核版本\n\n```shell\nuname -a\nLinux 5.17.9-1.el7.elrepo.x86_64 #1 SMP PREEMPT Tue May 17 16:17:10 EDT 2022 x86_64 x86_64 x86_64 GNU/Linux\n# 可以看出内核版本为5.17\ncat /etc/issue\nDebian GNU/Linux 10 \\n \\l\n# 系统为debian\ncat /etc/redhat-release\nCentOS Linux release 7.9.2009 (Core)\n#c 系统为centos\n```\n\n2 查看系统是bbr开启情况\n\n```shell\nsysctl net.ipv4.tcp_available_congestion_control\nnet.ipv4.tcp_available_congestion_control = reno cubic bbr\n\nlsmod |grep bbr\ntcp_bbr                20480  16\n\n#两种方式返回值带bbr 则表示已开启\n```\n\n\n\n3 开启BBR\n\n1）当系统内核版本低于4.1\n\n```shell\n#首先安装elrepo源\nrpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org\nrpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm\n#启用elrepo-kernel\nyum-config-manager --enable elrepo-kernel\n#安装内核\nyum -y install kernel-ml kernel-ml-devel\n#查看内核启动项排序正常新安装的内核版本在0号位\nawk -F\\' '$1==\"menuentry \" {print i++ \" : \" $2}' /etc/grub2.cfg\n#如果内核启动编号不是0执行\ngrub2-set-default 0\n#重启\nreboot\n```\n\n2） 当系统内核版本高于4.1\n\n```shell\n#直接开启BBR\necho \"net.core.default_qdisc=fq\" >> /etc/sysctl.conf\necho \"net.ipv4.tcp_congestion_control=bbr\" >> /etc/sysctl.conf\nsysctl -p\n#完成后按照2 进行验证\n```\n\n4 注意事项\n\n**低版本的内核在升级时偶尔会遇到更新完内核重启失联的现象，故尽量选择高版本系统直接执行3-2）步骤**\n","tags":["BBR","server"],"categories":["Linux"]},{"title":"甲骨文MFA验证设备丢失解决办法","url":"/2023/11/01/甲骨文 MFA 验证设备丢失解决办法/","content":"\n### 背景：\n\n旧手机处理后发现oracle authentictor 忘记迁移，oracle cloud 无法登陆；\n\n### 解决方案：\n\n\n先进登录页面，然后在输入邮箱和密码页面中记下以`` https://idcs-********.identity.oraclecloud.com/`` 开头的网址\n\n现在修改该 网址，如下所示\n\n``https://idcs-*****.identity.oraclecloud.com/ui/v1/myconsole?root=my-info&my-info=my_profile_security``\n\n在此新网址中，可以禁用 MFA 的选项，或获取绕过代码，或任何可以恢复的此类选项。\n\n> [来源](https://community.oracle.com/customerconnect/discussion/710608/lost-my-access-2fa)\n\n","tags":["MFA","Oracle"],"categories":["Others"]},{"title":"vim中文乱码问题解决","url":"/2023/11/01/解决vim开启中文乱码问题/","content":"\n#### 新建～/.vimrc文件\n\n```shell\nvim ~/.vimrc\n```\n\n#### 写入以下内容\n\n```shell\nset fileencodings=utf-8,gb2312,gb18030,gbk,ucs-bom,cp936,latin1\nset enc=utf8\nset fencs=utf8,gbk,gb2312,gb18030\n```\n\n#### 更新\n\n```shell\nsource ~/.vimrc\n```\n\n#### 其他\n\nlinux 无法正常显示中文\n\n```shell\necho \"export LANG=en_US.UTF-8\" >> ~/.bashrc \nsource ~/.bashrc\n```\n\n\n\n","tags":["vim","encoding"],"categories":["Linux"]},{"title":"基于ddddocr库的验证码识别","url":"/2023/10/15/基于ddddocr实现验证识别/","content":"\n## 背景\n\nEbay公益服的持续使用往往需要通过签到赚取积分来维持，偶尔因忙于他事容易忘记，故在翻阅他人资料后着手开发。\n\n## 依赖准备\n\n- ddddocr 用于验证码识别\n- telethon 同telegram交互\n- asyncio 异步任务处理\n- requests 通知请求\n\n```shell\n# 安装\npip install ddddocr\n# 安装时发生了报错\nAttributeError: module 'PIL.Image' has no attribute 'ANTIALIAS'\n```\n\n在github上发现有[issue](https://github.com/sml2h3/ddddocr/issues/139)解决方案\n\n```sh\n#按照方案二处理，降级Pillow的版本，比如使用9.5.0版本（先卸载，再重新安装）\npip uninstall -y Pillow\npip install Pillow==9.5.0\n```\n\n其他正常安装即可\n\n## ddddocr 使用\n\n```python\nimport ddddocr\n\nocr = ddddocr.DdddOcr(beta=True,show_ad=False) #开启测试版 关闭广告\n\nwith open(\"test.jpg\", 'rb') as f:\n    image = f.read()\n\nres = ocr.classification(image)\nprint(res)\n```\n\n图片示例：\n\n![识别图片示例](https://od.wadaho.cf/api/raw/?path=/picture/blog/2023-11-13%2012.54.05.jpg)\n\n其他一些[用法](https://github.com/sml2h3/ddddocr#readme)\n\n## 实现\n\n实现路径为\n\n- 通过与telegram交互发送签到/checkin 命令，\n- 获取验证码，\n- 通过captcha_solver识别验证码，\n- 最后交互发送识别结果，\n- 判断结果信息，失败重试。\n\n```python\n# -*- coding: utf-8 -*-\nimport os\nimport time\nfrom telethon import TelegramClient, events\nimport ddddocr\nimport asyncio\nimport requests\ndef notice(text):\n    api_url = \"https://noticurl.com/send_notification\"\n    data = {\n        \"chat_id\": \"{chat_id}\",\n        \"text\": text\n    }\n    response = requests.post(api_url, json=data)\n    return print(response.json())\n\ndef captcha_solver(f):\n    with open(f, 'rb') as image_file: \n        image_bytes = image_file.read()\n        ocr = ddddocr.DdddOcr(beta=True, show_ad=False)\n        res = ocr.classification(image_bytes)\n    return res\n\nasync def tg_qd(client, tg_bot, tg_command):\n    await client.send_message(tg_bot, tg_command)\n    await asyncio.sleep(5)  # 使用 asyncio.sleep 代替 time.sleep\n    messages = await client.get_messages(tg_bot)\n    await messages[0].download_media(file=\"1.jpg\")\n    the_code = captcha_solver(\"1.jpg\")\n    await client.send_message(tg_bot, the_code)\n    await asyncio.sleep(5)\n    messages = await client.get_messages(tg_bot)\n    return messages[0].message\n\napi_id = [{api_id}]  # 输入api_id，一个账号一项\napi_hash = ['{api_hash}']  # 输入api_hash，一个账号一项\nsession_name = api_id[:]\nbots_commands = [\"@{channelname}\", \"/checkin\", \"成功\",\"/cancle\"]\n\nasync def main():\n    for num in range(len(api_id)):\n        session_name[num] = \"id_\" + str(session_name[num])\n        client = TelegramClient(\n            session_name[num],\n            api_id[num],\n            api_hash[num],\n            proxy=(\"socks5\", \"127.0.0.1\", 1087), #本地运行开启代理\n        )\n        try:\n            await client.start()\n            the_result = await tg_qd(client, bots_commands[0], bots_commands[1])\n            print(the_result)\n            i = 0\n            while bots_commands[2] not in the_result: \n                i += 1\n                await client.send_message(bots_commands[0], bots_commands[3])\n                await asyncio.sleep(5)\n                the_result = await tg_qd(client, bots_commands[0], bots_commands[1])\n                if i > 2: \n                    break\n            text = the_result + \"终点站\"\n            notice(text)\n        except Exception as err:\n            error_message = f\"Error in main(): {str(err)}\"\n            notice(error_message)\n        finally:\n            await client.disconnect()\n\nasyncio.run(main())\n\n```\n\n# 其他\n\n### Truecaptcha\n\n本来想通过[truecaptcha](https://truecaptcha.org/code.html)来实现验证码识别\n\n```python\nimport requests\nimport base64\n\ndef solve(f):\n\twith open(f, \"rb\") as image_file:\n\t\tencoded_string = base64.b64encode(image_file.read()).decode('ascii')\n\t\turl = 'https://api.apitruecaptcha.org/one/gettext'\n\n\t\tdata = { \n\t\t\t'userid':'{id}', #填入id\n\t\t\t'apikey':'{key}',  #填入key\n\t\t\t'data':encoded_string\n\t\t}\n\t\tresponse = requests.post(url = url, json = data)\n\t\tdata = response.json()\n\t\treturn data\n```\n\n但是之前的单日100的限额变成了一次。\n\n另外试了一下改服务对于中文识别不太行，英文数字还可以\n\n### telegram交互\n\n后续还需要多研究研究！\n","tags":["python","ddddocr"],"categories":["Python"]}]